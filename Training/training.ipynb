{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Firstly let's yoink the code of the model from Karpathy's GitHub :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tiktoken\n",
    "import inspect\n",
    "from dataclasses import dataclass\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I got the LoRA code from here https://towardsdatascience.com/implementing-lora-from-scratch-20f838b046f1?gi=503a60de473f.\n",
    "\n",
    "Also Got some code from here https://github.com/danielgrittner/nanoGPT-LoRA/blob/master/model.py to fix an error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LoraLinear(nn.Linear):\n",
    "    \"\"\"\n",
    "    Extends a PyTorch linear layer with Low-Rank Adaptation (LoRA).\n",
    "    LoRA adds two matrices to the layer, allowing for efficient training of large models.\n",
    "    \"\"\"\n",
    "    def __init__(self, in_features, out_features, rank, bias, *args, **kwargs):\n",
    "        super().__init__(in_features, out_features, bias, *args, **kwargs)\n",
    "\n",
    "        # Initialize LoRA matrices\n",
    "        self.lora_matrix_B = nn.Parameter(torch.zeros(out_features, rank))\n",
    "        self.lora_matrix_A = nn.Parameter(torch.randn(rank, in_features))\n",
    "        \n",
    "        # Freeze the original weight matrix\n",
    "        self.weight.requires_grad = False\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        # Compute LoRA weight adjustment\n",
    "        lora_weights = torch.matmul(self.lora_matrix_B, self.lora_matrix_A)\n",
    "        # Apply the original and LoRA-adjusted linear transformations\n",
    "        return super().forward(x) + F.linear(x, lora_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    \"\"\" LayerNorm but with an optional bias. PyTorch doesn't support simply bias=False \"\"\"\n",
    "\n",
    "    def __init__(self, ndim, bias):\n",
    "        super().__init__()\n",
    "        self.weight = nn.Parameter(torch.ones(ndim))\n",
    "        self.bias = nn.Parameter(torch.zeros(ndim)) if bias else None\n",
    "\n",
    "    def forward(self, input):\n",
    "        return F.layer_norm(input, self.weight.shape, self.weight, self.bias, 1e-5)\n",
    "\n",
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads, but in a batch\n",
    "        self.c_attn = LoraLinear(config.n_embd, 3 * config.n_embd, rank=config.lora_rank, bias=config.bias)\n",
    "        # output projection\n",
    "        self.c_proj = LoraLinear(config.n_embd, config.n_embd, rank=config.lora_rank, bias=config.bias)\n",
    "        # regularization\n",
    "        self.attn_dropout = nn.Dropout(config.dropout)\n",
    "        self.resid_dropout = nn.Dropout(config.dropout)\n",
    "        self.n_head = config.n_head\n",
    "        self.n_embd = config.n_embd\n",
    "        self.dropout = config.dropout\n",
    "        # flash attention make GPU go brrrrr but support is only in PyTorch >= 2.0\n",
    "        self.flash = hasattr(torch.nn.functional, 'scaled_dot_product_attention')\n",
    "        if not self.flash:\n",
    "            print(\"WARNING: using slow attention. Flash Attention requires PyTorch >= 2.0\")\n",
    "            # causal mask to ensure that attention is only applied to the left in the input sequence\n",
    "            self.register_buffer(\"bias\", torch.tril(torch.ones(config.block_size, config.block_size))\n",
    "                                        .view(1, 1, config.block_size, config.block_size))\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, T, C = x.size() # batch size, sequence length, embedding dimensionality (n_embd)\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        q, k, v  = self.c_attn(x).split(self.n_embd, dim=2)\n",
    "        k = k.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = q.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = v.view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        if self.flash:\n",
    "            # efficient attention using Flash Attention CUDA kernels\n",
    "            y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n",
    "        else:\n",
    "            # manual implementation of attention\n",
    "            att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "            att = att.masked_fill(self.bias[:,:,:T,:T] == 0, float('-inf'))\n",
    "            att = F.softmax(att, dim=-1)\n",
    "            att = self.attn_dropout(att)\n",
    "            y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_dropout(self.c_proj(y))\n",
    "        return y\n",
    "\n",
    "class MLP(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.c_fc    = nn.Linear(config.n_embd, 4 * config.n_embd, bias=config.bias)\n",
    "        self.gelu    = nn.GELU()\n",
    "        self.c_proj  = nn.Linear(4 * config.n_embd, config.n_embd, bias=config.bias)\n",
    "        self.dropout = nn.Dropout(config.dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.c_fc(x)\n",
    "        x = self.gelu(x)\n",
    "        x = self.c_proj(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class Block(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln_1 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.ln_2 = LayerNorm(config.n_embd, bias=config.bias)\n",
    "        self.mlp = MLP(config)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 1024\n",
    "    vocab_size: int = 50304 # GPT-2 vocab_size of 50257, padded up to nearest multiple of 64 for efficiency\n",
    "    n_layer: int = 12\n",
    "    n_head: int = 12\n",
    "    n_embd: int = 768\n",
    "    dropout: float = 0.0\n",
    "    bias: bool = True # True: bias in Linears and LayerNorms, like GPT-2. False: a bit better and faster\n",
    "    lora_rank: int = 8\n",
    "\n",
    "class GPT(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.vocab_size is not None\n",
    "        assert config.block_size is not None\n",
    "        self.config = config\n",
    "\n",
    "        self.transformer = nn.ModuleDict(dict(\n",
    "            wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "            wpe = nn.Embedding(config.block_size, config.n_embd),\n",
    "            drop = nn.Dropout(config.dropout),\n",
    "            h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "            ln_f = LayerNorm(config.n_embd, bias=config.bias),\n",
    "        ))\n",
    "        self.lm_head = nn.Linear(config.n_embd, config.vocab_size, bias=False)\n",
    "        # with weight tying when using torch.compile() some warnings get generated:\n",
    "        # \"UserWarning: functional_call was passed multiple values for tied weights.\n",
    "        # This behavior is deprecated and will be an error in future versions\"\n",
    "        # not 100% sure what this is, so far seems to be harmless. TODO investigate\n",
    "        self.transformer.wte.weight = self.lm_head.weight # https://paperswithcode.com/method/weight-tying\n",
    "\n",
    "        # init all weights\n",
    "        self.apply(self._init_weights)\n",
    "        # apply special scaled init to the residual projections, per GPT-2 paper\n",
    "        for pn, p in self.named_parameters():\n",
    "            if pn.endswith('c_proj.weight'):\n",
    "                torch.nn.init.normal_(p, mean=0.0, std=0.02/math.sqrt(2 * config.n_layer))\n",
    "\n",
    "        # report number of parameters\n",
    "        print(\"number of parameters: %.2fM\" % (self.get_num_params()/1e6,))\n",
    "\n",
    "    def get_num_params(self, non_embedding=True):\n",
    "        \"\"\"\n",
    "        Return the number of parameters in the model.\n",
    "        For non-embedding count (default), the position embeddings get subtracted.\n",
    "        The token embeddings would too, except due to the parameter sharing these\n",
    "        params are actually used as weights in the final layer, so we include them.\n",
    "        \"\"\"\n",
    "        n_params = sum(p.numel() for p in self.parameters())\n",
    "        if non_embedding:\n",
    "            n_params -= self.transformer.wpe.weight.numel()\n",
    "        return n_params\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, nn.Linear):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "            if module.bias is not None:\n",
    "                torch.nn.init.zeros_(module.bias)\n",
    "        elif isinstance(module, nn.Embedding):\n",
    "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        device = idx.device\n",
    "        b, t = idx.size()\n",
    "        assert t <= self.config.block_size, f\"Cannot forward sequence of length {t}, block size is only {self.config.block_size}\"\n",
    "        pos = torch.arange(0, t, dtype=torch.long, device=device) # shape (t)\n",
    "\n",
    "        # forward the GPT model itself\n",
    "        tok_emb = self.transformer.wte(idx) # token embeddings of shape (b, t, n_embd)\n",
    "        pos_emb = self.transformer.wpe(pos) # position embeddings of shape (t, n_embd)\n",
    "        x = self.transformer.drop(tok_emb + pos_emb)\n",
    "        for block in self.transformer.h:\n",
    "            x = block(x)\n",
    "        x = self.transformer.ln_f(x)\n",
    "\n",
    "        if targets is not None:\n",
    "            # if we are given some desired targets also calculate the loss\n",
    "            logits = self.lm_head(x)\n",
    "            loss = F.cross_entropy(logits.view(-1, logits.size(-1)), targets.view(-1), ignore_index=-1)\n",
    "        else:\n",
    "            # inference-time mini-optimization: only forward the lm_head on the very last position\n",
    "            logits = self.lm_head(x[:, [-1], :]) # note: using list [-1] to preserve the time dim\n",
    "            loss = None\n",
    "\n",
    "        return logits, loss\n",
    "\n",
    "    def crop_block_size(self, block_size):\n",
    "        # model surgery to decrease the block size if necessary\n",
    "        # e.g. we may load the GPT2 pretrained model checkpoint (block size 1024)\n",
    "        # but want to use a smaller block size for some smaller, simpler model\n",
    "        assert block_size <= self.config.block_size\n",
    "        self.config.block_size = block_size\n",
    "        self.transformer.wpe.weight = nn.Parameter(self.transformer.wpe.weight[:block_size])\n",
    "        for block in self.transformer.h:\n",
    "            if hasattr(block.attn, 'bias'):\n",
    "                block.attn.bias = block.attn.bias[:,:,:block_size,:block_size]\n",
    "\n",
    "    @classmethod\n",
    "    def from_pretrained(cls, model_type, override_args=None):\n",
    "        assert model_type in {'gpt2', 'gpt2-medium', 'gpt2-large', 'gpt2-xl'}\n",
    "        override_args = override_args or {} # default to empty dict\n",
    "        # only dropout can be overridden see more notes below\n",
    "        assert all(k == 'dropout' for k in override_args)\n",
    "        from transformers import GPT2LMHeadModel\n",
    "        print(\"loading weights from pretrained gpt: %s\" % model_type)\n",
    "\n",
    "        # n_layer, n_head and n_embd are determined from model_type\n",
    "        config_args = {\n",
    "            'gpt2':         dict(n_layer=12, n_head=12, n_embd=768),  # 124M params\n",
    "            'gpt2-medium':  dict(n_layer=24, n_head=16, n_embd=1024), # 350M params\n",
    "            'gpt2-large':   dict(n_layer=36, n_head=20, n_embd=1280), # 774M params\n",
    "            'gpt2-xl':      dict(n_layer=48, n_head=25, n_embd=1600), # 1558M params\n",
    "        }[model_type]\n",
    "        print(\"forcing vocab_size=50257, block_size=1024, bias=True\")\n",
    "        config_args['vocab_size'] = 50257 # always 50257 for GPT model checkpoints\n",
    "        config_args['block_size'] = 1024 # always 1024 for GPT model checkpoints\n",
    "        config_args['bias'] = True # always True for GPT model checkpoints\n",
    "        # we can override the dropout rate, if desired\n",
    "        if 'dropout' in override_args:\n",
    "            print(f\"overriding dropout rate to {override_args['dropout']}\")\n",
    "            config_args['dropout'] = override_args['dropout']\n",
    "        # create a from-scratch initialized minGPT model\n",
    "        config = GPTConfig(**config_args)\n",
    "        model = GPT(config)\n",
    "        sd = model.state_dict()\n",
    "        sd_keys = sd.keys()\n",
    "        sd_keys = [k for k in sd_keys if not k.endswith('.attn.bias')] # discard this mask / buffer, not a param\n",
    "\n",
    "        # init a huggingface/transformers model\n",
    "        model_hf = GPT2LMHeadModel.from_pretrained(model_type)\n",
    "        sd_hf = model_hf.state_dict()\n",
    "\n",
    "        # copy while ensuring all of the parameters are aligned and match in names and shapes\n",
    "        sd_keys_hf = sd_hf.keys()\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.masked_bias')] # ignore these, just a buffer\n",
    "        sd_keys_hf = [k for k in sd_keys_hf if not k.endswith('.attn.bias')] # same, just the mask (buffer)\n",
    "        transposed = ['attn.c_attn.weight', 'attn.c_proj.weight', 'mlp.c_fc.weight', 'mlp.c_proj.weight']\n",
    "        # basically the openai checkpoints use a \"Conv1D\" module, but we only want to use a vanilla Linear\n",
    "        # this means that we have to transpose these weights when we import them\n",
    "        if config.lora_rank == 0:\n",
    "            assert len(sd_keys_hf) == len(sd_keys), f\"mismatched keys: {len(sd_keys_hf)} != {len(sd_keys)}\"\n",
    "        else:\n",
    "            # Subtract the LoRA parameters from len(sd_keys)\n",
    "            assert len(sd_keys_hf) == len(sd_keys) - 4 * config.n_layer\n",
    "        for k in sd_keys_hf:\n",
    "            if any(k.endswith(w) for w in transposed):\n",
    "                # special treatment for the Conv1D weights we need to transpose\n",
    "                assert sd_hf[k].shape[::-1] == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k].t())\n",
    "            else:\n",
    "                # vanilla copy over the other parameters\n",
    "                assert sd_hf[k].shape == sd[k].shape\n",
    "                with torch.no_grad():\n",
    "                    sd[k].copy_(sd_hf[k])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def configure_optimizers(self, weight_decay, learning_rate, device_type):\n",
    "        # start with all of the candidate parameters\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        # filter out those that do not require grad\n",
    "        param_dict = {pn: p for pn, p in param_dict.items() if p.requires_grad}\n",
    "        # create optim groups. Any parameters that is 2D will be weight decayed, otherwise no.\n",
    "        # i.e. all weight tensors in matmuls + embeddings decay, all biases and layernorms don't.\n",
    "        decay_params = [p for n, p in param_dict.items() if p.dim() >= 2]\n",
    "        nodecay_params = [p for n, p in param_dict.items() if p.dim() < 2]\n",
    "        optim_groups = [\n",
    "            {'params': decay_params, 'weight_decay': weight_decay},\n",
    "            {'params': nodecay_params, 'weight_decay': 0.0}\n",
    "        ]\n",
    "        num_decay_params = sum(p.numel() for p in decay_params)\n",
    "        num_nodecay_params = sum(p.numel() for p in nodecay_params)\n",
    "        print(f\"num decayed parameter tensors: {len(decay_params)}, with {num_decay_params:,} parameters\")\n",
    "        print(f\"num non-decayed parameter tensors: {len(nodecay_params)}, with {num_nodecay_params:,} parameters\")\n",
    "        # Create AdamW optimizer and use the fused version if it is available\n",
    "        fused_available = 'fused' in inspect.signature(torch.optim.AdamW).parameters\n",
    "        use_fused = fused_available and device_type == 'cuda'\n",
    "        extra_args = dict(fused=True) if use_fused else dict()\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=learning_rate, **extra_args)\n",
    "        print(f\"using fused AdamW: {use_fused}\")\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def estimate_mfu(self, fwdbwd_per_iter, dt):\n",
    "        \"\"\" estimate model flops utilization (MFU) in units of A100 bfloat16 peak FLOPS \"\"\"\n",
    "        # first estimate the number of flops we do per iteration.\n",
    "        # see PaLM paper Appendix B as ref: https://arxiv.org/abs/2204.02311\n",
    "        N = self.get_num_params()\n",
    "        cfg = self.config\n",
    "        L, H, Q, T = cfg.n_layer, cfg.n_head, cfg.n_embd//cfg.n_head, cfg.block_size\n",
    "        flops_per_token = 6*N + 12*L*H*Q*T\n",
    "        flops_per_fwdbwd = flops_per_token * T\n",
    "        flops_per_iter = flops_per_fwdbwd * fwdbwd_per_iter\n",
    "        # express our flops throughput as ratio of A100 bfloat16 peak flops\n",
    "        flops_achieved = flops_per_iter * (1.0/dt) # per second\n",
    "        flops_promised = 312e12 # A100 GPU bfloat16 peak flops is 312 TFLOPS\n",
    "        mfu = flops_achieved / flops_promised\n",
    "        return mfu\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def generate(self, idx, max_new_tokens, temperature=1.0, top_k=None):\n",
    "        \"\"\"\n",
    "        Take a conditioning sequence of indices idx (LongTensor of shape (b,t)) and complete\n",
    "        the sequence max_new_tokens times, feeding the predictions back into the model each time.\n",
    "        Most likely you'll want to make sure to be in model.eval() mode of operation for this.\n",
    "        \"\"\"\n",
    "        for _ in range(max_new_tokens):\n",
    "            # if the sequence context is growing too long we must crop it at block_size\n",
    "            idx_cond = idx if idx.size(1) <= self.config.block_size else idx[:, -self.config.block_size:]\n",
    "            # forward the model to get the logits for the index in the sequence\n",
    "            logits, _ = self(idx_cond)\n",
    "            # pluck the logits at the final step and scale by desired temperature\n",
    "            logits = logits[:, -1, :] / temperature\n",
    "            # optionally crop the logits to only the top k options\n",
    "            if top_k is not None:\n",
    "                v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n",
    "                logits[logits < v[:, [-1]]] = -float('Inf')\n",
    "            # apply softmax to convert logits to (normalized) probabilities\n",
    "            probs = F.softmax(logits, dim=-1)\n",
    "            # sample from the distribution\n",
    "            idx_next = torch.multinomial(probs, num_samples=1)\n",
    "            # append sampled index to the running sequence and continue\n",
    "            idx = torch.cat((idx, idx_next), dim=1)\n",
    "\n",
    "        return idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now making the dataset. I did not create validation set because I do want the data to overfit to an extent (totally not because I didn't want try generating data for it cuz the generations are identical)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoaderLite:\n",
    "    def __init__(self, B, T):\n",
    "        self.B = B\n",
    "        self.T = T\n",
    "\n",
    "        current_dir = os.getcwd()\n",
    "        parent_dir = os.path.abspath(os.path.join(current_dir, os.pardir))\n",
    "        CreateDataset_path = os.path.join(parent_dir, 'CreateDataset')\n",
    "        trainingdata_path = os.path.join(CreateDataset_path, 'trainingdata/data.txt')\n",
    "\n",
    "        with open(trainingdata_path, 'r', encoding='utf-8') as file:\n",
    "            text = file.read()\n",
    "        enc = tiktoken.get_encoding('gpt2')\n",
    "        tokens = enc.encode(text)\n",
    "        self.tokens = torch.tensor(tokens)\n",
    "        print(f'loaded {len(self.tokens)} tokens')\n",
    "        print(f'1 epoch = {len(self.tokens) // (B * T) } batches')\n",
    "\n",
    "        self.current_position = 0\n",
    "\n",
    "    def next_batch(self):\n",
    "        B, T = self.B, self.T\n",
    "        buf = self.tokens[self.current_position : self.current_position+B*T+1]\n",
    "        x = (buf[:-1]).view(B, T) # inputs\n",
    "        y = (buf[1:]).view(B, T) # targets\n",
    "        # advance the position in the tensor\n",
    "        self.current_position += B * T \n",
    "        # if loading the next batch would be out of bounds, advance to next shard\n",
    "        if self.current_position + (B * T + 1) > len(self.tokens):\n",
    "            self.current_position = 0\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training time, then saving :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    device = \"cpu\"\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "    elif hasattr(torch.backends, \"mps\") and torch.backends.mps.is_available():\n",
    "        device = \"mps\"\n",
    "    print(f\"using device: {device}\")\n",
    "\n",
    "    model = GPT.from_pretrained('gpt2')\n",
    "    model.to(device)\n",
    "    # model = torch.compile(model) # RuntimeError: Windows not yet supported for torch.compile\n",
    "\n",
    "    optimizer = model.configure_optimizers(weight_decay=0.1, learning_rate=6e-4, device_type=device)\n",
    "    print('\\n yay loading worked:3')\n",
    "\n",
    "    train_writer = SummaryWriter('runs/train')\n",
    "    train_loader = DataLoaderLite(B=4, T=64)\n",
    "    steps = 11200 # about 10 epochs for my dataset\n",
    "    for i in range(steps):\n",
    "        x, y = train_loader.next_batch()\n",
    "        x, y = x.to(device), y.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits, loss = model(x, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        print(f\"step {i}, loss: {loss.item()}\")\n",
    "\n",
    "        train_writer.add_scalar('train_loss', loss.item(), i)\n",
    "        train_writer.close()\n",
    "    \n",
    "    torch.save(model, \"andrejgpt.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n",
      "loading weights from pretrained gpt: gpt2\n",
      "forcing vocab_size=50257, block_size=1024, bias=True\n",
      "number of parameters: 124.10M\n",
      "num decayed parameter tensors: 74, with 96,449,280 parameters\n",
      "num non-decayed parameter tensors: 98, with 121,344 parameters\n",
      "using fused AdamW: True\n",
      "\n",
      " yay loading worked:3\n",
      "loaded 286910 tokens\n",
      "1 epoch = 1120 batches\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\XPS\\AppData\\Local\\Temp\\ipykernel_28136\\1352349083.py:47: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at ..\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:263.)\n",
      "  y = torch.nn.functional.scaled_dot_product_attention(q, k, v, attn_mask=None, dropout_p=self.dropout if self.training else 0, is_causal=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, loss: 3.890249729156494\n",
      "step 1, loss: 4.210996627807617\n",
      "step 2, loss: 3.619464874267578\n",
      "step 3, loss: 4.030375957489014\n",
      "step 4, loss: 4.109189033508301\n",
      "step 5, loss: 3.7648096084594727\n",
      "step 6, loss: 3.5123369693756104\n",
      "step 7, loss: 3.8242733478546143\n",
      "step 8, loss: 3.625845193862915\n",
      "step 9, loss: 3.8695595264434814\n",
      "step 10, loss: 3.7515037059783936\n",
      "step 11, loss: 4.418963432312012\n",
      "step 12, loss: 4.182020664215088\n",
      "step 13, loss: 3.8902981281280518\n",
      "step 14, loss: 3.769645929336548\n",
      "step 15, loss: 3.360283374786377\n",
      "step 16, loss: 3.7280452251434326\n",
      "step 17, loss: 3.444542407989502\n",
      "step 18, loss: 3.7565982341766357\n",
      "step 19, loss: 3.768284797668457\n",
      "step 20, loss: 4.024630069732666\n",
      "step 21, loss: 3.852309465408325\n",
      "step 22, loss: 3.9537339210510254\n",
      "step 23, loss: 4.049595355987549\n",
      "step 24, loss: 3.8421764373779297\n",
      "step 25, loss: 3.859069585800171\n",
      "step 26, loss: 4.196441173553467\n",
      "step 27, loss: 4.412689208984375\n",
      "step 28, loss: 3.926161289215088\n",
      "step 29, loss: 4.2498555183410645\n",
      "step 30, loss: 3.8310718536376953\n",
      "step 31, loss: 3.6843326091766357\n",
      "step 32, loss: 3.7418904304504395\n",
      "step 33, loss: 3.921957015991211\n",
      "step 34, loss: 3.9777004718780518\n",
      "step 35, loss: 3.472540855407715\n",
      "step 36, loss: 3.4895570278167725\n",
      "step 37, loss: 3.7449729442596436\n",
      "step 38, loss: 3.5953400135040283\n",
      "step 39, loss: 3.6372005939483643\n",
      "step 40, loss: 3.324056625366211\n",
      "step 41, loss: 3.9705798625946045\n",
      "step 42, loss: 3.5239367485046387\n",
      "step 43, loss: 4.103086948394775\n",
      "step 44, loss: 3.552025318145752\n",
      "step 45, loss: 3.5520730018615723\n",
      "step 46, loss: 3.4753599166870117\n",
      "step 47, loss: 3.222496747970581\n",
      "step 48, loss: 3.363715171813965\n",
      "step 49, loss: 3.3498334884643555\n",
      "step 50, loss: 3.4194681644439697\n",
      "step 51, loss: 3.6349668502807617\n",
      "step 52, loss: 3.5329246520996094\n",
      "step 53, loss: 3.4679830074310303\n",
      "step 54, loss: 3.787572145462036\n",
      "step 55, loss: 3.2756967544555664\n",
      "step 56, loss: 3.6066269874572754\n",
      "step 57, loss: 3.966902017593384\n",
      "step 58, loss: 3.984604597091675\n",
      "step 59, loss: 3.5362632274627686\n",
      "step 60, loss: 2.696681022644043\n",
      "step 61, loss: 2.7993133068084717\n",
      "step 62, loss: 2.6776320934295654\n",
      "step 63, loss: 2.7900285720825195\n",
      "step 64, loss: 2.2592711448669434\n",
      "step 65, loss: 2.3994295597076416\n",
      "step 66, loss: 2.369049549102783\n",
      "step 67, loss: 2.2565622329711914\n",
      "step 68, loss: 2.0640594959259033\n",
      "step 69, loss: 2.1157126426696777\n",
      "step 70, loss: 2.297933340072632\n",
      "step 71, loss: 3.096430778503418\n",
      "step 72, loss: 3.931297779083252\n",
      "step 73, loss: 4.31116247177124\n",
      "step 74, loss: 4.210946559906006\n",
      "step 75, loss: 3.932863712310791\n",
      "step 76, loss: 3.7949907779693604\n",
      "step 77, loss: 3.583146095275879\n",
      "step 78, loss: 3.6989312171936035\n",
      "step 79, loss: 4.016499996185303\n",
      "step 80, loss: 3.6766953468322754\n",
      "step 81, loss: 3.9130728244781494\n",
      "step 82, loss: 3.9354286193847656\n",
      "step 83, loss: 3.836116075515747\n",
      "step 84, loss: 2.9918463230133057\n",
      "step 85, loss: 3.6597743034362793\n",
      "step 86, loss: 3.6066672801971436\n",
      "step 87, loss: 3.6849589347839355\n",
      "step 88, loss: 3.94923734664917\n",
      "step 89, loss: 3.626972198486328\n",
      "step 90, loss: 3.8389055728912354\n",
      "step 91, loss: 3.678361415863037\n",
      "step 92, loss: 3.6769542694091797\n",
      "step 93, loss: 4.004406452178955\n",
      "step 94, loss: 3.5366058349609375\n",
      "step 95, loss: 3.1110801696777344\n",
      "step 96, loss: 3.3019888401031494\n",
      "step 97, loss: 3.4389166831970215\n",
      "step 98, loss: 3.6902577877044678\n",
      "step 99, loss: 3.51293683052063\n",
      "step 100, loss: 3.852151393890381\n",
      "step 101, loss: 3.8362486362457275\n",
      "step 102, loss: 3.260122776031494\n",
      "step 103, loss: 3.3492510318756104\n",
      "step 104, loss: 3.3584649562835693\n",
      "step 105, loss: 3.8901431560516357\n",
      "step 106, loss: 3.624730348587036\n",
      "step 107, loss: 3.5322606563568115\n",
      "step 108, loss: 3.711374044418335\n",
      "step 109, loss: 3.4773659706115723\n",
      "step 110, loss: 3.5495574474334717\n",
      "step 111, loss: 3.2117457389831543\n",
      "step 112, loss: 3.4901251792907715\n",
      "step 113, loss: 3.7851760387420654\n",
      "step 114, loss: 3.471632480621338\n",
      "step 115, loss: 3.840730905532837\n",
      "step 116, loss: 3.3700318336486816\n",
      "step 117, loss: 3.4335005283355713\n",
      "step 118, loss: 3.637899398803711\n",
      "step 119, loss: 3.3440074920654297\n",
      "step 120, loss: 3.172036647796631\n",
      "step 121, loss: 3.651392936706543\n",
      "step 122, loss: 3.469815731048584\n",
      "step 123, loss: 3.4386589527130127\n",
      "step 124, loss: 3.4204859733581543\n",
      "step 125, loss: 3.566455841064453\n",
      "step 126, loss: 3.6284189224243164\n",
      "step 127, loss: 3.1312620639801025\n",
      "step 128, loss: 3.40696382522583\n",
      "step 129, loss: 3.820880651473999\n",
      "step 130, loss: 3.561943769454956\n",
      "step 131, loss: 3.89719820022583\n",
      "step 132, loss: 3.5245521068573\n",
      "step 133, loss: 3.871128559112549\n",
      "step 134, loss: 3.3920388221740723\n",
      "step 135, loss: 3.5263712406158447\n",
      "step 136, loss: 3.5234477519989014\n",
      "step 137, loss: 3.5605175495147705\n",
      "step 138, loss: 3.223818778991699\n",
      "step 139, loss: 3.864551544189453\n",
      "step 140, loss: 3.9030826091766357\n",
      "step 141, loss: 3.4689927101135254\n",
      "step 142, loss: 3.497361660003662\n",
      "step 143, loss: 3.4661202430725098\n",
      "step 144, loss: 3.600229263305664\n",
      "step 145, loss: 3.3693950176239014\n",
      "step 146, loss: 3.2628347873687744\n",
      "step 147, loss: 3.5374183654785156\n",
      "step 148, loss: 2.9654195308685303\n",
      "step 149, loss: 3.5477147102355957\n",
      "step 150, loss: 3.482717514038086\n",
      "step 151, loss: 3.486370086669922\n",
      "step 152, loss: 3.3778223991394043\n",
      "step 153, loss: 3.5168099403381348\n",
      "step 154, loss: 3.881915807723999\n",
      "step 155, loss: 3.4418087005615234\n",
      "step 156, loss: 4.048936367034912\n",
      "step 157, loss: 3.5278635025024414\n",
      "step 158, loss: 2.9805779457092285\n",
      "step 159, loss: 3.915405511856079\n",
      "step 160, loss: 3.727454900741577\n",
      "step 161, loss: 3.4457297325134277\n",
      "step 162, loss: 3.3173258304595947\n",
      "step 163, loss: 3.736011028289795\n",
      "step 164, loss: 4.0220465660095215\n",
      "step 165, loss: 3.7571277618408203\n",
      "step 166, loss: 3.169543504714966\n",
      "step 167, loss: 3.6144416332244873\n",
      "step 168, loss: 3.2738254070281982\n",
      "step 169, loss: 3.763929843902588\n",
      "step 170, loss: 3.5341062545776367\n",
      "step 171, loss: 4.172238349914551\n",
      "step 172, loss: 3.3190042972564697\n",
      "step 173, loss: 3.420363426208496\n",
      "step 174, loss: 3.478945016860962\n",
      "step 175, loss: 4.056065559387207\n",
      "step 176, loss: 3.8145480155944824\n",
      "step 177, loss: 4.123391628265381\n",
      "step 178, loss: 3.830005645751953\n",
      "step 179, loss: 3.830704927444458\n",
      "step 180, loss: 3.452928304672241\n",
      "step 181, loss: 3.8455216884613037\n",
      "step 182, loss: 4.017203330993652\n",
      "step 183, loss: 3.508289098739624\n",
      "step 184, loss: 2.9747226238250732\n",
      "step 185, loss: 2.555148124694824\n",
      "step 186, loss: 2.6434834003448486\n",
      "step 187, loss: 2.0726089477539062\n",
      "step 188, loss: 2.0656771659851074\n",
      "step 189, loss: 2.471951484680176\n",
      "step 190, loss: 1.7371000051498413\n",
      "step 191, loss: 2.0925166606903076\n",
      "step 192, loss: 2.0232620239257812\n",
      "step 193, loss: 1.6254154443740845\n",
      "step 194, loss: 1.7794630527496338\n",
      "step 195, loss: 2.2304093837738037\n",
      "step 196, loss: 2.205974578857422\n",
      "step 197, loss: 1.9798623323440552\n",
      "step 198, loss: 1.4298864603042603\n",
      "step 199, loss: 2.005786657333374\n",
      "step 200, loss: 1.5862879753112793\n",
      "step 201, loss: 2.391425609588623\n",
      "step 202, loss: 1.9550946950912476\n",
      "step 203, loss: 1.5951298475265503\n",
      "step 204, loss: 2.221766233444214\n",
      "step 205, loss: 2.307734727859497\n",
      "step 206, loss: 2.190783977508545\n",
      "step 207, loss: 2.232785701751709\n",
      "step 208, loss: 2.202272891998291\n",
      "step 209, loss: 2.2123677730560303\n",
      "step 210, loss: 2.522430658340454\n",
      "step 211, loss: 3.0947861671447754\n",
      "step 212, loss: 4.056189060211182\n",
      "step 213, loss: 3.9227139949798584\n",
      "step 214, loss: 4.142317295074463\n",
      "step 215, loss: 4.249129295349121\n",
      "step 216, loss: 3.7483115196228027\n",
      "step 217, loss: 3.4901633262634277\n",
      "step 218, loss: 3.696120500564575\n",
      "step 219, loss: 3.4960622787475586\n",
      "step 220, loss: 3.281930685043335\n",
      "step 221, loss: 3.67781662940979\n",
      "step 222, loss: 3.922717332839966\n",
      "step 223, loss: 3.9744551181793213\n",
      "step 224, loss: 3.78024959564209\n",
      "step 225, loss: 3.683441400527954\n",
      "step 226, loss: 4.119688034057617\n",
      "step 227, loss: 3.657815456390381\n",
      "step 228, loss: 3.7191550731658936\n",
      "step 229, loss: 4.065670967102051\n",
      "step 230, loss: 3.934333562850952\n",
      "step 231, loss: 3.856037139892578\n",
      "step 232, loss: 3.6149585247039795\n",
      "step 233, loss: 3.5742146968841553\n",
      "step 234, loss: 3.5459909439086914\n",
      "step 235, loss: 3.8959856033325195\n",
      "step 236, loss: 3.750671625137329\n",
      "step 237, loss: 3.950525999069214\n",
      "step 238, loss: 4.036318778991699\n",
      "step 239, loss: 3.8910837173461914\n",
      "step 240, loss: 3.3543550968170166\n",
      "step 241, loss: 3.6326522827148438\n",
      "step 242, loss: 3.9623093605041504\n",
      "step 243, loss: 3.491945266723633\n",
      "step 244, loss: 3.6267290115356445\n",
      "step 245, loss: 3.591416597366333\n",
      "step 246, loss: 3.7827343940734863\n",
      "step 247, loss: 3.5738563537597656\n",
      "step 248, loss: 3.6781005859375\n",
      "step 249, loss: 3.548685312271118\n",
      "step 250, loss: 3.6095433235168457\n",
      "step 251, loss: 3.2644572257995605\n",
      "step 252, loss: 4.0835700035095215\n",
      "step 253, loss: 3.747622489929199\n",
      "step 254, loss: 3.680690288543701\n",
      "step 255, loss: 3.875009298324585\n",
      "step 256, loss: 3.6223978996276855\n",
      "step 257, loss: 3.6961357593536377\n",
      "step 258, loss: 3.3441526889801025\n",
      "step 259, loss: 3.620290994644165\n",
      "step 260, loss: 3.457099437713623\n",
      "step 261, loss: 3.6763739585876465\n",
      "step 262, loss: 3.4150068759918213\n",
      "step 263, loss: 3.908295154571533\n",
      "step 264, loss: 3.0749473571777344\n",
      "step 265, loss: 3.7882306575775146\n",
      "step 266, loss: 4.027922630310059\n",
      "step 267, loss: 3.5505361557006836\n",
      "step 268, loss: 3.4822921752929688\n",
      "step 269, loss: 3.420527219772339\n",
      "step 270, loss: 3.118865966796875\n",
      "step 271, loss: 3.0732898712158203\n",
      "step 272, loss: 3.628629684448242\n",
      "step 273, loss: 3.5094213485717773\n",
      "step 274, loss: 3.259916305541992\n",
      "step 275, loss: 3.6291110515594482\n",
      "step 276, loss: 3.374591827392578\n",
      "step 277, loss: 3.3626010417938232\n",
      "step 278, loss: 3.4366488456726074\n",
      "step 279, loss: 4.122012615203857\n",
      "step 280, loss: 3.6704294681549072\n",
      "step 281, loss: 3.5708792209625244\n",
      "step 282, loss: 3.334912061691284\n",
      "step 283, loss: 3.467550039291382\n",
      "step 284, loss: 3.6836228370666504\n",
      "step 285, loss: 3.6447558403015137\n",
      "step 286, loss: 3.765631914138794\n",
      "step 287, loss: 3.452319622039795\n",
      "step 288, loss: 3.7109968662261963\n",
      "step 289, loss: 3.5469603538513184\n",
      "step 290, loss: 3.380568027496338\n",
      "step 291, loss: 3.8563365936279297\n",
      "step 292, loss: 3.3713152408599854\n",
      "step 293, loss: 3.5495972633361816\n",
      "step 294, loss: 3.790715456008911\n",
      "step 295, loss: 3.542985439300537\n",
      "step 296, loss: 3.5134708881378174\n",
      "step 297, loss: 4.078383922576904\n",
      "step 298, loss: 3.687366485595703\n",
      "step 299, loss: 3.217869997024536\n",
      "step 300, loss: 3.915531635284424\n",
      "step 301, loss: 3.5043630599975586\n",
      "step 302, loss: 3.438692569732666\n",
      "step 303, loss: 3.4918010234832764\n",
      "step 304, loss: 3.4375784397125244\n",
      "step 305, loss: 3.4406325817108154\n",
      "step 306, loss: 3.4406521320343018\n",
      "step 307, loss: 3.410003900527954\n",
      "step 308, loss: 3.1407625675201416\n",
      "step 309, loss: 3.269932985305786\n",
      "step 310, loss: 2.635622024536133\n",
      "step 311, loss: 2.924264907836914\n",
      "step 312, loss: 3.071956157684326\n",
      "step 313, loss: 2.949089288711548\n",
      "step 314, loss: 3.091784715652466\n",
      "step 315, loss: 2.801372528076172\n",
      "step 316, loss: 3.3319671154022217\n",
      "step 317, loss: 2.272461414337158\n",
      "step 318, loss: 2.7853822708129883\n",
      "step 319, loss: 2.6378870010375977\n",
      "step 320, loss: 2.332951784133911\n",
      "step 321, loss: 2.265305995941162\n",
      "step 322, loss: 2.3263795375823975\n",
      "step 323, loss: 2.213219165802002\n",
      "step 324, loss: 2.610074520111084\n",
      "step 325, loss: 2.8864877223968506\n",
      "step 326, loss: 2.830331325531006\n",
      "step 327, loss: 2.915834903717041\n",
      "step 328, loss: 2.2651684284210205\n",
      "step 329, loss: 2.4458205699920654\n",
      "step 330, loss: 3.895618200302124\n",
      "step 331, loss: 4.003288269042969\n",
      "step 332, loss: 3.841787576675415\n",
      "step 333, loss: 4.084228038787842\n",
      "step 334, loss: 4.53552770614624\n",
      "step 335, loss: 3.92529296875\n",
      "step 336, loss: 3.3404784202575684\n",
      "step 337, loss: 3.3447062969207764\n",
      "step 338, loss: 3.3720273971557617\n",
      "step 339, loss: 3.4239418506622314\n",
      "step 340, loss: 3.413266897201538\n",
      "step 341, loss: 3.6079514026641846\n",
      "step 342, loss: 3.839704751968384\n",
      "step 343, loss: 4.090755939483643\n",
      "step 344, loss: 3.933974027633667\n",
      "step 345, loss: 3.724358558654785\n",
      "step 346, loss: 3.568781852722168\n",
      "step 347, loss: 4.3713555335998535\n",
      "step 348, loss: 4.008012771606445\n",
      "step 349, loss: 3.8514440059661865\n",
      "step 350, loss: 3.9869213104248047\n",
      "step 351, loss: 3.593369722366333\n",
      "step 352, loss: 3.524534225463867\n",
      "step 353, loss: 4.065018653869629\n",
      "step 354, loss: 3.665339231491089\n",
      "step 355, loss: 3.1804568767547607\n",
      "step 356, loss: 3.3593127727508545\n",
      "step 357, loss: 3.792102575302124\n",
      "step 358, loss: 3.608273983001709\n",
      "step 359, loss: 3.8550734519958496\n",
      "step 360, loss: 3.751208782196045\n",
      "step 361, loss: 4.027775287628174\n",
      "step 362, loss: 3.7156217098236084\n",
      "step 363, loss: 3.9894397258758545\n",
      "step 364, loss: 3.6307711601257324\n",
      "step 365, loss: 3.974048137664795\n",
      "step 366, loss: 3.8361856937408447\n",
      "step 367, loss: 4.055301666259766\n",
      "step 368, loss: 3.6657299995422363\n",
      "step 369, loss: 4.030885696411133\n",
      "step 370, loss: 3.964778423309326\n",
      "step 371, loss: 3.949408531188965\n",
      "step 372, loss: 3.5880932807922363\n",
      "step 373, loss: 3.957054853439331\n",
      "step 374, loss: 4.143430233001709\n",
      "step 375, loss: 4.030645370483398\n",
      "step 376, loss: 3.68457293510437\n",
      "step 377, loss: 3.776193618774414\n",
      "step 378, loss: 4.123703956604004\n",
      "step 379, loss: 3.6450283527374268\n",
      "step 380, loss: 3.456285238265991\n",
      "step 381, loss: 3.5647130012512207\n",
      "step 382, loss: 3.9515717029571533\n",
      "step 383, loss: 4.610302448272705\n",
      "step 384, loss: 4.488662242889404\n",
      "step 385, loss: 3.7284162044525146\n",
      "step 386, loss: 3.8507680892944336\n",
      "step 387, loss: 3.6358165740966797\n",
      "step 388, loss: 3.916224241256714\n",
      "step 389, loss: 3.9541006088256836\n",
      "step 390, loss: 3.714979648590088\n",
      "step 391, loss: 3.9619767665863037\n",
      "step 392, loss: 4.326860427856445\n",
      "step 393, loss: 4.014553070068359\n",
      "step 394, loss: 3.851604461669922\n",
      "step 395, loss: 4.0465288162231445\n",
      "step 396, loss: 4.152050495147705\n",
      "step 397, loss: 3.912039041519165\n",
      "step 398, loss: 3.7978227138519287\n",
      "step 399, loss: 4.087139129638672\n",
      "step 400, loss: 3.667872428894043\n",
      "step 401, loss: 3.783198595046997\n",
      "step 402, loss: 3.7371630668640137\n",
      "step 403, loss: 4.56722354888916\n",
      "step 404, loss: 4.183587074279785\n",
      "step 405, loss: 3.7691214084625244\n",
      "step 406, loss: 3.739630699157715\n",
      "step 407, loss: 4.153107643127441\n",
      "step 408, loss: 3.7063183784484863\n",
      "step 409, loss: 4.090923309326172\n",
      "step 410, loss: 4.114226341247559\n",
      "step 411, loss: 3.7370007038116455\n",
      "step 412, loss: 3.8551247119903564\n",
      "step 413, loss: 3.7912967205047607\n",
      "step 414, loss: 3.9600260257720947\n",
      "step 415, loss: 3.971165895462036\n",
      "step 416, loss: 4.013662815093994\n",
      "step 417, loss: 3.8982272148132324\n",
      "step 418, loss: 3.6620380878448486\n",
      "step 419, loss: 3.751325845718384\n",
      "step 420, loss: 3.9874634742736816\n",
      "step 421, loss: 3.4712398052215576\n",
      "step 422, loss: 3.114121437072754\n",
      "step 423, loss: 2.785341501235962\n",
      "step 424, loss: 2.733611583709717\n",
      "step 425, loss: 2.6415557861328125\n",
      "step 426, loss: 2.595508337020874\n",
      "step 427, loss: 2.4223313331604004\n",
      "step 428, loss: 2.6672918796539307\n",
      "step 429, loss: 2.4245615005493164\n",
      "step 430, loss: 2.5715503692626953\n",
      "step 431, loss: 2.1062867641448975\n",
      "step 432, loss: 2.1237027645111084\n",
      "step 433, loss: 2.059551954269409\n",
      "step 434, loss: 1.8940515518188477\n",
      "step 435, loss: 2.0326709747314453\n",
      "step 436, loss: 2.394726276397705\n",
      "step 437, loss: 3.2929792404174805\n",
      "step 438, loss: 4.059514999389648\n",
      "step 439, loss: 4.210455417633057\n",
      "step 440, loss: 4.33447790145874\n",
      "step 441, loss: 4.44453239440918\n",
      "step 442, loss: 3.970677137374878\n",
      "step 443, loss: 3.678590774536133\n",
      "step 444, loss: 3.5975828170776367\n",
      "step 445, loss: 3.810865879058838\n",
      "step 446, loss: 3.782268762588501\n",
      "step 447, loss: 3.832618236541748\n",
      "step 448, loss: 3.8597049713134766\n",
      "step 449, loss: 3.6692514419555664\n",
      "step 450, loss: 3.9107139110565186\n",
      "step 451, loss: 3.6346540451049805\n",
      "step 452, loss: 3.8805992603302\n",
      "step 453, loss: 3.8938381671905518\n",
      "step 454, loss: 4.0140299797058105\n",
      "step 455, loss: 3.800388813018799\n",
      "step 456, loss: 3.665252685546875\n",
      "step 457, loss: 3.7545037269592285\n",
      "step 458, loss: 3.5778439044952393\n",
      "step 459, loss: 3.583081007003784\n",
      "step 460, loss: 3.7344110012054443\n",
      "step 461, loss: 3.6108322143554688\n",
      "step 462, loss: 3.6683170795440674\n",
      "step 463, loss: 4.141999244689941\n",
      "step 464, loss: 3.495563507080078\n",
      "step 465, loss: 3.5793490409851074\n",
      "step 466, loss: 4.0572428703308105\n",
      "step 467, loss: 3.6329917907714844\n",
      "step 468, loss: 4.148058891296387\n",
      "step 469, loss: 3.6611576080322266\n",
      "step 470, loss: 3.695420503616333\n",
      "step 471, loss: 4.079507827758789\n",
      "step 472, loss: 4.140187740325928\n",
      "step 473, loss: 3.6544299125671387\n",
      "step 474, loss: 3.8369147777557373\n",
      "step 475, loss: 4.179330825805664\n",
      "step 476, loss: 4.033595561981201\n",
      "step 477, loss: 4.374311923980713\n",
      "step 478, loss: 3.715914726257324\n",
      "step 479, loss: 4.347498416900635\n",
      "step 480, loss: 4.4570770263671875\n",
      "step 481, loss: 4.2939534187316895\n",
      "step 482, loss: 3.563762664794922\n",
      "step 483, loss: 2.5067880153656006\n",
      "step 484, loss: 2.4366939067840576\n",
      "step 485, loss: 2.612133502960205\n",
      "step 486, loss: 2.497227907180786\n",
      "step 487, loss: 2.2255868911743164\n",
      "step 488, loss: 2.1981711387634277\n",
      "step 489, loss: 1.5911630392074585\n",
      "step 490, loss: 1.8464534282684326\n",
      "step 491, loss: 1.9832874536514282\n",
      "step 492, loss: 2.6792821884155273\n",
      "step 493, loss: 2.232835054397583\n",
      "step 494, loss: 2.7233498096466064\n",
      "step 495, loss: 2.3693947792053223\n",
      "step 496, loss: 2.5740814208984375\n",
      "step 497, loss: 3.428053617477417\n",
      "step 498, loss: 5.243615627288818\n",
      "step 499, loss: 4.298851490020752\n",
      "step 500, loss: 3.9070873260498047\n",
      "step 501, loss: 3.8429834842681885\n",
      "step 502, loss: 4.169362545013428\n",
      "step 503, loss: 4.098637104034424\n",
      "step 504, loss: 3.883331775665283\n",
      "step 505, loss: 3.7732937335968018\n",
      "step 506, loss: 3.891023874282837\n",
      "step 507, loss: 4.012218952178955\n",
      "step 508, loss: 3.251950740814209\n",
      "step 509, loss: 3.7180569171905518\n",
      "step 510, loss: 3.5321907997131348\n",
      "step 511, loss: 3.882546901702881\n",
      "step 512, loss: 3.657583236694336\n",
      "step 513, loss: 3.527758836746216\n",
      "step 514, loss: 3.902127504348755\n",
      "step 515, loss: 3.5624887943267822\n",
      "step 516, loss: 3.3993453979492188\n",
      "step 517, loss: 3.2594492435455322\n",
      "step 518, loss: 3.652682304382324\n",
      "step 519, loss: 3.0675337314605713\n",
      "step 520, loss: 3.5403642654418945\n",
      "step 521, loss: 3.5405702590942383\n",
      "step 522, loss: 3.5584940910339355\n",
      "step 523, loss: 3.6429381370544434\n",
      "step 524, loss: 3.6127407550811768\n",
      "step 525, loss: 3.6248676776885986\n",
      "step 526, loss: 3.944932460784912\n",
      "step 527, loss: 3.523122787475586\n",
      "step 528, loss: 3.2339980602264404\n",
      "step 529, loss: 3.6043848991394043\n",
      "step 530, loss: 3.6849398612976074\n",
      "step 531, loss: 3.9849772453308105\n",
      "step 532, loss: 3.4550700187683105\n",
      "step 533, loss: 3.9385876655578613\n",
      "step 534, loss: 3.757960796356201\n",
      "step 535, loss: 3.8309149742126465\n",
      "step 536, loss: 3.6349728107452393\n",
      "step 537, loss: 4.031449794769287\n",
      "step 538, loss: 3.7954535484313965\n",
      "step 539, loss: 3.5758726596832275\n",
      "step 540, loss: 3.3789029121398926\n",
      "step 541, loss: 4.274703502655029\n",
      "step 542, loss: 3.783093214035034\n",
      "step 543, loss: 4.201607704162598\n",
      "step 544, loss: 4.096760272979736\n",
      "step 545, loss: 3.798255443572998\n",
      "step 546, loss: 3.779770851135254\n",
      "step 547, loss: 3.823575735092163\n",
      "step 548, loss: 3.755910634994507\n",
      "step 549, loss: 4.109123229980469\n",
      "step 550, loss: 3.576059103012085\n",
      "step 551, loss: 4.195977687835693\n",
      "step 552, loss: 4.07172155380249\n",
      "step 553, loss: 3.9280128479003906\n",
      "step 554, loss: 4.01108980178833\n",
      "step 555, loss: 4.142632961273193\n",
      "step 556, loss: 3.957315444946289\n",
      "step 557, loss: 4.00299072265625\n",
      "step 558, loss: 4.40172004699707\n",
      "step 559, loss: 3.963477611541748\n",
      "step 560, loss: 3.963923454284668\n",
      "step 561, loss: 3.873481273651123\n",
      "step 562, loss: 3.6257123947143555\n",
      "step 563, loss: 4.1423420906066895\n",
      "step 564, loss: 4.236040115356445\n",
      "step 565, loss: 3.930570363998413\n",
      "step 566, loss: 3.8748905658721924\n",
      "step 567, loss: 4.912047863006592\n",
      "step 568, loss: 3.8882946968078613\n",
      "step 569, loss: 4.238656997680664\n",
      "step 570, loss: 4.075999736785889\n",
      "step 571, loss: 3.821338415145874\n",
      "step 572, loss: 4.276165962219238\n",
      "step 573, loss: 4.066397190093994\n",
      "step 574, loss: 4.146881580352783\n",
      "step 575, loss: 3.8263659477233887\n",
      "step 576, loss: 4.429194450378418\n",
      "step 577, loss: 3.919954299926758\n",
      "step 578, loss: 4.0641865730285645\n",
      "step 579, loss: 3.866708517074585\n",
      "step 580, loss: 4.072772979736328\n",
      "step 581, loss: 4.1521124839782715\n",
      "step 582, loss: 4.261763095855713\n",
      "step 583, loss: 4.38852071762085\n",
      "step 584, loss: 4.470824241638184\n",
      "step 585, loss: 4.720330238342285\n",
      "step 586, loss: 3.929429531097412\n",
      "step 587, loss: 3.308081865310669\n",
      "step 588, loss: 2.80063533782959\n",
      "step 589, loss: 2.5064644813537598\n",
      "step 590, loss: 2.222390651702881\n",
      "step 591, loss: 2.341223955154419\n",
      "step 592, loss: 2.1248390674591064\n",
      "step 593, loss: 2.2558486461639404\n",
      "step 594, loss: 2.3002288341522217\n",
      "step 595, loss: 2.353976011276245\n",
      "step 596, loss: 2.4590463638305664\n",
      "step 597, loss: 2.1647427082061768\n",
      "step 598, loss: 2.2799899578094482\n",
      "step 599, loss: 2.7288217544555664\n",
      "step 600, loss: 2.107506036758423\n",
      "step 601, loss: 1.7918062210083008\n",
      "step 602, loss: 1.5861550569534302\n",
      "step 603, loss: 2.4237453937530518\n",
      "step 604, loss: 1.9475743770599365\n",
      "step 605, loss: 2.066420078277588\n",
      "step 606, loss: 1.8711649179458618\n",
      "step 607, loss: 1.8814091682434082\n",
      "step 608, loss: 3.0055248737335205\n",
      "step 609, loss: 3.7649178504943848\n",
      "step 610, loss: 3.5435571670532227\n",
      "step 611, loss: 3.9620752334594727\n",
      "step 612, loss: 4.043715476989746\n",
      "step 613, loss: 4.527390003204346\n",
      "step 614, loss: 4.420973777770996\n",
      "step 615, loss: 4.127490520477295\n",
      "step 616, loss: 3.6478569507598877\n",
      "step 617, loss: 3.6514811515808105\n",
      "step 618, loss: 3.6024203300476074\n",
      "step 619, loss: 3.441041946411133\n",
      "step 620, loss: 3.5324881076812744\n",
      "step 621, loss: 4.053297519683838\n",
      "step 622, loss: 3.909893274307251\n",
      "step 623, loss: 3.9681787490844727\n",
      "step 624, loss: 4.010618686676025\n",
      "step 625, loss: 3.8900368213653564\n",
      "step 626, loss: 3.4181337356567383\n",
      "step 627, loss: 3.7857375144958496\n",
      "step 628, loss: 3.8867313861846924\n",
      "step 629, loss: 3.814533233642578\n",
      "step 630, loss: 3.719783306121826\n",
      "step 631, loss: 3.5574779510498047\n",
      "step 632, loss: 4.291822910308838\n",
      "step 633, loss: 3.589146137237549\n",
      "step 634, loss: 3.674550771713257\n",
      "step 635, loss: 3.800379514694214\n",
      "step 636, loss: 3.647279739379883\n",
      "step 637, loss: 3.6321229934692383\n",
      "step 638, loss: 3.5724079608917236\n",
      "step 639, loss: 3.9237027168273926\n",
      "step 640, loss: 3.803994655609131\n",
      "step 641, loss: 3.4391696453094482\n",
      "step 642, loss: 3.667341470718384\n",
      "step 643, loss: 3.402773380279541\n",
      "step 644, loss: 3.645404100418091\n",
      "step 645, loss: 4.093962669372559\n",
      "step 646, loss: 4.295449256896973\n",
      "step 647, loss: 3.7909977436065674\n",
      "step 648, loss: 3.118610143661499\n",
      "step 649, loss: 3.518329620361328\n",
      "step 650, loss: 3.4963717460632324\n",
      "step 651, loss: 3.75608229637146\n",
      "step 652, loss: 3.1692230701446533\n",
      "step 653, loss: 3.439990997314453\n",
      "step 654, loss: 3.4664413928985596\n",
      "step 655, loss: 3.6477322578430176\n",
      "step 656, loss: 4.031330108642578\n",
      "step 657, loss: 3.736116647720337\n",
      "step 658, loss: 3.790050506591797\n",
      "step 659, loss: 3.9540796279907227\n",
      "step 660, loss: 3.6788227558135986\n",
      "step 661, loss: 3.7628610134124756\n",
      "step 662, loss: 4.3613739013671875\n",
      "step 663, loss: 4.127087593078613\n",
      "step 664, loss: 3.639202117919922\n",
      "step 665, loss: 3.653322696685791\n",
      "step 666, loss: 3.9973511695861816\n",
      "step 667, loss: 3.5479118824005127\n",
      "step 668, loss: 3.572730779647827\n",
      "step 669, loss: 3.3011515140533447\n",
      "step 670, loss: 3.950080156326294\n",
      "step 671, loss: 3.6329259872436523\n",
      "step 672, loss: 3.4015212059020996\n",
      "step 673, loss: 3.796496629714966\n",
      "step 674, loss: 3.808434009552002\n",
      "step 675, loss: 4.216423988342285\n",
      "step 676, loss: 3.8097615242004395\n",
      "step 677, loss: 3.6188082695007324\n",
      "step 678, loss: 3.760928153991699\n",
      "step 679, loss: 3.768657684326172\n",
      "step 680, loss: 3.8778774738311768\n",
      "step 681, loss: 3.8627231121063232\n",
      "step 682, loss: 4.212898254394531\n",
      "step 683, loss: 3.902040719985962\n",
      "step 684, loss: 4.043903827667236\n",
      "step 685, loss: 3.798386573791504\n",
      "step 686, loss: 3.689613103866577\n",
      "step 687, loss: 3.592243194580078\n",
      "step 688, loss: 3.550074338912964\n",
      "step 689, loss: 3.664214849472046\n",
      "step 690, loss: 4.033377647399902\n",
      "step 691, loss: 3.891733169555664\n",
      "step 692, loss: 3.961364507675171\n",
      "step 693, loss: 3.350543975830078\n",
      "step 694, loss: 3.282074213027954\n",
      "step 695, loss: 3.285740613937378\n",
      "step 696, loss: 3.5948431491851807\n",
      "step 697, loss: 4.082546234130859\n",
      "step 698, loss: 3.8209853172302246\n",
      "step 699, loss: 3.9762730598449707\n",
      "step 700, loss: 4.203505039215088\n",
      "step 701, loss: 3.869229316711426\n",
      "step 702, loss: 3.7988593578338623\n",
      "step 703, loss: 3.8196089267730713\n",
      "step 704, loss: 4.1489458084106445\n",
      "step 705, loss: 3.8833792209625244\n",
      "step 706, loss: 3.8958187103271484\n",
      "step 707, loss: 4.074216842651367\n",
      "step 708, loss: 3.6208157539367676\n",
      "step 709, loss: 4.249050140380859\n",
      "step 710, loss: 4.348669052124023\n",
      "step 711, loss: 4.241776466369629\n",
      "step 712, loss: 4.831262588500977\n",
      "step 713, loss: 4.020849227905273\n",
      "step 714, loss: 3.8445661067962646\n",
      "step 715, loss: 4.082376956939697\n",
      "step 716, loss: 4.390486240386963\n",
      "step 717, loss: 3.7416064739227295\n",
      "step 718, loss: 3.467209577560425\n",
      "step 719, loss: 2.2703044414520264\n",
      "step 720, loss: 2.0713794231414795\n",
      "step 721, loss: 2.794976234436035\n",
      "step 722, loss: 2.130811929702759\n",
      "step 723, loss: 2.3248391151428223\n",
      "step 724, loss: 2.3775031566619873\n",
      "step 725, loss: 1.9037039279937744\n",
      "step 726, loss: 2.4873948097229004\n",
      "step 727, loss: 2.0260226726531982\n",
      "step 728, loss: 2.28780460357666\n",
      "step 729, loss: 2.3555471897125244\n",
      "step 730, loss: 2.4499120712280273\n",
      "step 731, loss: 2.340156316757202\n",
      "step 732, loss: 2.9128904342651367\n",
      "step 733, loss: 1.7733386754989624\n",
      "step 734, loss: 1.7094619274139404\n",
      "step 735, loss: 1.7826707363128662\n",
      "step 736, loss: 1.5086814165115356\n",
      "step 737, loss: 1.9309108257293701\n",
      "step 738, loss: 1.5045228004455566\n",
      "step 739, loss: 2.046846389770508\n",
      "step 740, loss: 2.1035780906677246\n",
      "step 741, loss: 1.9234085083007812\n",
      "step 742, loss: 1.8690003156661987\n",
      "step 743, loss: 2.7296526432037354\n",
      "step 744, loss: 2.1054177284240723\n",
      "step 745, loss: 1.889894962310791\n",
      "step 746, loss: 2.184779167175293\n",
      "step 747, loss: 1.5841727256774902\n",
      "step 748, loss: 3.632608413696289\n",
      "step 749, loss: 3.7122864723205566\n",
      "step 750, loss: 3.9098424911499023\n",
      "step 751, loss: 3.155183792114258\n",
      "step 752, loss: 3.6209635734558105\n",
      "step 753, loss: 3.6472811698913574\n",
      "step 754, loss: 3.320497512817383\n",
      "step 755, loss: 4.050154209136963\n",
      "step 756, loss: 4.08704137802124\n",
      "step 757, loss: 3.9600605964660645\n",
      "step 758, loss: 4.072396755218506\n",
      "step 759, loss: 3.158259630203247\n",
      "step 760, loss: 3.837481737136841\n",
      "step 761, loss: 3.604170322418213\n",
      "step 762, loss: 3.710975408554077\n",
      "step 763, loss: 3.631944417953491\n",
      "step 764, loss: 3.9696061611175537\n",
      "step 765, loss: 3.8795619010925293\n",
      "step 766, loss: 4.216887474060059\n",
      "step 767, loss: 3.9916415214538574\n",
      "step 768, loss: 3.496034860610962\n",
      "step 769, loss: 3.81742525100708\n",
      "step 770, loss: 3.6683402061462402\n",
      "step 771, loss: 3.6865060329437256\n",
      "step 772, loss: 3.5627706050872803\n",
      "step 773, loss: 3.529644727706909\n",
      "step 774, loss: 3.4382688999176025\n",
      "step 775, loss: 3.3702104091644287\n",
      "step 776, loss: 3.598891258239746\n",
      "step 777, loss: 3.527250289916992\n",
      "step 778, loss: 3.4349119663238525\n",
      "step 779, loss: 3.665630340576172\n",
      "step 780, loss: 4.146248817443848\n",
      "step 781, loss: 4.414383888244629\n",
      "step 782, loss: 3.9053289890289307\n",
      "step 783, loss: 3.799431085586548\n",
      "step 784, loss: 4.210236072540283\n",
      "step 785, loss: 4.227349281311035\n",
      "step 786, loss: 3.9294121265411377\n",
      "step 787, loss: 3.6590161323547363\n",
      "step 788, loss: 3.583869218826294\n",
      "step 789, loss: 3.9151227474212646\n",
      "step 790, loss: 3.879431962966919\n",
      "step 791, loss: 3.7275350093841553\n",
      "step 792, loss: 3.6814067363739014\n",
      "step 793, loss: 4.075175762176514\n",
      "step 794, loss: 3.99320650100708\n",
      "step 795, loss: 3.8068368434906006\n",
      "step 796, loss: 4.278674125671387\n",
      "step 797, loss: 3.6472227573394775\n",
      "step 798, loss: 3.8384757041931152\n",
      "step 799, loss: 3.7201995849609375\n",
      "step 800, loss: 3.9387903213500977\n",
      "step 801, loss: 3.5719094276428223\n",
      "step 802, loss: 3.8812386989593506\n",
      "step 803, loss: 3.835305690765381\n",
      "step 804, loss: 3.67055082321167\n",
      "step 805, loss: 3.904442071914673\n",
      "step 806, loss: 3.7956109046936035\n",
      "step 807, loss: 3.374270439147949\n",
      "step 808, loss: 3.7210915088653564\n",
      "step 809, loss: 4.197177886962891\n",
      "step 810, loss: 3.756105422973633\n",
      "step 811, loss: 4.227240085601807\n",
      "step 812, loss: 3.278204917907715\n",
      "step 813, loss: 4.357812404632568\n",
      "step 814, loss: 4.415260314941406\n",
      "step 815, loss: 4.39253044128418\n",
      "step 816, loss: 4.166640281677246\n",
      "step 817, loss: 4.511039733886719\n",
      "step 818, loss: 4.58513879776001\n",
      "step 819, loss: 4.592181205749512\n",
      "step 820, loss: 4.094411849975586\n",
      "step 821, loss: 3.8335094451904297\n",
      "step 822, loss: 4.1891584396362305\n",
      "step 823, loss: 4.306501865386963\n",
      "step 824, loss: 4.363978862762451\n",
      "step 825, loss: 3.9195618629455566\n",
      "step 826, loss: 4.263906478881836\n",
      "step 827, loss: 4.164004325866699\n",
      "step 828, loss: 4.063347339630127\n",
      "step 829, loss: 4.235785007476807\n",
      "step 830, loss: 3.77512526512146\n",
      "step 831, loss: 4.136056423187256\n",
      "step 832, loss: 3.9374887943267822\n",
      "step 833, loss: 3.903395891189575\n",
      "step 834, loss: 4.332926273345947\n",
      "step 835, loss: 4.428601264953613\n",
      "step 836, loss: 3.8097994327545166\n",
      "step 837, loss: 3.9480702877044678\n",
      "step 838, loss: 4.514624118804932\n",
      "step 839, loss: 4.116913795471191\n",
      "step 840, loss: 4.316734790802002\n",
      "step 841, loss: 4.14833927154541\n",
      "step 842, loss: 4.431606769561768\n",
      "step 843, loss: 4.474264621734619\n",
      "step 844, loss: 3.793363094329834\n",
      "step 845, loss: 4.287715911865234\n",
      "step 846, loss: 4.185088634490967\n",
      "step 847, loss: 3.976001262664795\n",
      "step 848, loss: 3.592956066131592\n",
      "step 849, loss: 3.8037896156311035\n",
      "step 850, loss: 4.406407356262207\n",
      "step 851, loss: 3.67012619972229\n",
      "step 852, loss: 3.6224093437194824\n",
      "step 853, loss: 3.8627796173095703\n",
      "step 854, loss: 4.040271759033203\n",
      "step 855, loss: 4.243171691894531\n",
      "step 856, loss: 4.154642581939697\n",
      "step 857, loss: 3.966792106628418\n",
      "step 858, loss: 4.100253105163574\n",
      "step 859, loss: 3.9071950912475586\n",
      "step 860, loss: 3.682748317718506\n",
      "step 861, loss: 3.7710371017456055\n",
      "step 862, loss: 4.0621795654296875\n",
      "step 863, loss: 4.01143217086792\n",
      "step 864, loss: 4.1969523429870605\n",
      "step 865, loss: 3.6642343997955322\n",
      "step 866, loss: 4.206234931945801\n",
      "step 867, loss: 3.930203676223755\n",
      "step 868, loss: 3.6423864364624023\n",
      "step 869, loss: 4.232278347015381\n",
      "step 870, loss: 3.868908405303955\n",
      "step 871, loss: 3.751744270324707\n",
      "step 872, loss: 3.875422716140747\n",
      "step 873, loss: 3.8231940269470215\n",
      "step 874, loss: 4.078047752380371\n",
      "step 875, loss: 4.129302501678467\n",
      "step 876, loss: 3.839818000793457\n",
      "step 877, loss: 4.438161849975586\n",
      "step 878, loss: 4.5985941886901855\n",
      "step 879, loss: 4.126828193664551\n",
      "step 880, loss: 4.2770562171936035\n",
      "step 881, loss: 4.21787691116333\n",
      "step 882, loss: 4.865877628326416\n",
      "step 883, loss: 3.6383750438690186\n",
      "step 884, loss: 4.128840923309326\n",
      "step 885, loss: 4.464965343475342\n",
      "step 886, loss: 4.031455993652344\n",
      "step 887, loss: 4.040080547332764\n",
      "step 888, loss: 4.318881511688232\n",
      "step 889, loss: 4.140003681182861\n",
      "step 890, loss: 4.0578107833862305\n",
      "step 891, loss: 4.095911502838135\n",
      "step 892, loss: 3.842376947402954\n",
      "step 893, loss: 4.446636199951172\n",
      "step 894, loss: 4.892249584197998\n",
      "step 895, loss: 4.52211332321167\n",
      "step 896, loss: 4.297004222869873\n",
      "step 897, loss: 4.204187870025635\n",
      "step 898, loss: 4.080589294433594\n",
      "step 899, loss: 4.302992343902588\n",
      "step 900, loss: 4.371089935302734\n",
      "step 901, loss: 4.014055252075195\n",
      "step 902, loss: 4.042732238769531\n",
      "step 903, loss: 3.9182188510894775\n",
      "step 904, loss: 4.095800399780273\n",
      "step 905, loss: 3.8671417236328125\n",
      "step 906, loss: 4.595088005065918\n",
      "step 907, loss: 4.789696216583252\n",
      "step 908, loss: 4.234559535980225\n",
      "step 909, loss: 4.276591777801514\n",
      "step 910, loss: 3.6912662982940674\n",
      "step 911, loss: 3.738785743713379\n",
      "step 912, loss: 3.7731192111968994\n",
      "step 913, loss: 4.244334697723389\n",
      "step 914, loss: 4.0549468994140625\n",
      "step 915, loss: 3.686760187149048\n",
      "step 916, loss: 3.7830655574798584\n",
      "step 917, loss: 4.097479820251465\n",
      "step 918, loss: 3.894526720046997\n",
      "step 919, loss: 3.928865671157837\n",
      "step 920, loss: 4.098354339599609\n",
      "step 921, loss: 4.402653694152832\n",
      "step 922, loss: 4.300046443939209\n",
      "step 923, loss: 3.842569589614868\n",
      "step 924, loss: 3.72062349319458\n",
      "step 925, loss: 3.7388288974761963\n",
      "step 926, loss: 4.160008907318115\n",
      "step 927, loss: 3.9775044918060303\n",
      "step 928, loss: 4.094968795776367\n",
      "step 929, loss: 4.34583854675293\n",
      "step 930, loss: 3.8273351192474365\n",
      "step 931, loss: 3.9373042583465576\n",
      "step 932, loss: 2.5898213386535645\n",
      "step 933, loss: 2.9407079219818115\n",
      "step 934, loss: 3.384143590927124\n",
      "step 935, loss: 2.7408485412597656\n",
      "step 936, loss: 2.205982208251953\n",
      "step 937, loss: 2.4777138233184814\n",
      "step 938, loss: 2.6296417713165283\n",
      "step 939, loss: 2.6416473388671875\n",
      "step 940, loss: 2.544173002243042\n",
      "step 941, loss: 2.7377262115478516\n",
      "step 942, loss: 2.967495918273926\n",
      "step 943, loss: 2.656756639480591\n",
      "step 944, loss: 2.8137097358703613\n",
      "step 945, loss: 2.656188726425171\n",
      "step 946, loss: 2.6734888553619385\n",
      "step 947, loss: 2.7785515785217285\n",
      "step 948, loss: 2.6595709323883057\n",
      "step 949, loss: 2.5617339611053467\n",
      "step 950, loss: 2.782536745071411\n",
      "step 951, loss: 2.679621696472168\n",
      "step 952, loss: 2.140049934387207\n",
      "step 953, loss: 1.8176934719085693\n",
      "step 954, loss: 2.295502185821533\n",
      "step 955, loss: 2.166215181350708\n",
      "step 956, loss: 2.4287288188934326\n",
      "step 957, loss: 1.8166701793670654\n",
      "step 958, loss: 2.2577643394470215\n",
      "step 959, loss: 2.269141435623169\n",
      "step 960, loss: 2.061042308807373\n",
      "step 961, loss: 2.331444025039673\n",
      "step 962, loss: 2.808332681655884\n",
      "step 963, loss: 3.9997150897979736\n",
      "step 964, loss: 4.280844211578369\n",
      "step 965, loss: 4.1295576095581055\n",
      "step 966, loss: 3.757628917694092\n",
      "step 967, loss: 3.356520891189575\n",
      "step 968, loss: 3.4723095893859863\n",
      "step 969, loss: 3.847736358642578\n",
      "step 970, loss: 3.8000049591064453\n",
      "step 971, loss: 4.101718425750732\n",
      "step 972, loss: 3.965937852859497\n",
      "step 973, loss: 4.721652507781982\n",
      "step 974, loss: 3.7886874675750732\n",
      "step 975, loss: 3.753180980682373\n",
      "step 976, loss: 3.577171564102173\n",
      "step 977, loss: 3.7667746543884277\n",
      "step 978, loss: 4.367493152618408\n",
      "step 979, loss: 4.222478866577148\n",
      "step 980, loss: 3.9354794025421143\n",
      "step 981, loss: 4.2473464012146\n",
      "step 982, loss: 3.960928440093994\n",
      "step 983, loss: 3.8735828399658203\n",
      "step 984, loss: 3.7553248405456543\n",
      "step 985, loss: 3.9860684871673584\n",
      "step 986, loss: 3.8498690128326416\n",
      "step 987, loss: 4.006178379058838\n",
      "step 988, loss: 4.4377312660217285\n",
      "step 989, loss: 3.9428329467773438\n",
      "step 990, loss: 3.574042558670044\n",
      "step 991, loss: 3.845367431640625\n",
      "step 992, loss: 3.925079822540283\n",
      "step 993, loss: 4.077116012573242\n",
      "step 994, loss: 3.6982431411743164\n",
      "step 995, loss: 4.147585868835449\n",
      "step 996, loss: 4.35038423538208\n",
      "step 997, loss: 4.156105041503906\n",
      "step 998, loss: 4.693935394287109\n",
      "step 999, loss: 4.643032550811768\n",
      "step 1000, loss: 2.9320883750915527\n",
      "step 1001, loss: 2.886469841003418\n",
      "step 1002, loss: 3.0895562171936035\n",
      "step 1003, loss: 2.842991352081299\n",
      "step 1004, loss: 2.837968587875366\n",
      "step 1005, loss: 3.3283498287200928\n",
      "step 1006, loss: 3.1088786125183105\n",
      "step 1007, loss: 3.449521064758301\n",
      "step 1008, loss: 3.0601184368133545\n",
      "step 1009, loss: 2.1378564834594727\n",
      "step 1010, loss: 2.205111503601074\n",
      "step 1011, loss: 2.000230073928833\n",
      "step 1012, loss: 2.1284608840942383\n",
      "step 1013, loss: 2.34846830368042\n",
      "step 1014, loss: 2.4220709800720215\n",
      "step 1015, loss: 1.8609567880630493\n",
      "step 1016, loss: 3.8219151496887207\n",
      "step 1017, loss: 3.6728196144104004\n",
      "step 1018, loss: 3.811707019805908\n",
      "step 1019, loss: 3.8326876163482666\n",
      "step 1020, loss: 3.788933038711548\n",
      "step 1021, loss: 3.4442267417907715\n",
      "step 1022, loss: 3.4337587356567383\n",
      "step 1023, loss: 3.3916196823120117\n",
      "step 1024, loss: 3.4979186058044434\n",
      "step 1025, loss: 3.4646830558776855\n",
      "step 1026, loss: 3.416783332824707\n",
      "step 1027, loss: 3.461439371109009\n",
      "step 1028, loss: 3.2452287673950195\n",
      "step 1029, loss: 3.440821647644043\n",
      "step 1030, loss: 3.8935275077819824\n",
      "step 1031, loss: 3.6886954307556152\n",
      "step 1032, loss: 3.3519508838653564\n",
      "step 1033, loss: 3.5004169940948486\n",
      "step 1034, loss: 3.573139190673828\n",
      "step 1035, loss: 3.1097495555877686\n",
      "step 1036, loss: 3.133585214614868\n",
      "step 1037, loss: 3.6744983196258545\n",
      "step 1038, loss: 3.4738285541534424\n",
      "step 1039, loss: 3.508777141571045\n",
      "step 1040, loss: 3.2970335483551025\n",
      "step 1041, loss: 3.5140013694763184\n",
      "step 1042, loss: 3.4540271759033203\n",
      "step 1043, loss: 3.5679404735565186\n",
      "step 1044, loss: 3.2208478450775146\n",
      "step 1045, loss: 3.3470799922943115\n",
      "step 1046, loss: 3.537482261657715\n",
      "step 1047, loss: 3.516212224960327\n",
      "step 1048, loss: 3.415459156036377\n",
      "step 1049, loss: 3.049323558807373\n",
      "step 1050, loss: 3.416461706161499\n",
      "step 1051, loss: 3.5629284381866455\n",
      "step 1052, loss: 3.280306339263916\n",
      "step 1053, loss: 3.507396936416626\n",
      "step 1054, loss: 3.619767665863037\n",
      "step 1055, loss: 2.757504940032959\n",
      "step 1056, loss: 3.4806530475616455\n",
      "step 1057, loss: 3.2898976802825928\n",
      "step 1058, loss: 3.7219955921173096\n",
      "step 1059, loss: 3.3513216972351074\n",
      "step 1060, loss: 3.237380027770996\n",
      "step 1061, loss: 3.340640068054199\n",
      "step 1062, loss: 3.548584461212158\n",
      "step 1063, loss: 3.5220437049865723\n",
      "step 1064, loss: 3.4952569007873535\n",
      "step 1065, loss: 3.317690372467041\n",
      "step 1066, loss: 2.6351521015167236\n",
      "step 1067, loss: 3.1046664714813232\n",
      "step 1068, loss: 3.566798210144043\n",
      "step 1069, loss: 4.067648410797119\n",
      "step 1070, loss: 3.4114975929260254\n",
      "step 1071, loss: 3.7367517948150635\n",
      "step 1072, loss: 3.2650864124298096\n",
      "step 1073, loss: 3.851356267929077\n",
      "step 1074, loss: 3.24023699760437\n",
      "step 1075, loss: 3.2934632301330566\n",
      "step 1076, loss: 3.4289588928222656\n",
      "step 1077, loss: 3.091466188430786\n",
      "step 1078, loss: 3.2954626083374023\n",
      "step 1079, loss: 3.599647045135498\n",
      "step 1080, loss: 3.001781702041626\n",
      "step 1081, loss: 3.300435781478882\n",
      "step 1082, loss: 3.386901378631592\n",
      "step 1083, loss: 3.445258140563965\n",
      "step 1084, loss: 3.2288002967834473\n",
      "step 1085, loss: 3.6908457279205322\n",
      "step 1086, loss: 3.7522976398468018\n",
      "step 1087, loss: 3.1266541481018066\n",
      "step 1088, loss: 3.4013164043426514\n",
      "step 1089, loss: 3.208737373352051\n",
      "step 1090, loss: 3.249744415283203\n",
      "step 1091, loss: 3.025771141052246\n",
      "step 1092, loss: 3.4314842224121094\n",
      "step 1093, loss: 3.350943088531494\n",
      "step 1094, loss: 3.484727621078491\n",
      "step 1095, loss: 3.505496025085449\n",
      "step 1096, loss: 3.5523862838745117\n",
      "step 1097, loss: 3.3373982906341553\n",
      "step 1098, loss: 3.6300065517425537\n",
      "step 1099, loss: 3.627617359161377\n",
      "step 1100, loss: 3.774052381515503\n",
      "step 1101, loss: 3.461754083633423\n",
      "step 1102, loss: 3.554075002670288\n",
      "step 1103, loss: 3.7555036544799805\n",
      "step 1104, loss: 2.935884952545166\n",
      "step 1105, loss: 2.786921262741089\n",
      "step 1106, loss: 2.7455241680145264\n",
      "step 1107, loss: 2.7152388095855713\n",
      "step 1108, loss: 3.3540146350860596\n",
      "step 1109, loss: 3.052795648574829\n",
      "step 1110, loss: 2.3818318843841553\n",
      "step 1111, loss: 2.2440598011016846\n",
      "step 1112, loss: 2.709394693374634\n",
      "step 1113, loss: 2.0774483680725098\n",
      "step 1114, loss: 2.5808827877044678\n",
      "step 1115, loss: 2.500276803970337\n",
      "step 1116, loss: 2.1843528747558594\n",
      "step 1117, loss: 1.991105318069458\n",
      "step 1118, loss: 2.455547332763672\n",
      "step 1119, loss: 2.658226728439331\n",
      "step 1120, loss: 2.845339298248291\n",
      "step 1121, loss: 3.044475793838501\n",
      "step 1122, loss: 3.095860481262207\n",
      "step 1123, loss: 3.5481441020965576\n",
      "step 1124, loss: 3.3838248252868652\n",
      "step 1125, loss: 2.9624595642089844\n",
      "step 1126, loss: 2.760915756225586\n",
      "step 1127, loss: 2.8707239627838135\n",
      "step 1128, loss: 2.8982934951782227\n",
      "step 1129, loss: 3.253053665161133\n",
      "step 1130, loss: 3.0630011558532715\n",
      "step 1131, loss: 3.2613017559051514\n",
      "step 1132, loss: 3.099471092224121\n",
      "step 1133, loss: 3.243574619293213\n",
      "step 1134, loss: 3.0548527240753174\n",
      "step 1135, loss: 2.7638492584228516\n",
      "step 1136, loss: 2.893933057785034\n",
      "step 1137, loss: 2.886566162109375\n",
      "step 1138, loss: 3.146327257156372\n",
      "step 1139, loss: 3.100011110305786\n",
      "step 1140, loss: 3.2582805156707764\n",
      "step 1141, loss: 3.01859712600708\n",
      "step 1142, loss: 3.1338746547698975\n",
      "step 1143, loss: 2.9990551471710205\n",
      "step 1144, loss: 3.115928888320923\n",
      "step 1145, loss: 3.0877525806427\n",
      "step 1146, loss: 3.062973976135254\n",
      "step 1147, loss: 3.2518062591552734\n",
      "step 1148, loss: 3.3157312870025635\n",
      "step 1149, loss: 3.3235132694244385\n",
      "step 1150, loss: 2.732975959777832\n",
      "step 1151, loss: 2.986556053161621\n",
      "step 1152, loss: 3.109304189682007\n",
      "step 1153, loss: 3.113302230834961\n",
      "step 1154, loss: 3.183762311935425\n",
      "step 1155, loss: 2.833620548248291\n",
      "step 1156, loss: 3.0973763465881348\n",
      "step 1157, loss: 3.037747383117676\n",
      "step 1158, loss: 2.9662907123565674\n",
      "step 1159, loss: 3.0973026752471924\n",
      "step 1160, loss: 2.7198972702026367\n",
      "step 1161, loss: 3.0791757106781006\n",
      "step 1162, loss: 2.977120876312256\n",
      "step 1163, loss: 3.118614435195923\n",
      "step 1164, loss: 2.861809492111206\n",
      "step 1165, loss: 3.183399200439453\n",
      "step 1166, loss: 2.887880325317383\n",
      "step 1167, loss: 2.6699366569519043\n",
      "step 1168, loss: 2.863511562347412\n",
      "step 1169, loss: 2.833427906036377\n",
      "step 1170, loss: 2.9052939414978027\n",
      "step 1171, loss: 3.1248538494110107\n",
      "step 1172, loss: 3.0214405059814453\n",
      "step 1173, loss: 2.9171252250671387\n",
      "step 1174, loss: 3.108487129211426\n",
      "step 1175, loss: 2.657952070236206\n",
      "step 1176, loss: 2.938971996307373\n",
      "step 1177, loss: 3.1859683990478516\n",
      "step 1178, loss: 2.6474809646606445\n",
      "step 1179, loss: 2.086289167404175\n",
      "step 1180, loss: 1.7287975549697876\n",
      "step 1181, loss: 1.9578660726547241\n",
      "step 1182, loss: 1.778166651725769\n",
      "step 1183, loss: 1.667366623878479\n",
      "step 1184, loss: 1.6067837476730347\n",
      "step 1185, loss: 1.7718569040298462\n",
      "step 1186, loss: 1.7573466300964355\n",
      "step 1187, loss: 1.679106593132019\n",
      "step 1188, loss: 1.6034811735153198\n",
      "step 1189, loss: 1.4320846796035767\n",
      "step 1190, loss: 1.6415443420410156\n",
      "step 1191, loss: 2.0956554412841797\n",
      "step 1192, loss: 2.665297269821167\n",
      "step 1193, loss: 3.2503292560577393\n",
      "step 1194, loss: 3.0982022285461426\n",
      "step 1195, loss: 3.195955991744995\n",
      "step 1196, loss: 3.092700481414795\n",
      "step 1197, loss: 2.9263505935668945\n",
      "step 1198, loss: 3.0091705322265625\n",
      "step 1199, loss: 3.344599962234497\n",
      "step 1200, loss: 3.0561180114746094\n",
      "step 1201, loss: 3.139881134033203\n",
      "step 1202, loss: 3.1594655513763428\n",
      "step 1203, loss: 3.193317413330078\n",
      "step 1204, loss: 2.4772887229919434\n",
      "step 1205, loss: 3.0970845222473145\n",
      "step 1206, loss: 3.0563132762908936\n",
      "step 1207, loss: 3.0513391494750977\n",
      "step 1208, loss: 3.2639355659484863\n",
      "step 1209, loss: 3.0490872859954834\n",
      "step 1210, loss: 3.0951287746429443\n",
      "step 1211, loss: 3.187135934829712\n",
      "step 1212, loss: 3.0743000507354736\n",
      "step 1213, loss: 3.303119659423828\n",
      "step 1214, loss: 2.9593124389648438\n",
      "step 1215, loss: 2.5300064086914062\n",
      "step 1216, loss: 2.802119255065918\n",
      "step 1217, loss: 2.87041974067688\n",
      "step 1218, loss: 3.0122532844543457\n",
      "step 1219, loss: 2.9147067070007324\n",
      "step 1220, loss: 3.159871816635132\n",
      "step 1221, loss: 3.1248905658721924\n",
      "step 1222, loss: 2.6681156158447266\n",
      "step 1223, loss: 2.8295273780822754\n",
      "step 1224, loss: 2.7083170413970947\n",
      "step 1225, loss: 3.1259710788726807\n",
      "step 1226, loss: 2.972682237625122\n",
      "step 1227, loss: 2.879540205001831\n",
      "step 1228, loss: 2.9680354595184326\n",
      "step 1229, loss: 2.851367473602295\n",
      "step 1230, loss: 2.8613007068634033\n",
      "step 1231, loss: 2.844496965408325\n",
      "step 1232, loss: 3.076399564743042\n",
      "step 1233, loss: 3.0075185298919678\n",
      "step 1234, loss: 2.930826425552368\n",
      "step 1235, loss: 3.198662757873535\n",
      "step 1236, loss: 2.8823602199554443\n",
      "step 1237, loss: 2.8908846378326416\n",
      "step 1238, loss: 3.09141206741333\n",
      "step 1239, loss: 2.831284284591675\n",
      "step 1240, loss: 2.725515127182007\n",
      "step 1241, loss: 2.9978713989257812\n",
      "step 1242, loss: 2.8671629428863525\n",
      "step 1243, loss: 2.949908494949341\n",
      "step 1244, loss: 2.871145009994507\n",
      "step 1245, loss: 2.955275058746338\n",
      "step 1246, loss: 3.031683921813965\n",
      "step 1247, loss: 2.568998336791992\n",
      "step 1248, loss: 2.751415252685547\n",
      "step 1249, loss: 3.0067689418792725\n",
      "step 1250, loss: 2.9335238933563232\n",
      "step 1251, loss: 3.0925776958465576\n",
      "step 1252, loss: 2.936269998550415\n",
      "step 1253, loss: 3.2057034969329834\n",
      "step 1254, loss: 2.9778122901916504\n",
      "step 1255, loss: 3.1201705932617188\n",
      "step 1256, loss: 2.8933918476104736\n",
      "step 1257, loss: 2.9775476455688477\n",
      "step 1258, loss: 2.8341102600097656\n",
      "step 1259, loss: 3.267059803009033\n",
      "step 1260, loss: 3.3563835620880127\n",
      "step 1261, loss: 2.97615385055542\n",
      "step 1262, loss: 2.9973459243774414\n",
      "step 1263, loss: 2.9707796573638916\n",
      "step 1264, loss: 3.113825559616089\n",
      "step 1265, loss: 2.933260440826416\n",
      "step 1266, loss: 2.8266234397888184\n",
      "step 1267, loss: 3.0648250579833984\n",
      "step 1268, loss: 2.329596996307373\n",
      "step 1269, loss: 2.882434368133545\n",
      "step 1270, loss: 2.822059392929077\n",
      "step 1271, loss: 2.879478931427002\n",
      "step 1272, loss: 2.817950963973999\n",
      "step 1273, loss: 2.8633971214294434\n",
      "step 1274, loss: 3.335663080215454\n",
      "step 1275, loss: 2.894926071166992\n",
      "step 1276, loss: 3.205543279647827\n",
      "step 1277, loss: 3.025393486022949\n",
      "step 1278, loss: 2.5466346740722656\n",
      "step 1279, loss: 3.0470023155212402\n",
      "step 1280, loss: 2.9786860942840576\n",
      "step 1281, loss: 3.0348057746887207\n",
      "step 1282, loss: 2.928194284439087\n",
      "step 1283, loss: 3.1397955417633057\n",
      "step 1284, loss: 3.4374680519104004\n",
      "step 1285, loss: 3.0538196563720703\n",
      "step 1286, loss: 2.7461163997650146\n",
      "step 1287, loss: 3.039696455001831\n",
      "step 1288, loss: 2.7092936038970947\n",
      "step 1289, loss: 3.292581081390381\n",
      "step 1290, loss: 2.851015567779541\n",
      "step 1291, loss: 3.4019875526428223\n",
      "step 1292, loss: 2.8069002628326416\n",
      "step 1293, loss: 3.0237059593200684\n",
      "step 1294, loss: 2.918023109436035\n",
      "step 1295, loss: 3.225912094116211\n",
      "step 1296, loss: 3.1452393531799316\n",
      "step 1297, loss: 3.3814828395843506\n",
      "step 1298, loss: 3.0426762104034424\n",
      "step 1299, loss: 3.104667901992798\n",
      "step 1300, loss: 2.9203834533691406\n",
      "step 1301, loss: 3.0786378383636475\n",
      "step 1302, loss: 3.3017079830169678\n",
      "step 1303, loss: 2.6465365886688232\n",
      "step 1304, loss: 2.152344226837158\n",
      "step 1305, loss: 1.9048744440078735\n",
      "step 1306, loss: 1.99636709690094\n",
      "step 1307, loss: 1.6336644887924194\n",
      "step 1308, loss: 1.426403522491455\n",
      "step 1309, loss: 1.9080743789672852\n",
      "step 1310, loss: 1.3578916788101196\n",
      "step 1311, loss: 1.6765400171279907\n",
      "step 1312, loss: 1.6303120851516724\n",
      "step 1313, loss: 1.3855327367782593\n",
      "step 1314, loss: 1.3463326692581177\n",
      "step 1315, loss: 1.7206931114196777\n",
      "step 1316, loss: 1.732938289642334\n",
      "step 1317, loss: 1.3847393989562988\n",
      "step 1318, loss: 0.9872511625289917\n",
      "step 1319, loss: 1.644083023071289\n",
      "step 1320, loss: 1.249930500984192\n",
      "step 1321, loss: 1.5187305212020874\n",
      "step 1322, loss: 1.411638855934143\n",
      "step 1323, loss: 1.072770357131958\n",
      "step 1324, loss: 1.7738425731658936\n",
      "step 1325, loss: 1.8771940469741821\n",
      "step 1326, loss: 1.69545316696167\n",
      "step 1327, loss: 1.7308635711669922\n",
      "step 1328, loss: 1.5115272998809814\n",
      "step 1329, loss: 1.5874755382537842\n",
      "step 1330, loss: 1.669142723083496\n",
      "step 1331, loss: 2.004542827606201\n",
      "step 1332, loss: 2.825643539428711\n",
      "step 1333, loss: 2.919236183166504\n",
      "step 1334, loss: 3.2184131145477295\n",
      "step 1335, loss: 3.3211920261383057\n",
      "step 1336, loss: 3.0550832748413086\n",
      "step 1337, loss: 2.8861169815063477\n",
      "step 1338, loss: 3.0076558589935303\n",
      "step 1339, loss: 2.9005000591278076\n",
      "step 1340, loss: 2.8649673461914062\n",
      "step 1341, loss: 3.034489631652832\n",
      "step 1342, loss: 3.2729620933532715\n",
      "step 1343, loss: 3.153744697570801\n",
      "step 1344, loss: 3.1255743503570557\n",
      "step 1345, loss: 3.0429389476776123\n",
      "step 1346, loss: 3.287538528442383\n",
      "step 1347, loss: 3.0173652172088623\n",
      "step 1348, loss: 2.955439329147339\n",
      "step 1349, loss: 3.0709078311920166\n",
      "step 1350, loss: 2.927165985107422\n",
      "step 1351, loss: 3.009000539779663\n",
      "step 1352, loss: 3.140714645385742\n",
      "step 1353, loss: 2.8866782188415527\n",
      "step 1354, loss: 3.0479509830474854\n",
      "step 1355, loss: 3.044795274734497\n",
      "step 1356, loss: 3.0216124057769775\n",
      "step 1357, loss: 2.9402568340301514\n",
      "step 1358, loss: 3.132481813430786\n",
      "step 1359, loss: 3.116298198699951\n",
      "step 1360, loss: 2.8533926010131836\n",
      "step 1361, loss: 2.829383134841919\n",
      "step 1362, loss: 2.9580461978912354\n",
      "step 1363, loss: 2.8737993240356445\n",
      "step 1364, loss: 3.021024227142334\n",
      "step 1365, loss: 2.7669155597686768\n",
      "step 1366, loss: 2.9256374835968018\n",
      "step 1367, loss: 2.9077529907226562\n",
      "step 1368, loss: 3.038983106613159\n",
      "step 1369, loss: 2.9214868545532227\n",
      "step 1370, loss: 3.0979807376861572\n",
      "step 1371, loss: 2.5963504314422607\n",
      "step 1372, loss: 3.1122689247131348\n",
      "step 1373, loss: 3.0600037574768066\n",
      "step 1374, loss: 2.8943707942962646\n",
      "step 1375, loss: 3.0569839477539062\n",
      "step 1376, loss: 2.905595302581787\n",
      "step 1377, loss: 2.86252498626709\n",
      "step 1378, loss: 2.6169052124023438\n",
      "step 1379, loss: 2.9404304027557373\n",
      "step 1380, loss: 2.741621732711792\n",
      "step 1381, loss: 2.851449489593506\n",
      "step 1382, loss: 2.7305586338043213\n",
      "step 1383, loss: 2.7931759357452393\n",
      "step 1384, loss: 2.4554014205932617\n",
      "step 1385, loss: 3.0930750370025635\n",
      "step 1386, loss: 3.3028955459594727\n",
      "step 1387, loss: 2.8290815353393555\n",
      "step 1388, loss: 2.7424979209899902\n",
      "step 1389, loss: 2.7849409580230713\n",
      "step 1390, loss: 2.640836477279663\n",
      "step 1391, loss: 2.6756796836853027\n",
      "step 1392, loss: 2.9598069190979004\n",
      "step 1393, loss: 2.911367893218994\n",
      "step 1394, loss: 2.603679895401001\n",
      "step 1395, loss: 2.824883460998535\n",
      "step 1396, loss: 2.6943347454071045\n",
      "step 1397, loss: 2.616875171661377\n",
      "step 1398, loss: 2.794072389602661\n",
      "step 1399, loss: 3.071537494659424\n",
      "step 1400, loss: 2.7458548545837402\n",
      "step 1401, loss: 2.8956360816955566\n",
      "step 1402, loss: 2.6895840167999268\n",
      "step 1403, loss: 2.9168381690979004\n",
      "step 1404, loss: 3.203932762145996\n",
      "step 1405, loss: 3.0277090072631836\n",
      "step 1406, loss: 3.089388608932495\n",
      "step 1407, loss: 2.8593595027923584\n",
      "step 1408, loss: 3.0851242542266846\n",
      "step 1409, loss: 3.0019617080688477\n",
      "step 1410, loss: 2.81137752532959\n",
      "step 1411, loss: 3.147797107696533\n",
      "step 1412, loss: 2.7860114574432373\n",
      "step 1413, loss: 2.9457733631134033\n",
      "step 1414, loss: 3.243464231491089\n",
      "step 1415, loss: 2.890024185180664\n",
      "step 1416, loss: 3.003589391708374\n",
      "step 1417, loss: 3.3756983280181885\n",
      "step 1418, loss: 3.135529041290283\n",
      "step 1419, loss: 2.7715346813201904\n",
      "step 1420, loss: 3.1496121883392334\n",
      "step 1421, loss: 3.017423391342163\n",
      "step 1422, loss: 2.8572442531585693\n",
      "step 1423, loss: 3.110468864440918\n",
      "step 1424, loss: 2.8259127140045166\n",
      "step 1425, loss: 2.8450191020965576\n",
      "step 1426, loss: 2.792407751083374\n",
      "step 1427, loss: 2.7665162086486816\n",
      "step 1428, loss: 2.2986502647399902\n",
      "step 1429, loss: 2.2129180431365967\n",
      "step 1430, loss: 1.9042888879776\n",
      "step 1431, loss: 2.0913426876068115\n",
      "step 1432, loss: 2.3408546447753906\n",
      "step 1433, loss: 2.3251452445983887\n",
      "step 1434, loss: 2.3056211471557617\n",
      "step 1435, loss: 2.0574734210968018\n",
      "step 1436, loss: 2.2603917121887207\n",
      "step 1437, loss: 1.6542338132858276\n",
      "step 1438, loss: 1.8740124702453613\n",
      "step 1439, loss: 1.857398509979248\n",
      "step 1440, loss: 1.6541447639465332\n",
      "step 1441, loss: 1.5359476804733276\n",
      "step 1442, loss: 1.5316025018692017\n",
      "step 1443, loss: 1.3393282890319824\n",
      "step 1444, loss: 1.6457209587097168\n",
      "step 1445, loss: 2.001077651977539\n",
      "step 1446, loss: 1.7356637716293335\n",
      "step 1447, loss: 1.6410635709762573\n",
      "step 1448, loss: 1.6602164506912231\n",
      "step 1449, loss: 1.5820130109786987\n",
      "step 1450, loss: 2.5662851333618164\n",
      "step 1451, loss: 3.078094244003296\n",
      "step 1452, loss: 2.8642959594726562\n",
      "step 1453, loss: 3.003437042236328\n",
      "step 1454, loss: 3.3943021297454834\n",
      "step 1455, loss: 3.0513718128204346\n",
      "step 1456, loss: 2.7290005683898926\n",
      "step 1457, loss: 2.771975040435791\n",
      "step 1458, loss: 2.972785711288452\n",
      "step 1459, loss: 2.7759151458740234\n",
      "step 1460, loss: 2.843029737472534\n",
      "step 1461, loss: 2.8451435565948486\n",
      "step 1462, loss: 2.868302345275879\n",
      "step 1463, loss: 3.3390963077545166\n",
      "step 1464, loss: 3.256068229675293\n",
      "step 1465, loss: 3.045107364654541\n",
      "step 1466, loss: 2.87504243850708\n",
      "step 1467, loss: 3.347959041595459\n",
      "step 1468, loss: 3.2772164344787598\n",
      "step 1469, loss: 3.214442491531372\n",
      "step 1470, loss: 3.173173666000366\n",
      "step 1471, loss: 2.9672493934631348\n",
      "step 1472, loss: 2.8465867042541504\n",
      "step 1473, loss: 3.2635364532470703\n",
      "step 1474, loss: 2.7455332279205322\n",
      "step 1475, loss: 2.7033309936523438\n",
      "step 1476, loss: 2.641685962677002\n",
      "step 1477, loss: 3.05002498626709\n",
      "step 1478, loss: 2.9114651679992676\n",
      "step 1479, loss: 3.132581949234009\n",
      "step 1480, loss: 2.9038608074188232\n",
      "step 1481, loss: 3.3423266410827637\n",
      "step 1482, loss: 3.1180901527404785\n",
      "step 1483, loss: 3.4933347702026367\n",
      "step 1484, loss: 3.005432605743408\n",
      "step 1485, loss: 3.338327169418335\n",
      "step 1486, loss: 3.207871198654175\n",
      "step 1487, loss: 3.2592475414276123\n",
      "step 1488, loss: 3.075235605239868\n",
      "step 1489, loss: 3.2219560146331787\n",
      "step 1490, loss: 3.107386827468872\n",
      "step 1491, loss: 3.0694334506988525\n",
      "step 1492, loss: 2.927473783493042\n",
      "step 1493, loss: 3.2710728645324707\n",
      "step 1494, loss: 3.4251959323883057\n",
      "step 1495, loss: 3.0203497409820557\n",
      "step 1496, loss: 3.019516706466675\n",
      "step 1497, loss: 3.135312080383301\n",
      "step 1498, loss: 3.1268768310546875\n",
      "step 1499, loss: 2.8102269172668457\n",
      "step 1500, loss: 2.661712169647217\n",
      "step 1501, loss: 2.9798481464385986\n",
      "step 1502, loss: 3.0244905948638916\n",
      "step 1503, loss: 3.4405250549316406\n",
      "step 1504, loss: 3.3797171115875244\n",
      "step 1505, loss: 2.805396318435669\n",
      "step 1506, loss: 3.2271029949188232\n",
      "step 1507, loss: 3.1053543090820312\n",
      "step 1508, loss: 3.169055223464966\n",
      "step 1509, loss: 3.142185688018799\n",
      "step 1510, loss: 2.8513729572296143\n",
      "step 1511, loss: 3.145134687423706\n",
      "step 1512, loss: 3.4288599491119385\n",
      "step 1513, loss: 3.3159382343292236\n",
      "step 1514, loss: 3.1083056926727295\n",
      "step 1515, loss: 3.343743324279785\n",
      "step 1516, loss: 3.485790729522705\n",
      "step 1517, loss: 3.28122878074646\n",
      "step 1518, loss: 3.2421176433563232\n",
      "step 1519, loss: 3.5060601234436035\n",
      "step 1520, loss: 3.1874618530273438\n",
      "step 1521, loss: 3.2587945461273193\n",
      "step 1522, loss: 3.2885897159576416\n",
      "step 1523, loss: 3.706209897994995\n",
      "step 1524, loss: 3.2885844707489014\n",
      "step 1525, loss: 3.0533931255340576\n",
      "step 1526, loss: 3.0284886360168457\n",
      "step 1527, loss: 3.557786226272583\n",
      "step 1528, loss: 2.8153915405273438\n",
      "step 1529, loss: 3.1785528659820557\n",
      "step 1530, loss: 3.2285633087158203\n",
      "step 1531, loss: 2.7627153396606445\n",
      "step 1532, loss: 3.310098886489868\n",
      "step 1533, loss: 3.0817387104034424\n",
      "step 1534, loss: 3.1989450454711914\n",
      "step 1535, loss: 3.247729539871216\n",
      "step 1536, loss: 3.3788084983825684\n",
      "step 1537, loss: 3.3210670948028564\n",
      "step 1538, loss: 3.1050844192504883\n",
      "step 1539, loss: 3.1493148803710938\n",
      "step 1540, loss: 2.522765636444092\n",
      "step 1541, loss: 2.4982962608337402\n",
      "step 1542, loss: 2.1497132778167725\n",
      "step 1543, loss: 1.89324951171875\n",
      "step 1544, loss: 2.1007256507873535\n",
      "step 1545, loss: 1.7467572689056396\n",
      "step 1546, loss: 1.8701728582382202\n",
      "step 1547, loss: 1.6893094778060913\n",
      "step 1548, loss: 1.8363037109375\n",
      "step 1549, loss: 1.644347071647644\n",
      "step 1550, loss: 1.5275105237960815\n",
      "step 1551, loss: 1.457114338874817\n",
      "step 1552, loss: 1.2858706712722778\n",
      "step 1553, loss: 1.5067808628082275\n",
      "step 1554, loss: 1.3246935606002808\n",
      "step 1555, loss: 1.5209240913391113\n",
      "step 1556, loss: 1.5845869779586792\n",
      "step 1557, loss: 1.9951469898223877\n",
      "step 1558, loss: 2.8097009658813477\n",
      "step 1559, loss: 3.2754790782928467\n",
      "step 1560, loss: 3.341696262359619\n",
      "step 1561, loss: 3.437133550643921\n",
      "step 1562, loss: 3.087843179702759\n",
      "step 1563, loss: 2.946780204772949\n",
      "step 1564, loss: 2.934154748916626\n",
      "step 1565, loss: 3.2177679538726807\n",
      "step 1566, loss: 3.0044002532958984\n",
      "step 1567, loss: 3.1605138778686523\n",
      "step 1568, loss: 3.1800177097320557\n",
      "step 1569, loss: 3.078699827194214\n",
      "step 1570, loss: 3.1487362384796143\n",
      "step 1571, loss: 3.0224099159240723\n",
      "step 1572, loss: 3.129210948944092\n",
      "step 1573, loss: 3.025146245956421\n",
      "step 1574, loss: 3.136352300643921\n",
      "step 1575, loss: 3.155127763748169\n",
      "step 1576, loss: 2.8515889644622803\n",
      "step 1577, loss: 3.2365243434906006\n",
      "step 1578, loss: 3.0062294006347656\n",
      "step 1579, loss: 2.7931644916534424\n",
      "step 1580, loss: 3.0722742080688477\n",
      "step 1581, loss: 2.9633677005767822\n",
      "step 1582, loss: 3.1601450443267822\n",
      "step 1583, loss: 3.3312158584594727\n",
      "step 1584, loss: 2.943178653717041\n",
      "step 1585, loss: 2.924128293991089\n",
      "step 1586, loss: 3.2169759273529053\n",
      "step 1587, loss: 3.074766159057617\n",
      "step 1588, loss: 3.3068184852600098\n",
      "step 1589, loss: 3.085897445678711\n",
      "step 1590, loss: 2.87432861328125\n",
      "step 1591, loss: 3.268468141555786\n",
      "step 1592, loss: 3.3710291385650635\n",
      "step 1593, loss: 2.918539047241211\n",
      "step 1594, loss: 3.18229079246521\n",
      "step 1595, loss: 3.245223045349121\n",
      "step 1596, loss: 3.403672218322754\n",
      "step 1597, loss: 3.670398473739624\n",
      "step 1598, loss: 2.9016952514648438\n",
      "step 1599, loss: 3.429398775100708\n",
      "step 1600, loss: 3.337846517562866\n",
      "step 1601, loss: 3.225445508956909\n",
      "step 1602, loss: 2.7381479740142822\n",
      "step 1603, loss: 1.588638186454773\n",
      "step 1604, loss: 1.7780885696411133\n",
      "step 1605, loss: 1.5831091403961182\n",
      "step 1606, loss: 1.5167129039764404\n",
      "step 1607, loss: 1.3966633081436157\n",
      "step 1608, loss: 1.2663393020629883\n",
      "step 1609, loss: 1.0195454359054565\n",
      "step 1610, loss: 1.3718430995941162\n",
      "step 1611, loss: 1.3909605741500854\n",
      "step 1612, loss: 1.7313058376312256\n",
      "step 1613, loss: 1.5199968814849854\n",
      "step 1614, loss: 1.7335461378097534\n",
      "step 1615, loss: 1.6482168436050415\n",
      "step 1616, loss: 2.0586137771606445\n",
      "step 1617, loss: 2.2480854988098145\n",
      "step 1618, loss: 3.634882688522339\n",
      "step 1619, loss: 3.043053150177002\n",
      "step 1620, loss: 2.838416814804077\n",
      "step 1621, loss: 2.7372331619262695\n",
      "step 1622, loss: 2.9660630226135254\n",
      "step 1623, loss: 2.86917781829834\n",
      "step 1624, loss: 2.963613748550415\n",
      "step 1625, loss: 3.093691110610962\n",
      "step 1626, loss: 2.9862492084503174\n",
      "step 1627, loss: 2.7119052410125732\n",
      "step 1628, loss: 2.594834566116333\n",
      "step 1629, loss: 2.9749505519866943\n",
      "step 1630, loss: 2.8265507221221924\n",
      "step 1631, loss: 3.1671221256256104\n",
      "step 1632, loss: 2.9794883728027344\n",
      "step 1633, loss: 2.9208712577819824\n",
      "step 1634, loss: 3.3437960147857666\n",
      "step 1635, loss: 2.739896297454834\n",
      "step 1636, loss: 2.6117119789123535\n",
      "step 1637, loss: 2.7574851512908936\n",
      "step 1638, loss: 2.9737956523895264\n",
      "step 1639, loss: 2.6005682945251465\n",
      "step 1640, loss: 3.0842278003692627\n",
      "step 1641, loss: 2.935600996017456\n",
      "step 1642, loss: 2.906001567840576\n",
      "step 1643, loss: 3.0196211338043213\n",
      "step 1644, loss: 3.155266523361206\n",
      "step 1645, loss: 3.1689512729644775\n",
      "step 1646, loss: 3.30894136428833\n",
      "step 1647, loss: 3.1480870246887207\n",
      "step 1648, loss: 2.832834243774414\n",
      "step 1649, loss: 2.925689220428467\n",
      "step 1650, loss: 3.0691490173339844\n",
      "step 1651, loss: 3.21138858795166\n",
      "step 1652, loss: 2.868263006210327\n",
      "step 1653, loss: 3.176893949508667\n",
      "step 1654, loss: 3.0113401412963867\n",
      "step 1655, loss: 3.283663511276245\n",
      "step 1656, loss: 3.297943592071533\n",
      "step 1657, loss: 3.4371988773345947\n",
      "step 1658, loss: 3.2708475589752197\n",
      "step 1659, loss: 3.017631769180298\n",
      "step 1660, loss: 2.875450611114502\n",
      "step 1661, loss: 3.536815881729126\n",
      "step 1662, loss: 3.06598162651062\n",
      "step 1663, loss: 3.430373191833496\n",
      "step 1664, loss: 3.3118786811828613\n",
      "step 1665, loss: 3.0708494186401367\n",
      "step 1666, loss: 3.258384943008423\n",
      "step 1667, loss: 3.176109552383423\n",
      "step 1668, loss: 2.911733627319336\n",
      "step 1669, loss: 3.4276394844055176\n",
      "step 1670, loss: 3.0655312538146973\n",
      "step 1671, loss: 3.4525840282440186\n",
      "step 1672, loss: 3.154282569885254\n",
      "step 1673, loss: 3.078036308288574\n",
      "step 1674, loss: 3.2774152755737305\n",
      "step 1675, loss: 3.3328301906585693\n",
      "step 1676, loss: 3.2062199115753174\n",
      "step 1677, loss: 3.200122833251953\n",
      "step 1678, loss: 3.5890400409698486\n",
      "step 1679, loss: 3.235356569290161\n",
      "step 1680, loss: 3.2075085639953613\n",
      "step 1681, loss: 3.1281909942626953\n",
      "step 1682, loss: 3.0168070793151855\n",
      "step 1683, loss: 3.363276481628418\n",
      "step 1684, loss: 3.4786338806152344\n",
      "step 1685, loss: 3.1655776500701904\n",
      "step 1686, loss: 3.125243663787842\n",
      "step 1687, loss: 3.715017795562744\n",
      "step 1688, loss: 3.0946738719940186\n",
      "step 1689, loss: 3.3576033115386963\n",
      "step 1690, loss: 3.271380662918091\n",
      "step 1691, loss: 3.1354682445526123\n",
      "step 1692, loss: 3.435220718383789\n",
      "step 1693, loss: 3.157581090927124\n",
      "step 1694, loss: 3.391741991043091\n",
      "step 1695, loss: 3.197190284729004\n",
      "step 1696, loss: 3.5532374382019043\n",
      "step 1697, loss: 3.1320226192474365\n",
      "step 1698, loss: 3.343412399291992\n",
      "step 1699, loss: 3.0866310596466064\n",
      "step 1700, loss: 3.3752870559692383\n",
      "step 1701, loss: 3.346219539642334\n",
      "step 1702, loss: 3.2393195629119873\n",
      "step 1703, loss: 3.447716474533081\n",
      "step 1704, loss: 3.5465288162231445\n",
      "step 1705, loss: 3.5786538124084473\n",
      "step 1706, loss: 3.21407413482666\n",
      "step 1707, loss: 2.2777373790740967\n",
      "step 1708, loss: 1.8284982442855835\n",
      "step 1709, loss: 1.5806330442428589\n",
      "step 1710, loss: 1.5168414115905762\n",
      "step 1711, loss: 1.734104037284851\n",
      "step 1712, loss: 1.5330009460449219\n",
      "step 1713, loss: 1.6162607669830322\n",
      "step 1714, loss: 1.6752028465270996\n",
      "step 1715, loss: 1.5150015354156494\n",
      "step 1716, loss: 1.4615178108215332\n",
      "step 1717, loss: 1.357075810432434\n",
      "step 1718, loss: 1.2569817304611206\n",
      "step 1719, loss: 1.8751529455184937\n",
      "step 1720, loss: 1.4561562538146973\n",
      "step 1721, loss: 1.213009238243103\n",
      "step 1722, loss: 1.2262018918991089\n",
      "step 1723, loss: 1.6835286617279053\n",
      "step 1724, loss: 1.396386981010437\n",
      "step 1725, loss: 1.461612343788147\n",
      "step 1726, loss: 1.1559414863586426\n",
      "step 1727, loss: 1.2696781158447266\n",
      "step 1728, loss: 1.9715971946716309\n",
      "step 1729, loss: 2.872798442840576\n",
      "step 1730, loss: 2.735921859741211\n",
      "step 1731, loss: 3.005117416381836\n",
      "step 1732, loss: 2.9150705337524414\n",
      "step 1733, loss: 3.2960503101348877\n",
      "step 1734, loss: 3.6638169288635254\n",
      "step 1735, loss: 3.247779607772827\n",
      "step 1736, loss: 2.8650803565979004\n",
      "step 1737, loss: 3.0708425045013428\n",
      "step 1738, loss: 2.939999580383301\n",
      "step 1739, loss: 2.827399492263794\n",
      "step 1740, loss: 2.930896043777466\n",
      "step 1741, loss: 2.9932034015655518\n",
      "step 1742, loss: 3.204965829849243\n",
      "step 1743, loss: 2.933380365371704\n",
      "step 1744, loss: 3.056349754333496\n",
      "step 1745, loss: 2.973025321960449\n",
      "step 1746, loss: 2.8804829120635986\n",
      "step 1747, loss: 3.0939393043518066\n",
      "step 1748, loss: 3.03356671333313\n",
      "step 1749, loss: 3.0090534687042236\n",
      "step 1750, loss: 2.9942848682403564\n",
      "step 1751, loss: 2.871462821960449\n",
      "step 1752, loss: 3.3770968914031982\n",
      "step 1753, loss: 2.845135450363159\n",
      "step 1754, loss: 2.9336748123168945\n",
      "step 1755, loss: 3.0883641242980957\n",
      "step 1756, loss: 3.0116398334503174\n",
      "step 1757, loss: 2.975529432296753\n",
      "step 1758, loss: 2.8510446548461914\n",
      "step 1759, loss: 2.9867918491363525\n",
      "step 1760, loss: 2.9404807090759277\n",
      "step 1761, loss: 2.7969229221343994\n",
      "step 1762, loss: 2.9216995239257812\n",
      "step 1763, loss: 2.6616194248199463\n",
      "step 1764, loss: 2.8636128902435303\n",
      "step 1765, loss: 3.331512928009033\n",
      "step 1766, loss: 3.487541913986206\n",
      "step 1767, loss: 3.253267526626587\n",
      "step 1768, loss: 2.518284797668457\n",
      "step 1769, loss: 2.9524412155151367\n",
      "step 1770, loss: 2.919072151184082\n",
      "step 1771, loss: 3.065701961517334\n",
      "step 1772, loss: 2.628882884979248\n",
      "step 1773, loss: 2.894059658050537\n",
      "step 1774, loss: 2.9728124141693115\n",
      "step 1775, loss: 2.9853270053863525\n",
      "step 1776, loss: 3.179952621459961\n",
      "step 1777, loss: 2.9266629219055176\n",
      "step 1778, loss: 3.026991128921509\n",
      "step 1779, loss: 3.099999189376831\n",
      "step 1780, loss: 3.1848063468933105\n",
      "step 1781, loss: 3.0584964752197266\n",
      "step 1782, loss: 3.278902530670166\n",
      "step 1783, loss: 3.1629884243011475\n",
      "step 1784, loss: 2.9872591495513916\n",
      "step 1785, loss: 3.0908637046813965\n",
      "step 1786, loss: 3.1570868492126465\n",
      "step 1787, loss: 2.8607165813446045\n",
      "step 1788, loss: 2.8890955448150635\n",
      "step 1789, loss: 2.8390142917633057\n",
      "step 1790, loss: 3.2554996013641357\n",
      "step 1791, loss: 2.967442512512207\n",
      "step 1792, loss: 2.804966688156128\n",
      "step 1793, loss: 3.071009874343872\n",
      "step 1794, loss: 3.168436050415039\n",
      "step 1795, loss: 3.354320526123047\n",
      "step 1796, loss: 3.194037437438965\n",
      "step 1797, loss: 3.091689348220825\n",
      "step 1798, loss: 2.927565813064575\n",
      "step 1799, loss: 2.9040610790252686\n",
      "step 1800, loss: 3.139038324356079\n",
      "step 1801, loss: 3.0336155891418457\n",
      "step 1802, loss: 3.4694623947143555\n",
      "step 1803, loss: 3.163079023361206\n",
      "step 1804, loss: 3.2885379791259766\n",
      "step 1805, loss: 3.1831369400024414\n",
      "step 1806, loss: 3.0965983867645264\n",
      "step 1807, loss: 3.0008161067962646\n",
      "step 1808, loss: 3.005211591720581\n",
      "step 1809, loss: 3.1139113903045654\n",
      "step 1810, loss: 3.211379051208496\n",
      "step 1811, loss: 3.2357263565063477\n",
      "step 1812, loss: 3.1041455268859863\n",
      "step 1813, loss: 2.865466594696045\n",
      "step 1814, loss: 2.6533236503601074\n",
      "step 1815, loss: 2.781142473220825\n",
      "step 1816, loss: 3.077446699142456\n",
      "step 1817, loss: 3.3408148288726807\n",
      "step 1818, loss: 3.2060418128967285\n",
      "step 1819, loss: 3.1249196529388428\n",
      "step 1820, loss: 3.3128507137298584\n",
      "step 1821, loss: 3.1744515895843506\n",
      "step 1822, loss: 3.2481260299682617\n",
      "step 1823, loss: 3.085928440093994\n",
      "step 1824, loss: 3.520042657852173\n",
      "step 1825, loss: 3.3165056705474854\n",
      "step 1826, loss: 3.199410915374756\n",
      "step 1827, loss: 3.283330202102661\n",
      "step 1828, loss: 3.004560708999634\n",
      "step 1829, loss: 3.3673295974731445\n",
      "step 1830, loss: 3.5021934509277344\n",
      "step 1831, loss: 3.3591489791870117\n",
      "step 1832, loss: 3.691688060760498\n",
      "step 1833, loss: 2.9840269088745117\n",
      "step 1834, loss: 2.9161863327026367\n",
      "step 1835, loss: 3.1980998516082764\n",
      "step 1836, loss: 3.503499984741211\n",
      "step 1837, loss: 3.167449712753296\n",
      "step 1838, loss: 2.719444751739502\n",
      "step 1839, loss: 1.6292641162872314\n",
      "step 1840, loss: 1.46408212184906\n",
      "step 1841, loss: 1.6720820665359497\n",
      "step 1842, loss: 1.3549225330352783\n",
      "step 1843, loss: 1.5257980823516846\n",
      "step 1844, loss: 1.6667180061340332\n",
      "step 1845, loss: 1.502007007598877\n",
      "step 1846, loss: 1.7236590385437012\n",
      "step 1847, loss: 1.4636728763580322\n",
      "step 1848, loss: 1.5646026134490967\n",
      "step 1849, loss: 1.4738662242889404\n",
      "step 1850, loss: 1.6705846786499023\n",
      "step 1851, loss: 1.6275420188903809\n",
      "step 1852, loss: 1.898155927658081\n",
      "step 1853, loss: 1.17575204372406\n",
      "step 1854, loss: 1.0466572046279907\n",
      "step 1855, loss: 1.3500951528549194\n",
      "step 1856, loss: 0.982883870601654\n",
      "step 1857, loss: 1.272820234298706\n",
      "step 1858, loss: 1.2026925086975098\n",
      "step 1859, loss: 1.427039623260498\n",
      "step 1860, loss: 1.4797608852386475\n",
      "step 1861, loss: 1.3204479217529297\n",
      "step 1862, loss: 1.1425694227218628\n",
      "step 1863, loss: 1.6471478939056396\n",
      "step 1864, loss: 1.3540120124816895\n",
      "step 1865, loss: 1.3642915487289429\n",
      "step 1866, loss: 1.4557631015777588\n",
      "step 1867, loss: 1.1532630920410156\n",
      "step 1868, loss: 2.606318235397339\n",
      "step 1869, loss: 2.7668814659118652\n",
      "step 1870, loss: 2.826975107192993\n",
      "step 1871, loss: 2.407844305038452\n",
      "step 1872, loss: 2.7040536403656006\n",
      "step 1873, loss: 2.852180242538452\n",
      "step 1874, loss: 2.745687246322632\n",
      "step 1875, loss: 3.3285751342773438\n",
      "step 1876, loss: 3.283529281616211\n",
      "step 1877, loss: 3.2272443771362305\n",
      "step 1878, loss: 3.3650572299957275\n",
      "step 1879, loss: 2.620083808898926\n",
      "step 1880, loss: 3.175877094268799\n",
      "step 1881, loss: 2.8469743728637695\n",
      "step 1882, loss: 3.2015318870544434\n",
      "step 1883, loss: 2.9874160289764404\n",
      "step 1884, loss: 3.1118221282958984\n",
      "step 1885, loss: 3.0692737102508545\n",
      "step 1886, loss: 3.180450201034546\n",
      "step 1887, loss: 3.05593204498291\n",
      "step 1888, loss: 2.8639252185821533\n",
      "step 1889, loss: 3.196622848510742\n",
      "step 1890, loss: 3.0215020179748535\n",
      "step 1891, loss: 2.9526500701904297\n",
      "step 1892, loss: 2.6910245418548584\n",
      "step 1893, loss: 2.97050142288208\n",
      "step 1894, loss: 2.821195363998413\n",
      "step 1895, loss: 2.926276922225952\n",
      "step 1896, loss: 2.90354061126709\n",
      "step 1897, loss: 2.765756368637085\n",
      "step 1898, loss: 3.096985101699829\n",
      "step 1899, loss: 2.9980385303497314\n",
      "step 1900, loss: 3.2710723876953125\n",
      "step 1901, loss: 3.622673511505127\n",
      "step 1902, loss: 3.2751190662384033\n",
      "step 1903, loss: 3.1733758449554443\n",
      "step 1904, loss: 3.3433094024658203\n",
      "step 1905, loss: 3.544914722442627\n",
      "step 1906, loss: 3.483973503112793\n",
      "step 1907, loss: 3.1668903827667236\n",
      "step 1908, loss: 3.169693946838379\n",
      "step 1909, loss: 3.2991199493408203\n",
      "step 1910, loss: 3.314988136291504\n",
      "step 1911, loss: 3.0250704288482666\n",
      "step 1912, loss: 3.108335018157959\n",
      "step 1913, loss: 3.2817599773406982\n",
      "step 1914, loss: 3.242539405822754\n",
      "step 1915, loss: 3.1531693935394287\n",
      "step 1916, loss: 3.3827438354492188\n",
      "step 1917, loss: 2.9302163124084473\n",
      "step 1918, loss: 3.0397982597351074\n",
      "step 1919, loss: 3.1526801586151123\n",
      "step 1920, loss: 3.114353656768799\n",
      "step 1921, loss: 2.9244954586029053\n",
      "step 1922, loss: 3.235368251800537\n",
      "step 1923, loss: 3.1362357139587402\n",
      "step 1924, loss: 3.0427465438842773\n",
      "step 1925, loss: 3.1299028396606445\n",
      "step 1926, loss: 3.0649006366729736\n",
      "step 1927, loss: 2.8688950538635254\n",
      "step 1928, loss: 3.061075448989868\n",
      "step 1929, loss: 3.569561719894409\n",
      "step 1930, loss: 2.9932117462158203\n",
      "step 1931, loss: 3.5059309005737305\n",
      "step 1932, loss: 2.6501071453094482\n",
      "step 1933, loss: 3.5035502910614014\n",
      "step 1934, loss: 3.4412736892700195\n",
      "step 1935, loss: 3.289445638656616\n",
      "step 1936, loss: 3.294081926345825\n",
      "step 1937, loss: 3.4391305446624756\n",
      "step 1938, loss: 3.695134401321411\n",
      "step 1939, loss: 3.657259225845337\n",
      "step 1940, loss: 3.436922788619995\n",
      "step 1941, loss: 3.0950827598571777\n",
      "step 1942, loss: 3.4381353855133057\n",
      "step 1943, loss: 3.3169329166412354\n",
      "step 1944, loss: 3.399550199508667\n",
      "step 1945, loss: 3.1120498180389404\n",
      "step 1946, loss: 3.299325466156006\n",
      "step 1947, loss: 3.363057851791382\n",
      "step 1948, loss: 3.267885684967041\n",
      "step 1949, loss: 3.5007503032684326\n",
      "step 1950, loss: 3.0150692462921143\n",
      "step 1951, loss: 3.2053489685058594\n",
      "step 1952, loss: 3.0309417247772217\n",
      "step 1953, loss: 3.194133996963501\n",
      "step 1954, loss: 3.529402732849121\n",
      "step 1955, loss: 3.669564723968506\n",
      "step 1956, loss: 3.1431338787078857\n",
      "step 1957, loss: 2.9902918338775635\n",
      "step 1958, loss: 3.3613548278808594\n",
      "step 1959, loss: 3.290142059326172\n",
      "step 1960, loss: 3.1213622093200684\n",
      "step 1961, loss: 3.061814069747925\n",
      "step 1962, loss: 3.516740322113037\n",
      "step 1963, loss: 3.4162096977233887\n",
      "step 1964, loss: 3.0757105350494385\n",
      "step 1965, loss: 3.407102108001709\n",
      "step 1966, loss: 3.50363826751709\n",
      "step 1967, loss: 3.2264084815979004\n",
      "step 1968, loss: 2.9838478565216064\n",
      "step 1969, loss: 3.1790988445281982\n",
      "step 1970, loss: 3.5829222202301025\n",
      "step 1971, loss: 3.0689401626586914\n",
      "step 1972, loss: 3.074704647064209\n",
      "step 1973, loss: 3.190805673599243\n",
      "step 1974, loss: 3.4341771602630615\n",
      "step 1975, loss: 3.34100604057312\n",
      "step 1976, loss: 3.220156669616699\n",
      "step 1977, loss: 3.411799192428589\n",
      "step 1978, loss: 3.4881551265716553\n",
      "step 1979, loss: 3.314162254333496\n",
      "step 1980, loss: 3.003948926925659\n",
      "step 1981, loss: 3.1442275047302246\n",
      "step 1982, loss: 3.405641555786133\n",
      "step 1983, loss: 3.365882158279419\n",
      "step 1984, loss: 3.2284884452819824\n",
      "step 1985, loss: 3.058964967727661\n",
      "step 1986, loss: 3.5176639556884766\n",
      "step 1987, loss: 3.3468775749206543\n",
      "step 1988, loss: 3.1274917125701904\n",
      "step 1989, loss: 3.44891357421875\n",
      "step 1990, loss: 3.0871036052703857\n",
      "step 1991, loss: 3.0516490936279297\n",
      "step 1992, loss: 3.219801187515259\n",
      "step 1993, loss: 3.1813974380493164\n",
      "step 1994, loss: 3.2125892639160156\n",
      "step 1995, loss: 3.4424922466278076\n",
      "step 1996, loss: 3.0463438034057617\n",
      "step 1997, loss: 3.49540638923645\n",
      "step 1998, loss: 3.6062915325164795\n",
      "step 1999, loss: 3.256953239440918\n",
      "step 2000, loss: 3.4293007850646973\n",
      "step 2001, loss: 3.0741024017333984\n",
      "step 2002, loss: 3.8119139671325684\n",
      "step 2003, loss: 2.9107930660247803\n",
      "step 2004, loss: 3.411248207092285\n",
      "step 2005, loss: 3.643904447555542\n",
      "step 2006, loss: 3.265718698501587\n",
      "step 2007, loss: 3.307915210723877\n",
      "step 2008, loss: 3.598660707473755\n",
      "step 2009, loss: 3.534330129623413\n",
      "step 2010, loss: 3.2194628715515137\n",
      "step 2011, loss: 3.5261266231536865\n",
      "step 2012, loss: 3.304737091064453\n",
      "step 2013, loss: 3.462144374847412\n",
      "step 2014, loss: 3.798496961593628\n",
      "step 2015, loss: 3.5482428073883057\n",
      "step 2016, loss: 3.5359346866607666\n",
      "step 2017, loss: 3.4108400344848633\n",
      "step 2018, loss: 3.2625107765197754\n",
      "step 2019, loss: 3.454091787338257\n",
      "step 2020, loss: 3.580134153366089\n",
      "step 2021, loss: 3.28588604927063\n",
      "step 2022, loss: 3.3465006351470947\n",
      "step 2023, loss: 3.1699037551879883\n",
      "step 2024, loss: 3.46944260597229\n",
      "step 2025, loss: 3.1605405807495117\n",
      "step 2026, loss: 3.519258737564087\n",
      "step 2027, loss: 3.5617856979370117\n",
      "step 2028, loss: 3.2904584407806396\n",
      "step 2029, loss: 3.4199414253234863\n",
      "step 2030, loss: 3.1531152725219727\n",
      "step 2031, loss: 2.936964273452759\n",
      "step 2032, loss: 3.1106619834899902\n",
      "step 2033, loss: 3.3733468055725098\n",
      "step 2034, loss: 3.373211622238159\n",
      "step 2035, loss: 3.0209014415740967\n",
      "step 2036, loss: 3.1499431133270264\n",
      "step 2037, loss: 3.2229857444763184\n",
      "step 2038, loss: 3.0258891582489014\n",
      "step 2039, loss: 3.005070924758911\n",
      "step 2040, loss: 3.1332526206970215\n",
      "step 2041, loss: 3.480987548828125\n",
      "step 2042, loss: 3.4735541343688965\n",
      "step 2043, loss: 3.2379424571990967\n",
      "step 2044, loss: 3.2079994678497314\n",
      "step 2045, loss: 3.0946264266967773\n",
      "step 2046, loss: 3.3519725799560547\n",
      "step 2047, loss: 3.1012284755706787\n",
      "step 2048, loss: 3.1595165729522705\n",
      "step 2049, loss: 3.3588411808013916\n",
      "step 2050, loss: 3.021850824356079\n",
      "step 2051, loss: 2.955369234085083\n",
      "step 2052, loss: 1.797642707824707\n",
      "step 2053, loss: 1.9534026384353638\n",
      "step 2054, loss: 2.0864036083221436\n",
      "step 2055, loss: 1.8044254779815674\n",
      "step 2056, loss: 1.6279735565185547\n",
      "step 2057, loss: 1.71648108959198\n",
      "step 2058, loss: 1.6139336824417114\n",
      "step 2059, loss: 1.7745779752731323\n",
      "step 2060, loss: 1.7655044794082642\n",
      "step 2061, loss: 1.6429003477096558\n",
      "step 2062, loss: 1.642533540725708\n",
      "step 2063, loss: 1.5760090351104736\n",
      "step 2064, loss: 1.856644630432129\n",
      "step 2065, loss: 1.5941786766052246\n",
      "step 2066, loss: 1.6942161321640015\n",
      "step 2067, loss: 1.8242839574813843\n",
      "step 2068, loss: 1.7128689289093018\n",
      "step 2069, loss: 1.5046998262405396\n",
      "step 2070, loss: 1.7984130382537842\n",
      "step 2071, loss: 1.825011968612671\n",
      "step 2072, loss: 1.2192596197128296\n",
      "step 2073, loss: 1.2583096027374268\n",
      "step 2074, loss: 1.7252241373062134\n",
      "step 2075, loss: 1.525092363357544\n",
      "step 2076, loss: 1.7093310356140137\n",
      "step 2077, loss: 1.2833853960037231\n",
      "step 2078, loss: 1.5494598150253296\n",
      "step 2079, loss: 1.541903018951416\n",
      "step 2080, loss: 1.5721542835235596\n",
      "step 2081, loss: 1.5474720001220703\n",
      "step 2082, loss: 1.7399123907089233\n",
      "step 2083, loss: 3.128473997116089\n",
      "step 2084, loss: 3.232348918914795\n",
      "step 2085, loss: 3.2513368129730225\n",
      "step 2086, loss: 2.981325387954712\n",
      "step 2087, loss: 2.7118828296661377\n",
      "step 2088, loss: 2.775273084640503\n",
      "step 2089, loss: 3.050257921218872\n",
      "step 2090, loss: 3.0387580394744873\n",
      "step 2091, loss: 3.2085723876953125\n",
      "step 2092, loss: 2.9462790489196777\n",
      "step 2093, loss: 3.461059808731079\n",
      "step 2094, loss: 3.008833646774292\n",
      "step 2095, loss: 2.862494945526123\n",
      "step 2096, loss: 2.8294718265533447\n",
      "step 2097, loss: 2.8702220916748047\n",
      "step 2098, loss: 3.3901255130767822\n",
      "step 2099, loss: 3.406405448913574\n",
      "step 2100, loss: 2.950814723968506\n",
      "step 2101, loss: 3.4173786640167236\n",
      "step 2102, loss: 3.226731538772583\n",
      "step 2103, loss: 3.063123941421509\n",
      "step 2104, loss: 3.0190699100494385\n",
      "step 2105, loss: 3.056135654449463\n",
      "step 2106, loss: 3.0044307708740234\n",
      "step 2107, loss: 3.0446715354919434\n",
      "step 2108, loss: 3.388669013977051\n",
      "step 2109, loss: 2.8224170207977295\n",
      "step 2110, loss: 2.881535291671753\n",
      "step 2111, loss: 2.8712098598480225\n",
      "step 2112, loss: 3.057356595993042\n",
      "step 2113, loss: 3.4308676719665527\n",
      "step 2114, loss: 3.003239393234253\n",
      "step 2115, loss: 3.259340524673462\n",
      "step 2116, loss: 3.3829078674316406\n",
      "step 2117, loss: 3.3703458309173584\n",
      "step 2118, loss: 3.1921629905700684\n",
      "step 2119, loss: 2.9208500385284424\n",
      "step 2120, loss: 2.176196813583374\n",
      "step 2121, loss: 2.209367036819458\n",
      "step 2122, loss: 2.098942518234253\n",
      "step 2123, loss: 2.0145347118377686\n",
      "step 2124, loss: 1.864211916923523\n",
      "step 2125, loss: 2.200690269470215\n",
      "step 2126, loss: 1.859610915184021\n",
      "step 2127, loss: 2.1409904956817627\n",
      "step 2128, loss: 1.8989790678024292\n",
      "step 2129, loss: 1.4739902019500732\n",
      "step 2130, loss: 1.75471031665802\n",
      "step 2131, loss: 1.4011436700820923\n",
      "step 2132, loss: 1.3462611436843872\n",
      "step 2133, loss: 1.638750433921814\n",
      "step 2134, loss: 1.4252272844314575\n",
      "step 2135, loss: 1.0000123977661133\n",
      "step 2136, loss: 2.757319688796997\n",
      "step 2137, loss: 2.7903573513031006\n",
      "step 2138, loss: 3.031500816345215\n",
      "step 2139, loss: 3.203167676925659\n",
      "step 2140, loss: 3.06266188621521\n",
      "step 2141, loss: 2.9140024185180664\n",
      "step 2142, loss: 2.938695192337036\n",
      "step 2143, loss: 2.8713889122009277\n",
      "step 2144, loss: 2.796830177307129\n",
      "step 2145, loss: 2.7896509170532227\n",
      "step 2146, loss: 2.875178337097168\n",
      "step 2147, loss: 2.8783862590789795\n",
      "step 2148, loss: 2.7899680137634277\n",
      "step 2149, loss: 2.8798935413360596\n",
      "step 2150, loss: 3.298506736755371\n",
      "step 2151, loss: 3.1389174461364746\n",
      "step 2152, loss: 2.6966495513916016\n",
      "step 2153, loss: 2.888470411300659\n",
      "step 2154, loss: 3.0570478439331055\n",
      "step 2155, loss: 2.6334898471832275\n",
      "step 2156, loss: 2.5934019088745117\n",
      "step 2157, loss: 2.837653875350952\n",
      "step 2158, loss: 2.8108627796173096\n",
      "step 2159, loss: 2.8439879417419434\n",
      "step 2160, loss: 2.8434901237487793\n",
      "step 2161, loss: 2.877155303955078\n",
      "step 2162, loss: 2.9032580852508545\n",
      "step 2163, loss: 2.9972879886627197\n",
      "step 2164, loss: 2.792024612426758\n",
      "step 2165, loss: 2.6676576137542725\n",
      "step 2166, loss: 2.778332233428955\n",
      "step 2167, loss: 2.902503252029419\n",
      "step 2168, loss: 2.793285608291626\n",
      "step 2169, loss: 2.5397493839263916\n",
      "step 2170, loss: 2.8315672874450684\n",
      "step 2171, loss: 2.915936231613159\n",
      "step 2172, loss: 2.7041375637054443\n",
      "step 2173, loss: 2.9056406021118164\n",
      "step 2174, loss: 3.0438027381896973\n",
      "step 2175, loss: 2.322866678237915\n",
      "step 2176, loss: 2.7705178260803223\n",
      "step 2177, loss: 2.6756908893585205\n",
      "step 2178, loss: 3.1006288528442383\n",
      "step 2179, loss: 2.858057737350464\n",
      "step 2180, loss: 2.5858843326568604\n",
      "step 2181, loss: 2.7589662075042725\n",
      "step 2182, loss: 2.9502601623535156\n",
      "step 2183, loss: 2.8606746196746826\n",
      "step 2184, loss: 2.974209785461426\n",
      "step 2185, loss: 2.720132350921631\n",
      "step 2186, loss: 2.262596607208252\n",
      "step 2187, loss: 2.6444554328918457\n",
      "step 2188, loss: 2.9584848880767822\n",
      "step 2189, loss: 3.2925374507904053\n",
      "step 2190, loss: 2.917280435562134\n",
      "step 2191, loss: 3.126601219177246\n",
      "step 2192, loss: 2.6323020458221436\n",
      "step 2193, loss: 3.175255298614502\n",
      "step 2194, loss: 2.7950189113616943\n",
      "step 2195, loss: 2.8117146492004395\n",
      "step 2196, loss: 2.812267780303955\n",
      "step 2197, loss: 2.603804588317871\n",
      "step 2198, loss: 2.72221040725708\n",
      "step 2199, loss: 2.8986737728118896\n",
      "step 2200, loss: 2.6723389625549316\n",
      "step 2201, loss: 2.7867493629455566\n",
      "step 2202, loss: 2.8721423149108887\n",
      "step 2203, loss: 2.9148359298706055\n",
      "step 2204, loss: 2.798933982849121\n",
      "step 2205, loss: 3.0435545444488525\n",
      "step 2206, loss: 3.1417429447174072\n",
      "step 2207, loss: 2.7513787746429443\n",
      "step 2208, loss: 2.865381956100464\n",
      "step 2209, loss: 2.7888033390045166\n",
      "step 2210, loss: 2.8471450805664062\n",
      "step 2211, loss: 2.61444354057312\n",
      "step 2212, loss: 2.9573357105255127\n",
      "step 2213, loss: 2.8771486282348633\n",
      "step 2214, loss: 2.951486110687256\n",
      "step 2215, loss: 3.01529860496521\n",
      "step 2216, loss: 2.9971742630004883\n",
      "step 2217, loss: 2.8468873500823975\n",
      "step 2218, loss: 3.0178475379943848\n",
      "step 2219, loss: 2.9584429264068604\n",
      "step 2220, loss: 3.2403128147125244\n",
      "step 2221, loss: 2.8682773113250732\n",
      "step 2222, loss: 2.957655429840088\n",
      "step 2223, loss: 3.1907472610473633\n",
      "step 2224, loss: 2.461517095565796\n",
      "step 2225, loss: 2.3157646656036377\n",
      "step 2226, loss: 2.022430896759033\n",
      "step 2227, loss: 1.9473165273666382\n",
      "step 2228, loss: 2.4543018341064453\n",
      "step 2229, loss: 2.1872029304504395\n",
      "step 2230, loss: 1.816271185874939\n",
      "step 2231, loss: 1.7995306253433228\n",
      "step 2232, loss: 2.0764505863189697\n",
      "step 2233, loss: 1.4244850873947144\n",
      "step 2234, loss: 1.6738938093185425\n",
      "step 2235, loss: 1.7650041580200195\n",
      "step 2236, loss: 1.4974347352981567\n",
      "step 2237, loss: 1.5067726373672485\n",
      "step 2238, loss: 1.7551357746124268\n",
      "step 2239, loss: 1.883830189704895\n",
      "step 2240, loss: 2.2916855812072754\n",
      "step 2241, loss: 2.477713108062744\n",
      "step 2242, loss: 2.4488065242767334\n",
      "step 2243, loss: 2.806232213973999\n",
      "step 2244, loss: 2.582195997238159\n",
      "step 2245, loss: 2.600435972213745\n",
      "step 2246, loss: 2.370295524597168\n",
      "step 2247, loss: 2.308168649673462\n",
      "step 2248, loss: 2.547793388366699\n",
      "step 2249, loss: 2.9477031230926514\n",
      "step 2250, loss: 2.5075979232788086\n",
      "step 2251, loss: 2.7500367164611816\n",
      "step 2252, loss: 2.689674139022827\n",
      "step 2253, loss: 2.7075018882751465\n",
      "step 2254, loss: 2.6042017936706543\n",
      "step 2255, loss: 2.485685110092163\n",
      "step 2256, loss: 2.425469160079956\n",
      "step 2257, loss: 2.5000860691070557\n",
      "step 2258, loss: 2.569971799850464\n",
      "step 2259, loss: 2.4625799655914307\n",
      "step 2260, loss: 2.6875619888305664\n",
      "step 2261, loss: 2.6219630241394043\n",
      "step 2262, loss: 2.615203380584717\n",
      "step 2263, loss: 2.5906946659088135\n",
      "step 2264, loss: 2.7262344360351562\n",
      "step 2265, loss: 2.643338680267334\n",
      "step 2266, loss: 2.5063118934631348\n",
      "step 2267, loss: 2.6891567707061768\n",
      "step 2268, loss: 2.7963624000549316\n",
      "step 2269, loss: 2.7380874156951904\n",
      "step 2270, loss: 2.247694492340088\n",
      "step 2271, loss: 2.5097479820251465\n",
      "step 2272, loss: 2.7270820140838623\n",
      "step 2273, loss: 2.583336353302002\n",
      "step 2274, loss: 2.675769329071045\n",
      "step 2275, loss: 2.3560969829559326\n",
      "step 2276, loss: 2.630821704864502\n",
      "step 2277, loss: 2.4988081455230713\n",
      "step 2278, loss: 2.443007707595825\n",
      "step 2279, loss: 2.589155912399292\n",
      "step 2280, loss: 2.2917091846466064\n",
      "step 2281, loss: 2.53060245513916\n",
      "step 2282, loss: 2.5486738681793213\n",
      "step 2283, loss: 2.515932083129883\n",
      "step 2284, loss: 2.347477436065674\n",
      "step 2285, loss: 2.6331863403320312\n",
      "step 2286, loss: 2.3843557834625244\n",
      "step 2287, loss: 2.3204169273376465\n",
      "step 2288, loss: 2.384321689605713\n",
      "step 2289, loss: 2.2843902111053467\n",
      "step 2290, loss: 2.5122694969177246\n",
      "step 2291, loss: 2.6754443645477295\n",
      "step 2292, loss: 2.599393844604492\n",
      "step 2293, loss: 2.468632459640503\n",
      "step 2294, loss: 2.485142707824707\n",
      "step 2295, loss: 2.161715507507324\n",
      "step 2296, loss: 2.4033734798431396\n",
      "step 2297, loss: 2.720320463180542\n",
      "step 2298, loss: 2.1471431255340576\n",
      "step 2299, loss: 1.4865268468856812\n",
      "step 2300, loss: 1.312028408050537\n",
      "step 2301, loss: 1.3407868146896362\n",
      "step 2302, loss: 1.3904082775115967\n",
      "step 2303, loss: 1.2476742267608643\n",
      "step 2304, loss: 1.0657023191452026\n",
      "step 2305, loss: 1.2473224401474\n",
      "step 2306, loss: 1.314773678779602\n",
      "step 2307, loss: 1.2950448989868164\n",
      "step 2308, loss: 1.2793601751327515\n",
      "step 2309, loss: 1.1154910326004028\n",
      "step 2310, loss: 1.2199090719223022\n",
      "step 2311, loss: 1.587131142616272\n",
      "step 2312, loss: 2.03766131401062\n",
      "step 2313, loss: 2.5489110946655273\n",
      "step 2314, loss: 2.391178846359253\n",
      "step 2315, loss: 2.384772777557373\n",
      "step 2316, loss: 2.4405899047851562\n",
      "step 2317, loss: 2.3266615867614746\n",
      "step 2318, loss: 2.352792739868164\n",
      "step 2319, loss: 2.6896796226501465\n",
      "step 2320, loss: 2.38045334815979\n",
      "step 2321, loss: 2.4270756244659424\n",
      "step 2322, loss: 2.56122088432312\n",
      "step 2323, loss: 2.59854793548584\n",
      "step 2324, loss: 2.097954511642456\n",
      "step 2325, loss: 2.6719093322753906\n",
      "step 2326, loss: 2.598381996154785\n",
      "step 2327, loss: 2.476374387741089\n",
      "step 2328, loss: 2.786064624786377\n",
      "step 2329, loss: 2.5179591178894043\n",
      "step 2330, loss: 2.697465419769287\n",
      "step 2331, loss: 2.636103391647339\n",
      "step 2332, loss: 2.515007972717285\n",
      "step 2333, loss: 2.601930618286133\n",
      "step 2334, loss: 2.602169990539551\n",
      "step 2335, loss: 2.193324565887451\n",
      "step 2336, loss: 2.406367778778076\n",
      "step 2337, loss: 2.5010411739349365\n",
      "step 2338, loss: 2.4853291511535645\n",
      "step 2339, loss: 2.463127374649048\n",
      "step 2340, loss: 2.6089649200439453\n",
      "step 2341, loss: 2.587027072906494\n",
      "step 2342, loss: 2.309617757797241\n",
      "step 2343, loss: 2.548553705215454\n",
      "step 2344, loss: 2.3513827323913574\n",
      "step 2345, loss: 2.5018186569213867\n",
      "step 2346, loss: 2.3372178077697754\n",
      "step 2347, loss: 2.333345413208008\n",
      "step 2348, loss: 2.5179755687713623\n",
      "step 2349, loss: 2.4109370708465576\n",
      "step 2350, loss: 2.4100396633148193\n",
      "step 2351, loss: 2.4730849266052246\n",
      "step 2352, loss: 2.6843338012695312\n",
      "step 2353, loss: 2.364570140838623\n",
      "step 2354, loss: 2.2524900436401367\n",
      "step 2355, loss: 2.686415195465088\n",
      "step 2356, loss: 2.3625235557556152\n",
      "step 2357, loss: 2.476102590560913\n",
      "step 2358, loss: 2.7623918056488037\n",
      "step 2359, loss: 2.4585354328155518\n",
      "step 2360, loss: 2.3206167221069336\n",
      "step 2361, loss: 2.5618555545806885\n",
      "step 2362, loss: 2.3735299110412598\n",
      "step 2363, loss: 2.452101469039917\n",
      "step 2364, loss: 2.3914225101470947\n",
      "step 2365, loss: 2.530714988708496\n",
      "step 2366, loss: 2.560667037963867\n",
      "step 2367, loss: 2.18874454498291\n",
      "step 2368, loss: 2.362060785293579\n",
      "step 2369, loss: 2.596768379211426\n",
      "step 2370, loss: 2.579845428466797\n",
      "step 2371, loss: 2.5035464763641357\n",
      "step 2372, loss: 2.3507916927337646\n",
      "step 2373, loss: 2.563750982284546\n",
      "step 2374, loss: 2.5780045986175537\n",
      "step 2375, loss: 2.5653016567230225\n",
      "step 2376, loss: 2.365971088409424\n",
      "step 2377, loss: 2.52907395362854\n",
      "step 2378, loss: 2.3253650665283203\n",
      "step 2379, loss: 2.6529324054718018\n",
      "step 2380, loss: 2.695345401763916\n",
      "step 2381, loss: 2.5170161724090576\n",
      "step 2382, loss: 2.4136602878570557\n",
      "step 2383, loss: 2.5815045833587646\n",
      "step 2384, loss: 2.6723711490631104\n",
      "step 2385, loss: 2.53592848777771\n",
      "step 2386, loss: 2.651386260986328\n",
      "step 2387, loss: 2.6122169494628906\n",
      "step 2388, loss: 1.9195442199707031\n",
      "step 2389, loss: 2.419567346572876\n",
      "step 2390, loss: 2.487450122833252\n",
      "step 2391, loss: 2.4511239528656006\n",
      "step 2392, loss: 2.4106740951538086\n",
      "step 2393, loss: 2.4575765132904053\n",
      "step 2394, loss: 2.8147988319396973\n",
      "step 2395, loss: 2.462876796722412\n",
      "step 2396, loss: 2.6201095581054688\n",
      "step 2397, loss: 2.519312620162964\n",
      "step 2398, loss: 2.173614978790283\n",
      "step 2399, loss: 2.500922441482544\n",
      "step 2400, loss: 2.5116612911224365\n",
      "step 2401, loss: 2.606417179107666\n",
      "step 2402, loss: 2.656576156616211\n",
      "step 2403, loss: 2.622356653213501\n",
      "step 2404, loss: 2.8661036491394043\n",
      "step 2405, loss: 2.599952220916748\n",
      "step 2406, loss: 2.442464828491211\n",
      "step 2407, loss: 2.590130090713501\n",
      "step 2408, loss: 2.2426328659057617\n",
      "step 2409, loss: 2.7127654552459717\n",
      "step 2410, loss: 2.39798641204834\n",
      "step 2411, loss: 2.740793466567993\n",
      "step 2412, loss: 2.3742153644561768\n",
      "step 2413, loss: 2.547567367553711\n",
      "step 2414, loss: 2.449181079864502\n",
      "step 2415, loss: 2.520331621170044\n",
      "step 2416, loss: 2.5921664237976074\n",
      "step 2417, loss: 2.751096248626709\n",
      "step 2418, loss: 2.5658888816833496\n",
      "step 2419, loss: 2.699262857437134\n",
      "step 2420, loss: 2.4100635051727295\n",
      "step 2421, loss: 2.3770480155944824\n",
      "step 2422, loss: 2.7915523052215576\n",
      "step 2423, loss: 2.1720924377441406\n",
      "step 2424, loss: 1.550121545791626\n",
      "step 2425, loss: 1.4280626773834229\n",
      "step 2426, loss: 1.5282496213912964\n",
      "step 2427, loss: 1.1935889720916748\n",
      "step 2428, loss: 0.9248420596122742\n",
      "step 2429, loss: 1.3175979852676392\n",
      "step 2430, loss: 0.9659087657928467\n",
      "step 2431, loss: 1.123709797859192\n",
      "step 2432, loss: 1.1876357793807983\n",
      "step 2433, loss: 1.1110824346542358\n",
      "step 2434, loss: 1.1264764070510864\n",
      "step 2435, loss: 1.2512714862823486\n",
      "step 2436, loss: 1.278375506401062\n",
      "step 2437, loss: 0.9811259508132935\n",
      "step 2438, loss: 0.8027130961418152\n",
      "step 2439, loss: 1.2635871171951294\n",
      "step 2440, loss: 0.9655834436416626\n",
      "step 2441, loss: 1.1935274600982666\n",
      "step 2442, loss: 1.2208391427993774\n",
      "step 2443, loss: 0.8036489486694336\n",
      "step 2444, loss: 1.336765170097351\n",
      "step 2445, loss: 1.3895612955093384\n",
      "step 2446, loss: 1.2621864080429077\n",
      "step 2447, loss: 1.2963505983352661\n",
      "step 2448, loss: 1.0212382078170776\n",
      "step 2449, loss: 1.1536505222320557\n",
      "step 2450, loss: 1.3119652271270752\n",
      "step 2451, loss: 1.4058382511138916\n",
      "step 2452, loss: 2.2191824913024902\n",
      "step 2453, loss: 2.3515021800994873\n",
      "step 2454, loss: 2.5454986095428467\n",
      "step 2455, loss: 2.539823055267334\n",
      "step 2456, loss: 2.4755373001098633\n",
      "step 2457, loss: 2.435065746307373\n",
      "step 2458, loss: 2.4258077144622803\n",
      "step 2459, loss: 2.368532180786133\n",
      "step 2460, loss: 2.3357391357421875\n",
      "step 2461, loss: 2.5250043869018555\n",
      "step 2462, loss: 2.7172865867614746\n",
      "step 2463, loss: 2.6587443351745605\n",
      "step 2464, loss: 2.5890352725982666\n",
      "step 2465, loss: 2.6294829845428467\n",
      "step 2466, loss: 2.654905080795288\n",
      "step 2467, loss: 2.564727306365967\n",
      "step 2468, loss: 2.5124106407165527\n",
      "step 2469, loss: 2.48756742477417\n",
      "step 2470, loss: 2.3923981189727783\n",
      "step 2471, loss: 2.6180193424224854\n",
      "step 2472, loss: 2.5822675228118896\n",
      "step 2473, loss: 2.3740782737731934\n",
      "step 2474, loss: 2.4395837783813477\n",
      "step 2475, loss: 2.4073851108551025\n",
      "step 2476, loss: 2.4538683891296387\n",
      "step 2477, loss: 2.3607571125030518\n",
      "step 2478, loss: 2.4719460010528564\n",
      "step 2479, loss: 2.519355535507202\n",
      "step 2480, loss: 2.390204668045044\n",
      "step 2481, loss: 2.200528383255005\n",
      "step 2482, loss: 2.2317872047424316\n",
      "step 2483, loss: 2.34999680519104\n",
      "step 2484, loss: 2.5893218517303467\n",
      "step 2485, loss: 2.276278018951416\n",
      "step 2486, loss: 2.3729357719421387\n",
      "step 2487, loss: 2.364352226257324\n",
      "step 2488, loss: 2.499549627304077\n",
      "step 2489, loss: 2.4145007133483887\n",
      "step 2490, loss: 2.6321163177490234\n",
      "step 2491, loss: 2.1901137828826904\n",
      "step 2492, loss: 2.592789649963379\n",
      "step 2493, loss: 2.645282745361328\n",
      "step 2494, loss: 2.337006092071533\n",
      "step 2495, loss: 2.468825578689575\n",
      "step 2496, loss: 2.2859926223754883\n",
      "step 2497, loss: 2.3431687355041504\n",
      "step 2498, loss: 2.1692774295806885\n",
      "step 2499, loss: 2.499938488006592\n",
      "step 2500, loss: 2.1634104251861572\n",
      "step 2501, loss: 2.3022069931030273\n",
      "step 2502, loss: 2.232379674911499\n",
      "step 2503, loss: 2.1922335624694824\n",
      "step 2504, loss: 1.9968360662460327\n",
      "step 2505, loss: 2.437113046646118\n",
      "step 2506, loss: 2.581542491912842\n",
      "step 2507, loss: 2.312418222427368\n",
      "step 2508, loss: 2.2878265380859375\n",
      "step 2509, loss: 2.385647773742676\n",
      "step 2510, loss: 2.2349395751953125\n",
      "step 2511, loss: 2.35832142829895\n",
      "step 2512, loss: 2.438800096511841\n",
      "step 2513, loss: 2.4834892749786377\n",
      "step 2514, loss: 2.369866371154785\n",
      "step 2515, loss: 2.487083911895752\n",
      "step 2516, loss: 2.131429672241211\n",
      "step 2517, loss: 2.2316720485687256\n",
      "step 2518, loss: 2.227280378341675\n",
      "step 2519, loss: 2.5528111457824707\n",
      "step 2520, loss: 2.332996368408203\n",
      "step 2521, loss: 2.4317517280578613\n",
      "step 2522, loss: 2.3002686500549316\n",
      "step 2523, loss: 2.4287564754486084\n",
      "step 2524, loss: 2.6014461517333984\n",
      "step 2525, loss: 2.5236051082611084\n",
      "step 2526, loss: 2.650533437728882\n",
      "step 2527, loss: 2.3853728771209717\n",
      "step 2528, loss: 2.577502727508545\n",
      "step 2529, loss: 2.4942562580108643\n",
      "step 2530, loss: 2.35214900970459\n",
      "step 2531, loss: 2.715228319168091\n",
      "step 2532, loss: 2.2740347385406494\n",
      "step 2533, loss: 2.533526659011841\n",
      "step 2534, loss: 2.584317684173584\n",
      "step 2535, loss: 2.5307586193084717\n",
      "step 2536, loss: 2.5179407596588135\n",
      "step 2537, loss: 2.743471384048462\n",
      "step 2538, loss: 2.5827410221099854\n",
      "step 2539, loss: 2.363537073135376\n",
      "step 2540, loss: 2.5110459327697754\n",
      "step 2541, loss: 2.539137363433838\n",
      "step 2542, loss: 2.4211196899414062\n",
      "step 2543, loss: 2.585726261138916\n",
      "step 2544, loss: 2.276705265045166\n",
      "step 2545, loss: 2.3903450965881348\n",
      "step 2546, loss: 2.3062736988067627\n",
      "step 2547, loss: 2.3080646991729736\n",
      "step 2548, loss: 1.78361177444458\n",
      "step 2549, loss: 1.753779649734497\n",
      "step 2550, loss: 1.5232489109039307\n",
      "step 2551, loss: 1.5791348218917847\n",
      "step 2552, loss: 1.7828645706176758\n",
      "step 2553, loss: 1.7742259502410889\n",
      "step 2554, loss: 1.6405482292175293\n",
      "step 2555, loss: 1.5206230878829956\n",
      "step 2556, loss: 1.6059964895248413\n",
      "step 2557, loss: 1.2362680435180664\n",
      "step 2558, loss: 1.2958571910858154\n",
      "step 2559, loss: 1.3770382404327393\n",
      "step 2560, loss: 1.251977562904358\n",
      "step 2561, loss: 1.0541503429412842\n",
      "step 2562, loss: 1.0964487791061401\n",
      "step 2563, loss: 0.8390739560127258\n",
      "step 2564, loss: 1.16167414188385\n",
      "step 2565, loss: 1.4615986347198486\n",
      "step 2566, loss: 1.206493616104126\n",
      "step 2567, loss: 1.079923391342163\n",
      "step 2568, loss: 1.184890866279602\n",
      "step 2569, loss: 1.0858144760131836\n",
      "step 2570, loss: 2.052867889404297\n",
      "step 2571, loss: 2.250438928604126\n",
      "step 2572, loss: 2.158676862716675\n",
      "step 2573, loss: 2.106025218963623\n",
      "step 2574, loss: 2.627082347869873\n",
      "step 2575, loss: 2.3017752170562744\n",
      "step 2576, loss: 2.1419990062713623\n",
      "step 2577, loss: 2.2429466247558594\n",
      "step 2578, loss: 2.280386209487915\n",
      "step 2579, loss: 2.3765499591827393\n",
      "step 2580, loss: 2.4297382831573486\n",
      "step 2581, loss: 2.3214869499206543\n",
      "step 2582, loss: 2.258347272872925\n",
      "step 2583, loss: 2.7533748149871826\n",
      "step 2584, loss: 2.694714069366455\n",
      "step 2585, loss: 2.573413133621216\n",
      "step 2586, loss: 2.4961719512939453\n",
      "step 2587, loss: 2.7797114849090576\n",
      "step 2588, loss: 2.770930051803589\n",
      "step 2589, loss: 2.680518388748169\n",
      "step 2590, loss: 2.7177882194519043\n",
      "step 2591, loss: 2.565035581588745\n",
      "step 2592, loss: 2.492594003677368\n",
      "step 2593, loss: 2.6559853553771973\n",
      "step 2594, loss: 2.3351197242736816\n",
      "step 2595, loss: 2.2961056232452393\n",
      "step 2596, loss: 2.248358726501465\n",
      "step 2597, loss: 2.487605571746826\n",
      "step 2598, loss: 2.6389153003692627\n",
      "step 2599, loss: 2.518714427947998\n",
      "step 2600, loss: 2.440706253051758\n",
      "step 2601, loss: 2.749757766723633\n",
      "step 2602, loss: 2.693918466567993\n",
      "step 2603, loss: 2.963810920715332\n",
      "step 2604, loss: 2.51806902885437\n",
      "step 2605, loss: 2.864736318588257\n",
      "step 2606, loss: 2.8018078804016113\n",
      "step 2607, loss: 2.7763946056365967\n",
      "step 2608, loss: 2.635897159576416\n",
      "step 2609, loss: 2.7881863117218018\n",
      "step 2610, loss: 2.535061836242676\n",
      "step 2611, loss: 2.546349287033081\n",
      "step 2612, loss: 2.4902336597442627\n",
      "step 2613, loss: 2.692117929458618\n",
      "step 2614, loss: 2.872021198272705\n",
      "step 2615, loss: 2.745227813720703\n",
      "step 2616, loss: 2.699845790863037\n",
      "step 2617, loss: 2.5796754360198975\n",
      "step 2618, loss: 2.6051077842712402\n",
      "step 2619, loss: 2.2628471851348877\n",
      "step 2620, loss: 2.059253692626953\n",
      "step 2621, loss: 2.4696850776672363\n",
      "step 2622, loss: 2.510983943939209\n",
      "step 2623, loss: 2.8026418685913086\n",
      "step 2624, loss: 2.562641143798828\n",
      "step 2625, loss: 2.312889814376831\n",
      "step 2626, loss: 2.8218374252319336\n",
      "step 2627, loss: 2.642483711242676\n",
      "step 2628, loss: 2.684877872467041\n",
      "step 2629, loss: 2.5735671520233154\n",
      "step 2630, loss: 2.2817132472991943\n",
      "step 2631, loss: 2.6571953296661377\n",
      "step 2632, loss: 2.7591910362243652\n",
      "step 2633, loss: 2.8841187953948975\n",
      "step 2634, loss: 2.5429348945617676\n",
      "step 2635, loss: 2.780282497406006\n",
      "step 2636, loss: 2.932554244995117\n",
      "step 2637, loss: 2.6859853267669678\n",
      "step 2638, loss: 2.692978620529175\n",
      "step 2639, loss: 3.1038055419921875\n",
      "step 2640, loss: 2.8017852306365967\n",
      "step 2641, loss: 2.8695318698883057\n",
      "step 2642, loss: 2.8233964443206787\n",
      "step 2643, loss: 2.9332451820373535\n",
      "step 2644, loss: 2.6647844314575195\n",
      "step 2645, loss: 2.8138628005981445\n",
      "step 2646, loss: 2.558358907699585\n",
      "step 2647, loss: 3.0772433280944824\n",
      "step 2648, loss: 2.3301751613616943\n",
      "step 2649, loss: 2.5161900520324707\n",
      "step 2650, loss: 2.606994867324829\n",
      "step 2651, loss: 2.4964866638183594\n",
      "step 2652, loss: 2.8663249015808105\n",
      "step 2653, loss: 2.7517263889312744\n",
      "step 2654, loss: 2.6876537799835205\n",
      "step 2655, loss: 2.7857003211975098\n",
      "step 2656, loss: 2.844999074935913\n",
      "step 2657, loss: 2.738374948501587\n",
      "step 2658, loss: 2.5842695236206055\n",
      "step 2659, loss: 2.5839500427246094\n",
      "step 2660, loss: 1.786700963973999\n",
      "step 2661, loss: 1.8901827335357666\n",
      "step 2662, loss: 1.6669034957885742\n",
      "step 2663, loss: 1.3632662296295166\n",
      "step 2664, loss: 1.5149688720703125\n",
      "step 2665, loss: 1.187670111656189\n",
      "step 2666, loss: 1.196165680885315\n",
      "step 2667, loss: 1.2163364887237549\n",
      "step 2668, loss: 1.2580121755599976\n",
      "step 2669, loss: 1.2655929327011108\n",
      "step 2670, loss: 1.2391858100891113\n",
      "step 2671, loss: 1.125825047492981\n",
      "step 2672, loss: 1.0477955341339111\n",
      "step 2673, loss: 1.0672616958618164\n",
      "step 2674, loss: 0.9066413640975952\n",
      "step 2675, loss: 1.132197380065918\n",
      "step 2676, loss: 1.134132981300354\n",
      "step 2677, loss: 1.3122638463974\n",
      "step 2678, loss: 2.121157646179199\n",
      "step 2679, loss: 2.622809648513794\n",
      "step 2680, loss: 2.6709671020507812\n",
      "step 2681, loss: 2.8114941120147705\n",
      "step 2682, loss: 2.524122714996338\n",
      "step 2683, loss: 2.466402769088745\n",
      "step 2684, loss: 2.4077229499816895\n",
      "step 2685, loss: 2.704026937484741\n",
      "step 2686, loss: 2.4070277214050293\n",
      "step 2687, loss: 2.6517250537872314\n",
      "step 2688, loss: 2.6283209323883057\n",
      "step 2689, loss: 2.5699357986450195\n",
      "step 2690, loss: 2.662738084793091\n",
      "step 2691, loss: 2.671623468399048\n",
      "step 2692, loss: 2.6651737689971924\n",
      "step 2693, loss: 2.4986467361450195\n",
      "step 2694, loss: 2.526185989379883\n",
      "step 2695, loss: 2.573998212814331\n",
      "step 2696, loss: 2.346550226211548\n",
      "step 2697, loss: 2.70786190032959\n",
      "step 2698, loss: 2.4616267681121826\n",
      "step 2699, loss: 2.313746452331543\n",
      "step 2700, loss: 2.580644369125366\n",
      "step 2701, loss: 2.5212759971618652\n",
      "step 2702, loss: 2.636385917663574\n",
      "step 2703, loss: 2.842439889907837\n",
      "step 2704, loss: 2.4371070861816406\n",
      "step 2705, loss: 2.575571060180664\n",
      "step 2706, loss: 2.6655211448669434\n",
      "step 2707, loss: 2.5258140563964844\n",
      "step 2708, loss: 2.818559169769287\n",
      "step 2709, loss: 2.787652015686035\n",
      "step 2710, loss: 2.382756233215332\n",
      "step 2711, loss: 2.737677812576294\n",
      "step 2712, loss: 2.891740322113037\n",
      "step 2713, loss: 2.4383811950683594\n",
      "step 2714, loss: 2.6690144538879395\n",
      "step 2715, loss: 2.6662850379943848\n",
      "step 2716, loss: 2.9436652660369873\n",
      "step 2717, loss: 3.1942949295043945\n",
      "step 2718, loss: 2.4478750228881836\n",
      "step 2719, loss: 2.7147490978240967\n",
      "step 2720, loss: 2.663022756576538\n",
      "step 2721, loss: 2.3898849487304688\n",
      "step 2722, loss: 2.137381076812744\n",
      "step 2723, loss: 1.2068020105361938\n",
      "step 2724, loss: 1.3237515687942505\n",
      "step 2725, loss: 1.1618632078170776\n",
      "step 2726, loss: 1.142945408821106\n",
      "step 2727, loss: 0.9390759468078613\n",
      "step 2728, loss: 0.8767951130867004\n",
      "step 2729, loss: 0.7511932849884033\n",
      "step 2730, loss: 0.9228807687759399\n",
      "step 2731, loss: 0.8955315947532654\n",
      "step 2732, loss: 1.1162177324295044\n",
      "step 2733, loss: 1.0828630924224854\n",
      "step 2734, loss: 1.2524152994155884\n",
      "step 2735, loss: 1.226414680480957\n",
      "step 2736, loss: 1.4889453649520874\n",
      "step 2737, loss: 1.5983984470367432\n",
      "step 2738, loss: 2.6228761672973633\n",
      "step 2739, loss: 2.4161648750305176\n",
      "step 2740, loss: 2.306415557861328\n",
      "step 2741, loss: 2.280946969985962\n",
      "step 2742, loss: 2.1743760108947754\n",
      "step 2743, loss: 2.2418720722198486\n",
      "step 2744, loss: 2.3201630115509033\n",
      "step 2745, loss: 2.467425584793091\n",
      "step 2746, loss: 2.243922233581543\n",
      "step 2747, loss: 2.31101655960083\n",
      "step 2748, loss: 2.113114595413208\n",
      "step 2749, loss: 2.13801908493042\n",
      "step 2750, loss: 2.396852970123291\n",
      "step 2751, loss: 2.528510332107544\n",
      "step 2752, loss: 2.4876341819763184\n",
      "step 2753, loss: 2.4634461402893066\n",
      "step 2754, loss: 2.9618797302246094\n",
      "step 2755, loss: 2.230231761932373\n",
      "step 2756, loss: 2.2609124183654785\n",
      "step 2757, loss: 2.3958659172058105\n",
      "step 2758, loss: 2.43904709815979\n",
      "step 2759, loss: 2.261481523513794\n",
      "step 2760, loss: 2.6267573833465576\n",
      "step 2761, loss: 2.529693603515625\n",
      "step 2762, loss: 2.4611940383911133\n",
      "step 2763, loss: 2.5708489418029785\n",
      "step 2764, loss: 2.777803897857666\n",
      "step 2765, loss: 2.707942485809326\n",
      "step 2766, loss: 2.7140986919403076\n",
      "step 2767, loss: 2.71966552734375\n",
      "step 2768, loss: 2.554377317428589\n",
      "step 2769, loss: 2.5056986808776855\n",
      "step 2770, loss: 2.614912509918213\n",
      "step 2771, loss: 2.662564754486084\n",
      "step 2772, loss: 2.4444994926452637\n",
      "step 2773, loss: 2.696329355239868\n",
      "step 2774, loss: 2.554335355758667\n",
      "step 2775, loss: 2.866335391998291\n",
      "step 2776, loss: 2.8396990299224854\n",
      "step 2777, loss: 2.9115641117095947\n",
      "step 2778, loss: 2.84100604057312\n",
      "step 2779, loss: 2.546651601791382\n",
      "step 2780, loss: 2.395875930786133\n",
      "step 2781, loss: 2.8550548553466797\n",
      "step 2782, loss: 2.6514270305633545\n",
      "step 2783, loss: 2.7952303886413574\n",
      "step 2784, loss: 2.7734999656677246\n",
      "step 2785, loss: 2.5350897312164307\n",
      "step 2786, loss: 2.7023110389709473\n",
      "step 2787, loss: 2.726325511932373\n",
      "step 2788, loss: 2.3005471229553223\n",
      "step 2789, loss: 2.7572219371795654\n",
      "step 2790, loss: 2.6285479068756104\n",
      "step 2791, loss: 2.857593536376953\n",
      "step 2792, loss: 2.6105682849884033\n",
      "step 2793, loss: 2.580186367034912\n",
      "step 2794, loss: 2.6906142234802246\n",
      "step 2795, loss: 2.668177604675293\n",
      "step 2796, loss: 2.6433959007263184\n",
      "step 2797, loss: 2.564934730529785\n",
      "step 2798, loss: 3.0007686614990234\n",
      "step 2799, loss: 2.654559850692749\n",
      "step 2800, loss: 2.6937437057495117\n",
      "step 2801, loss: 2.633559465408325\n",
      "step 2802, loss: 2.6157901287078857\n",
      "step 2803, loss: 2.7911064624786377\n",
      "step 2804, loss: 2.821139097213745\n",
      "step 2805, loss: 2.6992099285125732\n",
      "step 2806, loss: 2.566138505935669\n",
      "step 2807, loss: 3.0353589057922363\n",
      "step 2808, loss: 2.4863479137420654\n",
      "step 2809, loss: 2.7679708003997803\n",
      "step 2810, loss: 2.702646493911743\n",
      "step 2811, loss: 2.507746934890747\n",
      "step 2812, loss: 2.869041681289673\n",
      "step 2813, loss: 2.5492289066314697\n",
      "step 2814, loss: 2.7449491024017334\n",
      "step 2815, loss: 2.459591865539551\n",
      "step 2816, loss: 2.841578722000122\n",
      "step 2817, loss: 2.5801775455474854\n",
      "step 2818, loss: 2.7690048217773438\n",
      "step 2819, loss: 2.4539122581481934\n",
      "step 2820, loss: 2.7177512645721436\n",
      "step 2821, loss: 2.6658525466918945\n",
      "step 2822, loss: 2.521745204925537\n",
      "step 2823, loss: 2.668449878692627\n",
      "step 2824, loss: 2.703756093978882\n",
      "step 2825, loss: 2.6963374614715576\n",
      "step 2826, loss: 2.5641560554504395\n",
      "step 2827, loss: 1.6385316848754883\n",
      "step 2828, loss: 1.2844539880752563\n",
      "step 2829, loss: 1.0882805585861206\n",
      "step 2830, loss: 1.078855276107788\n",
      "step 2831, loss: 1.1820321083068848\n",
      "step 2832, loss: 1.1226681470870972\n",
      "step 2833, loss: 1.2422415018081665\n",
      "step 2834, loss: 1.2132306098937988\n",
      "step 2835, loss: 1.0355076789855957\n",
      "step 2836, loss: 1.0202425718307495\n",
      "step 2837, loss: 1.070168375968933\n",
      "step 2838, loss: 0.8874806761741638\n",
      "step 2839, loss: 1.2657161951065063\n",
      "step 2840, loss: 1.0408681631088257\n",
      "step 2841, loss: 1.0333349704742432\n",
      "step 2842, loss: 0.8981294631958008\n",
      "step 2843, loss: 1.2103261947631836\n",
      "step 2844, loss: 0.9634016156196594\n",
      "step 2845, loss: 1.103986382484436\n",
      "step 2846, loss: 0.8997193574905396\n",
      "step 2847, loss: 0.923343300819397\n",
      "step 2848, loss: 1.3577646017074585\n",
      "step 2849, loss: 2.4192936420440674\n",
      "step 2850, loss: 2.3742666244506836\n",
      "step 2851, loss: 2.4374701976776123\n",
      "step 2852, loss: 2.0478274822235107\n",
      "step 2853, loss: 2.371124029159546\n",
      "step 2854, loss: 2.801459789276123\n",
      "step 2855, loss: 2.6289117336273193\n",
      "step 2856, loss: 2.088073492050171\n",
      "step 2857, loss: 2.3707892894744873\n",
      "step 2858, loss: 2.4370546340942383\n",
      "step 2859, loss: 2.280414342880249\n",
      "step 2860, loss: 2.5609169006347656\n",
      "step 2861, loss: 2.3074748516082764\n",
      "step 2862, loss: 2.804029703140259\n",
      "step 2863, loss: 2.2939319610595703\n",
      "step 2864, loss: 2.4274978637695312\n",
      "step 2865, loss: 2.4023094177246094\n",
      "step 2866, loss: 2.284541606903076\n",
      "step 2867, loss: 2.521969795227051\n",
      "step 2868, loss: 2.448603630065918\n",
      "step 2869, loss: 2.474233627319336\n",
      "step 2870, loss: 2.4531524181365967\n",
      "step 2871, loss: 2.384963274002075\n",
      "step 2872, loss: 2.7262179851531982\n",
      "step 2873, loss: 2.175971031188965\n",
      "step 2874, loss: 2.4141321182250977\n",
      "step 2875, loss: 2.448657989501953\n",
      "step 2876, loss: 2.3616466522216797\n",
      "step 2877, loss: 2.4407103061676025\n",
      "step 2878, loss: 2.264155149459839\n",
      "step 2879, loss: 2.430695056915283\n",
      "step 2880, loss: 2.4782419204711914\n",
      "step 2881, loss: 2.3793983459472656\n",
      "step 2882, loss: 2.3758747577667236\n",
      "step 2883, loss: 2.181536912918091\n",
      "step 2884, loss: 2.3163223266601562\n",
      "step 2885, loss: 2.7550787925720215\n",
      "step 2886, loss: 2.7660717964172363\n",
      "step 2887, loss: 2.6452369689941406\n",
      "step 2888, loss: 2.1197760105133057\n",
      "step 2889, loss: 2.4848480224609375\n",
      "step 2890, loss: 2.4249367713928223\n",
      "step 2891, loss: 2.4516592025756836\n",
      "step 2892, loss: 2.2077383995056152\n",
      "step 2893, loss: 2.468319892883301\n",
      "step 2894, loss: 2.5138494968414307\n",
      "step 2895, loss: 2.4545202255249023\n",
      "step 2896, loss: 2.4413599967956543\n",
      "step 2897, loss: 2.260894775390625\n",
      "step 2898, loss: 2.4083292484283447\n",
      "step 2899, loss: 2.4091832637786865\n",
      "step 2900, loss: 2.4719903469085693\n",
      "step 2901, loss: 2.419663906097412\n",
      "step 2902, loss: 2.6157822608947754\n",
      "step 2903, loss: 2.539940357208252\n",
      "step 2904, loss: 2.3902859687805176\n",
      "step 2905, loss: 2.6910476684570312\n",
      "step 2906, loss: 2.5566890239715576\n",
      "step 2907, loss: 2.3214521408081055\n",
      "step 2908, loss: 2.2824225425720215\n",
      "step 2909, loss: 2.2264299392700195\n",
      "step 2910, loss: 2.495176076889038\n",
      "step 2911, loss: 2.3762753009796143\n",
      "step 2912, loss: 2.4112558364868164\n",
      "step 2913, loss: 2.545154571533203\n",
      "step 2914, loss: 2.693570613861084\n",
      "step 2915, loss: 2.6339783668518066\n",
      "step 2916, loss: 2.657484292984009\n",
      "step 2917, loss: 2.5587096214294434\n",
      "step 2918, loss: 2.4108026027679443\n",
      "step 2919, loss: 2.3275585174560547\n",
      "step 2920, loss: 2.5771677494049072\n",
      "step 2921, loss: 2.4962692260742188\n",
      "step 2922, loss: 2.9043378829956055\n",
      "step 2923, loss: 2.403954267501831\n",
      "step 2924, loss: 2.6031203269958496\n",
      "step 2925, loss: 2.60449481010437\n",
      "step 2926, loss: 2.522362470626831\n",
      "step 2927, loss: 2.555189609527588\n",
      "step 2928, loss: 2.4275364875793457\n",
      "step 2929, loss: 2.6145689487457275\n",
      "step 2930, loss: 2.629646062850952\n",
      "step 2931, loss: 2.6180732250213623\n",
      "step 2932, loss: 2.5285379886627197\n",
      "step 2933, loss: 2.338766574859619\n",
      "step 2934, loss: 2.1399776935577393\n",
      "step 2935, loss: 2.2396059036254883\n",
      "step 2936, loss: 2.453556776046753\n",
      "step 2937, loss: 2.7288591861724854\n",
      "step 2938, loss: 2.680514335632324\n",
      "step 2939, loss: 2.5517477989196777\n",
      "step 2940, loss: 2.5629632472991943\n",
      "step 2941, loss: 2.5059926509857178\n",
      "step 2942, loss: 2.680278778076172\n",
      "step 2943, loss: 2.41257643699646\n",
      "step 2944, loss: 2.847524642944336\n",
      "step 2945, loss: 2.634225606918335\n",
      "step 2946, loss: 2.3928091526031494\n",
      "step 2947, loss: 2.589871644973755\n",
      "step 2948, loss: 2.4970903396606445\n",
      "step 2949, loss: 2.680035352706909\n",
      "step 2950, loss: 2.6849591732025146\n",
      "step 2951, loss: 2.6726791858673096\n",
      "step 2952, loss: 2.8896408081054688\n",
      "step 2953, loss: 2.212265968322754\n",
      "step 2954, loss: 2.2411530017852783\n",
      "step 2955, loss: 2.5565409660339355\n",
      "step 2956, loss: 2.7398157119750977\n",
      "step 2957, loss: 2.561734199523926\n",
      "step 2958, loss: 2.187082052230835\n",
      "step 2959, loss: 1.2221366167068481\n",
      "step 2960, loss: 1.1482950448989868\n",
      "step 2961, loss: 1.2556579113006592\n",
      "step 2962, loss: 1.1014999151229858\n",
      "step 2963, loss: 1.1171233654022217\n",
      "step 2964, loss: 1.2877897024154663\n",
      "step 2965, loss: 1.1502927541732788\n",
      "step 2966, loss: 1.3941128253936768\n",
      "step 2967, loss: 1.223543405532837\n",
      "step 2968, loss: 1.0416338443756104\n",
      "step 2969, loss: 1.0071568489074707\n",
      "step 2970, loss: 1.1680030822753906\n",
      "step 2971, loss: 1.2190614938735962\n",
      "step 2972, loss: 1.250200629234314\n",
      "step 2973, loss: 0.8345360159873962\n",
      "step 2974, loss: 0.9024014472961426\n",
      "step 2975, loss: 0.8818600177764893\n",
      "step 2976, loss: 0.6988650560379028\n",
      "step 2977, loss: 0.8906856775283813\n",
      "step 2978, loss: 0.9834977388381958\n",
      "step 2979, loss: 1.0313777923583984\n",
      "step 2980, loss: 1.1488792896270752\n",
      "step 2981, loss: 0.970678985118866\n",
      "step 2982, loss: 0.8740453124046326\n",
      "step 2983, loss: 0.9640032649040222\n",
      "step 2984, loss: 1.083880066871643\n",
      "step 2985, loss: 1.0115547180175781\n",
      "step 2986, loss: 1.1467136144638062\n",
      "step 2987, loss: 0.8457820415496826\n",
      "step 2988, loss: 2.0799713134765625\n",
      "step 2989, loss: 2.206326723098755\n",
      "step 2990, loss: 2.234257698059082\n",
      "step 2991, loss: 1.9487438201904297\n",
      "step 2992, loss: 2.0908894538879395\n",
      "step 2993, loss: 2.313380479812622\n",
      "step 2994, loss: 2.248896837234497\n",
      "step 2995, loss: 2.693066358566284\n",
      "step 2996, loss: 2.6660313606262207\n",
      "step 2997, loss: 2.709077835083008\n",
      "step 2998, loss: 2.8424463272094727\n",
      "step 2999, loss: 2.19496750831604\n",
      "step 3000, loss: 2.6588802337646484\n",
      "step 3001, loss: 2.342971086502075\n",
      "step 3002, loss: 2.605426073074341\n",
      "step 3003, loss: 2.534324884414673\n",
      "step 3004, loss: 2.4458749294281006\n",
      "step 3005, loss: 2.2985851764678955\n",
      "step 3006, loss: 2.4367220401763916\n",
      "step 3007, loss: 2.4138541221618652\n",
      "step 3008, loss: 2.2781877517700195\n",
      "step 3009, loss: 2.482804298400879\n",
      "step 3010, loss: 2.3476903438568115\n",
      "step 3011, loss: 2.4273812770843506\n",
      "step 3012, loss: 2.2024953365325928\n",
      "step 3013, loss: 2.4038143157958984\n",
      "step 3014, loss: 2.425271511077881\n",
      "step 3015, loss: 2.5049829483032227\n",
      "step 3016, loss: 2.411893606185913\n",
      "step 3017, loss: 2.195868968963623\n",
      "step 3018, loss: 2.6282882690429688\n",
      "step 3019, loss: 2.4168620109558105\n",
      "step 3020, loss: 2.4990451335906982\n",
      "step 3021, loss: 2.915132761001587\n",
      "step 3022, loss: 2.6919894218444824\n",
      "step 3023, loss: 2.5314059257507324\n",
      "step 3024, loss: 2.6058411598205566\n",
      "step 3025, loss: 2.9635891914367676\n",
      "step 3026, loss: 2.8433914184570312\n",
      "step 3027, loss: 2.6077804565429688\n",
      "step 3028, loss: 2.623060703277588\n",
      "step 3029, loss: 2.833522081375122\n",
      "step 3030, loss: 2.808055877685547\n",
      "step 3031, loss: 2.5425100326538086\n",
      "step 3032, loss: 2.582615852355957\n",
      "step 3033, loss: 2.6432855129241943\n",
      "step 3034, loss: 2.5305659770965576\n",
      "step 3035, loss: 2.7410213947296143\n",
      "step 3036, loss: 2.754786491394043\n",
      "step 3037, loss: 2.2948460578918457\n",
      "step 3038, loss: 2.525792360305786\n",
      "step 3039, loss: 2.5253589153289795\n",
      "step 3040, loss: 2.572446584701538\n",
      "step 3041, loss: 2.3495090007781982\n",
      "step 3042, loss: 2.719295024871826\n",
      "step 3043, loss: 2.451690435409546\n",
      "step 3044, loss: 2.3670437335968018\n",
      "step 3045, loss: 2.3356831073760986\n",
      "step 3046, loss: 2.406923770904541\n",
      "step 3047, loss: 2.4346060752868652\n",
      "step 3048, loss: 2.5224392414093018\n",
      "step 3049, loss: 2.794123888015747\n",
      "step 3050, loss: 2.4980199337005615\n",
      "step 3051, loss: 2.8641610145568848\n",
      "step 3052, loss: 2.267254590988159\n",
      "step 3053, loss: 2.5106306076049805\n",
      "step 3054, loss: 2.703557014465332\n",
      "step 3055, loss: 2.4453935623168945\n",
      "step 3056, loss: 2.5457417964935303\n",
      "step 3057, loss: 2.7334275245666504\n",
      "step 3058, loss: 2.805452585220337\n",
      "step 3059, loss: 2.734924077987671\n",
      "step 3060, loss: 2.680480718612671\n",
      "step 3061, loss: 2.3872573375701904\n",
      "step 3062, loss: 2.5746097564697266\n",
      "step 3063, loss: 2.4927053451538086\n",
      "step 3064, loss: 2.5010108947753906\n",
      "step 3065, loss: 2.4234492778778076\n",
      "step 3066, loss: 2.6050078868865967\n",
      "step 3067, loss: 2.6605277061462402\n",
      "step 3068, loss: 2.7100021839141846\n",
      "step 3069, loss: 2.8828248977661133\n",
      "step 3070, loss: 2.317807674407959\n",
      "step 3071, loss: 2.622887134552002\n",
      "step 3072, loss: 2.6004891395568848\n",
      "step 3073, loss: 2.695207357406616\n",
      "step 3074, loss: 2.825667381286621\n",
      "step 3075, loss: 2.921445846557617\n",
      "step 3076, loss: 2.4401087760925293\n",
      "step 3077, loss: 2.5190353393554688\n",
      "step 3078, loss: 2.5523037910461426\n",
      "step 3079, loss: 2.65142822265625\n",
      "step 3080, loss: 2.3848705291748047\n",
      "step 3081, loss: 2.5503735542297363\n",
      "step 3082, loss: 2.7174232006073\n",
      "step 3083, loss: 2.648129940032959\n",
      "step 3084, loss: 2.3860960006713867\n",
      "step 3085, loss: 2.5561182498931885\n",
      "step 3086, loss: 2.782532215118408\n",
      "step 3087, loss: 2.356837511062622\n",
      "step 3088, loss: 2.5654349327087402\n",
      "step 3089, loss: 2.725705146789551\n",
      "step 3090, loss: 2.837871551513672\n",
      "step 3091, loss: 2.326465368270874\n",
      "step 3092, loss: 2.602832317352295\n",
      "step 3093, loss: 2.635504961013794\n",
      "step 3094, loss: 2.6628522872924805\n",
      "step 3095, loss: 2.678014039993286\n",
      "step 3096, loss: 2.5429270267486572\n",
      "step 3097, loss: 2.8751397132873535\n",
      "step 3098, loss: 2.895026922225952\n",
      "step 3099, loss: 2.7726738452911377\n",
      "step 3100, loss: 2.367788791656494\n",
      "step 3101, loss: 2.669760227203369\n",
      "step 3102, loss: 2.810516595840454\n",
      "step 3103, loss: 2.8456826210021973\n",
      "step 3104, loss: 2.519798994064331\n",
      "step 3105, loss: 2.6642627716064453\n",
      "step 3106, loss: 2.8397719860076904\n",
      "step 3107, loss: 2.760153293609619\n",
      "step 3108, loss: 2.5697920322418213\n",
      "step 3109, loss: 3.0912108421325684\n",
      "step 3110, loss: 2.5951857566833496\n",
      "step 3111, loss: 2.5208938121795654\n",
      "step 3112, loss: 2.8239996433258057\n",
      "step 3113, loss: 2.722364902496338\n",
      "step 3114, loss: 2.7830777168273926\n",
      "step 3115, loss: 2.937591075897217\n",
      "step 3116, loss: 2.532716989517212\n",
      "step 3117, loss: 2.8113906383514404\n",
      "step 3118, loss: 2.8250796794891357\n",
      "step 3119, loss: 2.616273880004883\n",
      "step 3120, loss: 2.752776622772217\n",
      "step 3121, loss: 2.3237524032592773\n",
      "step 3122, loss: 2.9721126556396484\n",
      "step 3123, loss: 2.3230106830596924\n",
      "step 3124, loss: 2.93414568901062\n",
      "step 3125, loss: 2.72112774848938\n",
      "step 3126, loss: 2.5266523361206055\n",
      "step 3127, loss: 2.5789692401885986\n",
      "step 3128, loss: 2.880859851837158\n",
      "step 3129, loss: 2.8155550956726074\n",
      "step 3130, loss: 2.670029640197754\n",
      "step 3131, loss: 2.928746461868286\n",
      "step 3132, loss: 2.792510747909546\n",
      "step 3133, loss: 2.5952775478363037\n",
      "step 3134, loss: 2.944648027420044\n",
      "step 3135, loss: 2.803370237350464\n",
      "step 3136, loss: 2.8147661685943604\n",
      "step 3137, loss: 2.854221820831299\n",
      "step 3138, loss: 2.6703906059265137\n",
      "step 3139, loss: 2.849203109741211\n",
      "step 3140, loss: 2.884080410003662\n",
      "step 3141, loss: 2.5853309631347656\n",
      "step 3142, loss: 2.540935516357422\n",
      "step 3143, loss: 2.6322262287139893\n",
      "step 3144, loss: 2.759930372238159\n",
      "step 3145, loss: 2.74008846282959\n",
      "step 3146, loss: 2.7421393394470215\n",
      "step 3147, loss: 2.388564109802246\n",
      "step 3148, loss: 2.586437702178955\n",
      "step 3149, loss: 2.607543706893921\n",
      "step 3150, loss: 2.5181283950805664\n",
      "step 3151, loss: 2.3255863189697266\n",
      "step 3152, loss: 2.592611789703369\n",
      "step 3153, loss: 2.472367525100708\n",
      "step 3154, loss: 2.6353628635406494\n",
      "step 3155, loss: 2.427269220352173\n",
      "step 3156, loss: 2.5522875785827637\n",
      "step 3157, loss: 2.559131622314453\n",
      "step 3158, loss: 2.4329326152801514\n",
      "step 3159, loss: 2.31554913520813\n",
      "step 3160, loss: 2.4253439903259277\n",
      "step 3161, loss: 2.755495071411133\n",
      "step 3162, loss: 2.7853991985321045\n",
      "step 3163, loss: 2.6018967628479004\n",
      "step 3164, loss: 2.6602046489715576\n",
      "step 3165, loss: 2.5393080711364746\n",
      "step 3166, loss: 2.487919330596924\n",
      "step 3167, loss: 2.502821683883667\n",
      "step 3168, loss: 2.5274124145507812\n",
      "step 3169, loss: 2.559614419937134\n",
      "step 3170, loss: 2.4705255031585693\n",
      "step 3171, loss: 2.208231210708618\n",
      "step 3172, loss: 1.1769046783447266\n",
      "step 3173, loss: 1.5487942695617676\n",
      "step 3174, loss: 1.3686949014663696\n",
      "step 3175, loss: 1.2980133295059204\n",
      "step 3176, loss: 1.1972877979278564\n",
      "step 3177, loss: 1.1734226942062378\n",
      "step 3178, loss: 1.1752667427062988\n",
      "step 3179, loss: 1.1222182512283325\n",
      "step 3180, loss: 1.2297754287719727\n",
      "step 3181, loss: 1.1135531663894653\n",
      "step 3182, loss: 1.0373499393463135\n",
      "step 3183, loss: 0.8486022353172302\n",
      "step 3184, loss: 1.1839641332626343\n",
      "step 3185, loss: 0.9287852048873901\n",
      "step 3186, loss: 1.1467416286468506\n",
      "step 3187, loss: 1.390062689781189\n",
      "step 3188, loss: 1.2729711532592773\n",
      "step 3189, loss: 1.0097814798355103\n",
      "step 3190, loss: 1.1896699666976929\n",
      "step 3191, loss: 1.1595877408981323\n",
      "step 3192, loss: 0.7418512105941772\n",
      "step 3193, loss: 1.0168176889419556\n",
      "step 3194, loss: 1.3955795764923096\n",
      "step 3195, loss: 1.2505730390548706\n",
      "step 3196, loss: 1.297970175743103\n",
      "step 3197, loss: 0.9562214612960815\n",
      "step 3198, loss: 1.1169394254684448\n",
      "step 3199, loss: 1.0641785860061646\n",
      "step 3200, loss: 1.0387661457061768\n",
      "step 3201, loss: 0.9919512867927551\n",
      "step 3202, loss: 1.12546706199646\n",
      "step 3203, loss: 2.5217204093933105\n",
      "step 3204, loss: 2.462080717086792\n",
      "step 3205, loss: 2.513059377670288\n",
      "step 3206, loss: 2.2905101776123047\n",
      "step 3207, loss: 2.2191948890686035\n",
      "step 3208, loss: 2.377424955368042\n",
      "step 3209, loss: 2.417701005935669\n",
      "step 3210, loss: 2.4609830379486084\n",
      "step 3211, loss: 2.3492422103881836\n",
      "step 3212, loss: 2.2143404483795166\n",
      "step 3213, loss: 2.4498910903930664\n",
      "step 3214, loss: 2.3542680740356445\n",
      "step 3215, loss: 2.1681079864501953\n",
      "step 3216, loss: 2.307704448699951\n",
      "step 3217, loss: 2.3001549243927\n",
      "step 3218, loss: 2.3418853282928467\n",
      "step 3219, loss: 2.4822704792022705\n",
      "step 3220, loss: 2.1991806030273438\n",
      "step 3221, loss: 2.6678357124328613\n",
      "step 3222, loss: 2.5400261878967285\n",
      "step 3223, loss: 2.3722476959228516\n",
      "step 3224, loss: 2.3716819286346436\n",
      "step 3225, loss: 2.4112565517425537\n",
      "step 3226, loss: 2.381328582763672\n",
      "step 3227, loss: 2.2667946815490723\n",
      "step 3228, loss: 2.4999594688415527\n",
      "step 3229, loss: 2.071341037750244\n",
      "step 3230, loss: 2.009308099746704\n",
      "step 3231, loss: 2.1109044551849365\n",
      "step 3232, loss: 2.2965822219848633\n",
      "step 3233, loss: 2.631840705871582\n",
      "step 3234, loss: 2.396993398666382\n",
      "step 3235, loss: 2.526261806488037\n",
      "step 3236, loss: 2.467430591583252\n",
      "step 3237, loss: 2.611121416091919\n",
      "step 3238, loss: 2.2413387298583984\n",
      "step 3239, loss: 1.8941893577575684\n",
      "step 3240, loss: 1.6610338687896729\n",
      "step 3241, loss: 1.6849192380905151\n",
      "step 3242, loss: 1.561586856842041\n",
      "step 3243, loss: 1.4455442428588867\n",
      "step 3244, loss: 1.312595248222351\n",
      "step 3245, loss: 1.4326783418655396\n",
      "step 3246, loss: 1.319270372390747\n",
      "step 3247, loss: 1.3400542736053467\n",
      "step 3248, loss: 1.187155842781067\n",
      "step 3249, loss: 0.9996406435966492\n",
      "step 3250, loss: 1.355108380317688\n",
      "step 3251, loss: 1.0199564695358276\n",
      "step 3252, loss: 1.0210833549499512\n",
      "step 3253, loss: 1.1569677591323853\n",
      "step 3254, loss: 0.9131436944007874\n",
      "step 3255, loss: 0.8379656672477722\n",
      "step 3256, loss: 2.112537145614624\n",
      "step 3257, loss: 2.1735947132110596\n",
      "step 3258, loss: 2.417701005935669\n",
      "step 3259, loss: 2.583207368850708\n",
      "step 3260, loss: 2.328824996948242\n",
      "step 3261, loss: 2.425001621246338\n",
      "step 3262, loss: 2.317011594772339\n",
      "step 3263, loss: 2.397343397140503\n",
      "step 3264, loss: 2.225304126739502\n",
      "step 3265, loss: 2.2903082370758057\n",
      "step 3266, loss: 2.4305434226989746\n",
      "step 3267, loss: 2.5034584999084473\n",
      "step 3268, loss: 2.419818162918091\n",
      "step 3269, loss: 2.370689868927002\n",
      "step 3270, loss: 2.6980671882629395\n",
      "step 3271, loss: 2.571782350540161\n",
      "step 3272, loss: 2.3083584308624268\n",
      "step 3273, loss: 2.4861316680908203\n",
      "step 3274, loss: 2.611961603164673\n",
      "step 3275, loss: 2.2077062129974365\n",
      "step 3276, loss: 2.2098684310913086\n",
      "step 3277, loss: 2.369194746017456\n",
      "step 3278, loss: 2.2485511302948\n",
      "step 3279, loss: 2.327723741531372\n",
      "step 3280, loss: 2.4560916423797607\n",
      "step 3281, loss: 2.3728060722351074\n",
      "step 3282, loss: 2.2799758911132812\n",
      "step 3283, loss: 2.4705734252929688\n",
      "step 3284, loss: 2.418675661087036\n",
      "step 3285, loss: 2.1860547065734863\n",
      "step 3286, loss: 2.259021043777466\n",
      "step 3287, loss: 2.402513027191162\n",
      "step 3288, loss: 2.281632661819458\n",
      "step 3289, loss: 2.2414820194244385\n",
      "step 3290, loss: 2.282902956008911\n",
      "step 3291, loss: 2.3514175415039062\n",
      "step 3292, loss: 2.2375898361206055\n",
      "step 3293, loss: 2.427403450012207\n",
      "step 3294, loss: 2.4213342666625977\n",
      "step 3295, loss: 1.907569169998169\n",
      "step 3296, loss: 2.2809317111968994\n",
      "step 3297, loss: 2.14835262298584\n",
      "step 3298, loss: 2.5350759029388428\n",
      "step 3299, loss: 2.329158306121826\n",
      "step 3300, loss: 2.0771713256835938\n",
      "step 3301, loss: 2.2964043617248535\n",
      "step 3302, loss: 2.239919662475586\n",
      "step 3303, loss: 2.3446154594421387\n",
      "step 3304, loss: 2.3561837673187256\n",
      "step 3305, loss: 2.2671561241149902\n",
      "step 3306, loss: 1.942198395729065\n",
      "step 3307, loss: 2.374140739440918\n",
      "step 3308, loss: 2.3390583992004395\n",
      "step 3309, loss: 2.5722579956054688\n",
      "step 3310, loss: 2.444704294204712\n",
      "step 3311, loss: 2.567596912384033\n",
      "step 3312, loss: 2.0876004695892334\n",
      "step 3313, loss: 2.5091326236724854\n",
      "step 3314, loss: 2.331824779510498\n",
      "step 3315, loss: 2.3887252807617188\n",
      "step 3316, loss: 2.3190128803253174\n",
      "step 3317, loss: 2.123208522796631\n",
      "step 3318, loss: 2.3426122665405273\n",
      "step 3319, loss: 2.39778208732605\n",
      "step 3320, loss: 2.31208872795105\n",
      "step 3321, loss: 2.2947680950164795\n",
      "step 3322, loss: 2.4558115005493164\n",
      "step 3323, loss: 2.3785481452941895\n",
      "step 3324, loss: 2.333240032196045\n",
      "step 3325, loss: 2.6208291053771973\n",
      "step 3326, loss: 2.5836892127990723\n",
      "step 3327, loss: 2.2420597076416016\n",
      "step 3328, loss: 2.4032042026519775\n",
      "step 3329, loss: 2.3332579135894775\n",
      "step 3330, loss: 2.4704623222351074\n",
      "step 3331, loss: 2.29095721244812\n",
      "step 3332, loss: 2.3957793712615967\n",
      "step 3333, loss: 2.3752949237823486\n",
      "step 3334, loss: 2.531947612762451\n",
      "step 3335, loss: 2.412452459335327\n",
      "step 3336, loss: 2.469987154006958\n",
      "step 3337, loss: 2.3500852584838867\n",
      "step 3338, loss: 2.5526647567749023\n",
      "step 3339, loss: 2.4537394046783447\n",
      "step 3340, loss: 2.6358838081359863\n",
      "step 3341, loss: 2.2538328170776367\n",
      "step 3342, loss: 2.287252187728882\n",
      "step 3343, loss: 2.6121299266815186\n",
      "step 3344, loss: 2.038053035736084\n",
      "step 3345, loss: 1.9614391326904297\n",
      "step 3346, loss: 1.3564752340316772\n",
      "step 3347, loss: 1.4069949388504028\n",
      "step 3348, loss: 1.7215591669082642\n",
      "step 3349, loss: 1.4922257661819458\n",
      "step 3350, loss: 1.3531160354614258\n",
      "step 3351, loss: 1.3218613862991333\n",
      "step 3352, loss: 1.460497498512268\n",
      "step 3353, loss: 1.0883339643478394\n",
      "step 3354, loss: 1.1118993759155273\n",
      "step 3355, loss: 1.3185622692108154\n",
      "step 3356, loss: 1.1562246084213257\n",
      "step 3357, loss: 1.2169978618621826\n",
      "step 3358, loss: 1.222990870475769\n",
      "step 3359, loss: 1.3894500732421875\n",
      "step 3360, loss: 1.8238825798034668\n",
      "step 3361, loss: 1.9379757642745972\n",
      "step 3362, loss: 2.0565953254699707\n",
      "step 3363, loss: 2.290785551071167\n",
      "step 3364, loss: 1.9180505275726318\n",
      "step 3365, loss: 2.0164589881896973\n",
      "step 3366, loss: 1.8648619651794434\n",
      "step 3367, loss: 1.8397128582000732\n",
      "step 3368, loss: 2.0435373783111572\n",
      "step 3369, loss: 2.2719321250915527\n",
      "step 3370, loss: 1.9672718048095703\n",
      "step 3371, loss: 2.1380348205566406\n",
      "step 3372, loss: 2.1959941387176514\n",
      "step 3373, loss: 2.0771431922912598\n",
      "step 3374, loss: 2.162034749984741\n",
      "step 3375, loss: 2.1282575130462646\n",
      "step 3376, loss: 1.908257007598877\n",
      "step 3377, loss: 2.0555570125579834\n",
      "step 3378, loss: 2.054797410964966\n",
      "step 3379, loss: 1.9583269357681274\n",
      "step 3380, loss: 2.0695040225982666\n",
      "step 3381, loss: 2.21168851852417\n",
      "step 3382, loss: 2.0612308979034424\n",
      "step 3383, loss: 2.1516623497009277\n",
      "step 3384, loss: 2.1342790126800537\n",
      "step 3385, loss: 2.24241304397583\n",
      "step 3386, loss: 1.9502506256103516\n",
      "step 3387, loss: 2.0335001945495605\n",
      "step 3388, loss: 2.194821357727051\n",
      "step 3389, loss: 2.2144813537597656\n",
      "step 3390, loss: 1.838529109954834\n",
      "step 3391, loss: 1.9524333477020264\n",
      "step 3392, loss: 2.2228152751922607\n",
      "step 3393, loss: 2.043808698654175\n",
      "step 3394, loss: 2.1975064277648926\n",
      "step 3395, loss: 1.9590015411376953\n",
      "step 3396, loss: 2.170090675354004\n",
      "step 3397, loss: 1.8675053119659424\n",
      "step 3398, loss: 2.0361530780792236\n",
      "step 3399, loss: 2.0950140953063965\n",
      "step 3400, loss: 1.8560417890548706\n",
      "step 3401, loss: 1.9245030879974365\n",
      "step 3402, loss: 1.9211763143539429\n",
      "step 3403, loss: 2.1114113330841064\n",
      "step 3404, loss: 1.9441075325012207\n",
      "step 3405, loss: 2.295900821685791\n",
      "step 3406, loss: 1.9817426204681396\n",
      "step 3407, loss: 1.918370008468628\n",
      "step 3408, loss: 1.8711004257202148\n",
      "step 3409, loss: 1.7550619840621948\n",
      "step 3410, loss: 2.0545663833618164\n",
      "step 3411, loss: 2.046130657196045\n",
      "step 3412, loss: 2.171098232269287\n",
      "step 3413, loss: 2.016101121902466\n",
      "step 3414, loss: 1.928640604019165\n",
      "step 3415, loss: 1.6998467445373535\n",
      "step 3416, loss: 1.8582139015197754\n",
      "step 3417, loss: 2.2214887142181396\n",
      "step 3418, loss: 1.6885429620742798\n",
      "step 3419, loss: 1.1535451412200928\n",
      "step 3420, loss: 1.0916073322296143\n",
      "step 3421, loss: 1.1001098155975342\n",
      "step 3422, loss: 0.9804209470748901\n",
      "step 3423, loss: 0.9389054179191589\n",
      "step 3424, loss: 0.9251400828361511\n",
      "step 3425, loss: 0.9925270080566406\n",
      "step 3426, loss: 0.9029027223587036\n",
      "step 3427, loss: 1.0169402360916138\n",
      "step 3428, loss: 0.9573465585708618\n",
      "step 3429, loss: 0.8329145312309265\n",
      "step 3430, loss: 0.878122866153717\n",
      "step 3431, loss: 1.1534653902053833\n",
      "step 3432, loss: 1.6478652954101562\n",
      "step 3433, loss: 1.8948277235031128\n",
      "step 3434, loss: 1.8783130645751953\n",
      "step 3435, loss: 1.845842957496643\n",
      "step 3436, loss: 1.8822256326675415\n",
      "step 3437, loss: 1.8203870058059692\n",
      "step 3438, loss: 1.7366840839385986\n",
      "step 3439, loss: 1.998355507850647\n",
      "step 3440, loss: 1.797694444656372\n",
      "step 3441, loss: 1.933826208114624\n",
      "step 3442, loss: 2.053041696548462\n",
      "step 3443, loss: 1.991600513458252\n",
      "step 3444, loss: 1.73109769821167\n",
      "step 3445, loss: 2.1814959049224854\n",
      "step 3446, loss: 2.071546792984009\n",
      "step 3447, loss: 2.0930891036987305\n",
      "step 3448, loss: 2.2896008491516113\n",
      "step 3449, loss: 2.1416609287261963\n",
      "step 3450, loss: 2.1417465209960938\n",
      "step 3451, loss: 2.232558250427246\n",
      "step 3452, loss: 2.0061352252960205\n",
      "step 3453, loss: 2.0901660919189453\n",
      "step 3454, loss: 2.2505884170532227\n",
      "step 3455, loss: 1.859714388847351\n",
      "step 3456, loss: 1.9792307615280151\n",
      "step 3457, loss: 2.165450096130371\n",
      "step 3458, loss: 1.981178641319275\n",
      "step 3459, loss: 2.1106832027435303\n",
      "step 3460, loss: 2.2298965454101562\n",
      "step 3461, loss: 2.218337297439575\n",
      "step 3462, loss: 1.8047168254852295\n",
      "step 3463, loss: 2.1321513652801514\n",
      "step 3464, loss: 1.9680562019348145\n",
      "step 3465, loss: 1.9617542028427124\n",
      "step 3466, loss: 2.0366151332855225\n",
      "step 3467, loss: 1.9666862487792969\n",
      "step 3468, loss: 2.0120396614074707\n",
      "step 3469, loss: 2.0985302925109863\n",
      "step 3470, loss: 2.0068752765655518\n",
      "step 3471, loss: 2.0338692665100098\n",
      "step 3472, loss: 2.2125658988952637\n",
      "step 3473, loss: 2.036667585372925\n",
      "step 3474, loss: 1.8193871974945068\n",
      "step 3475, loss: 2.213900566101074\n",
      "step 3476, loss: 2.016453742980957\n",
      "step 3477, loss: 2.003114938735962\n",
      "step 3478, loss: 2.3124310970306396\n",
      "step 3479, loss: 1.9728024005889893\n",
      "step 3480, loss: 2.0615341663360596\n",
      "step 3481, loss: 2.0504684448242188\n",
      "step 3482, loss: 2.041023015975952\n",
      "step 3483, loss: 1.9814913272857666\n",
      "step 3484, loss: 2.006866216659546\n",
      "step 3485, loss: 2.0726912021636963\n",
      "step 3486, loss: 2.1189401149749756\n",
      "step 3487, loss: 1.8547364473342896\n",
      "step 3488, loss: 1.9649485349655151\n",
      "step 3489, loss: 2.145297050476074\n",
      "step 3490, loss: 2.095804452896118\n",
      "step 3491, loss: 1.9655730724334717\n",
      "step 3492, loss: 1.8335344791412354\n",
      "step 3493, loss: 1.9218089580535889\n",
      "step 3494, loss: 2.3002541065216064\n",
      "step 3495, loss: 2.067770481109619\n",
      "step 3496, loss: 1.9874480962753296\n",
      "step 3497, loss: 1.9513615369796753\n",
      "step 3498, loss: 1.941871166229248\n",
      "step 3499, loss: 2.1661415100097656\n",
      "step 3500, loss: 2.0370519161224365\n",
      "step 3501, loss: 2.0587539672851562\n",
      "step 3502, loss: 1.951328158378601\n",
      "step 3503, loss: 2.2013497352600098\n",
      "step 3504, loss: 2.1204123497009277\n",
      "step 3505, loss: 2.1914889812469482\n",
      "step 3506, loss: 2.0497865676879883\n",
      "step 3507, loss: 2.1688568592071533\n",
      "step 3508, loss: 1.6365079879760742\n",
      "step 3509, loss: 2.0167386531829834\n",
      "step 3510, loss: 2.1088852882385254\n",
      "step 3511, loss: 2.108238458633423\n",
      "step 3512, loss: 1.9923092126846313\n",
      "step 3513, loss: 2.113736152648926\n",
      "step 3514, loss: 2.3193931579589844\n",
      "step 3515, loss: 2.0312790870666504\n",
      "step 3516, loss: 2.096290111541748\n",
      "step 3517, loss: 1.9545568227767944\n",
      "step 3518, loss: 1.8664500713348389\n",
      "step 3519, loss: 1.8453978300094604\n",
      "step 3520, loss: 2.082746744155884\n",
      "step 3521, loss: 2.2258689403533936\n",
      "step 3522, loss: 2.3908653259277344\n",
      "step 3523, loss: 2.1500346660614014\n",
      "step 3524, loss: 2.3097667694091797\n",
      "step 3525, loss: 2.1576015949249268\n",
      "step 3526, loss: 2.0612659454345703\n",
      "step 3527, loss: 2.1120309829711914\n",
      "step 3528, loss: 1.8154900074005127\n",
      "step 3529, loss: 2.20141339302063\n",
      "step 3530, loss: 1.9990899562835693\n",
      "step 3531, loss: 2.1233434677124023\n",
      "step 3532, loss: 2.083207130432129\n",
      "step 3533, loss: 2.0679996013641357\n",
      "step 3534, loss: 1.909336805343628\n",
      "step 3535, loss: 1.8675529956817627\n",
      "step 3536, loss: 1.9934614896774292\n",
      "step 3537, loss: 2.095472812652588\n",
      "step 3538, loss: 1.9669468402862549\n",
      "step 3539, loss: 2.189406394958496\n",
      "step 3540, loss: 1.8812358379364014\n",
      "step 3541, loss: 1.7868824005126953\n",
      "step 3542, loss: 2.1260998249053955\n",
      "step 3543, loss: 1.7775802612304688\n",
      "step 3544, loss: 1.2122293710708618\n",
      "step 3545, loss: 1.093534231185913\n",
      "step 3546, loss: 1.0704227685928345\n",
      "step 3547, loss: 0.8903449773788452\n",
      "step 3548, loss: 0.7333477139472961\n",
      "step 3549, loss: 0.9620596170425415\n",
      "step 3550, loss: 0.7887097001075745\n",
      "step 3551, loss: 0.7747141122817993\n",
      "step 3552, loss: 1.011916160583496\n",
      "step 3553, loss: 0.8473008871078491\n",
      "step 3554, loss: 0.8106438517570496\n",
      "step 3555, loss: 0.9583110809326172\n",
      "step 3556, loss: 1.0010221004486084\n",
      "step 3557, loss: 0.7418792843818665\n",
      "step 3558, loss: 0.6918915510177612\n",
      "step 3559, loss: 0.9848381280899048\n",
      "step 3560, loss: 0.8389217257499695\n",
      "step 3561, loss: 0.8575287461280823\n",
      "step 3562, loss: 0.9178032279014587\n",
      "step 3563, loss: 0.7246895432472229\n",
      "step 3564, loss: 1.0697354078292847\n",
      "step 3565, loss: 1.0599892139434814\n",
      "step 3566, loss: 0.9437054991722107\n",
      "step 3567, loss: 1.0478144884109497\n",
      "step 3568, loss: 0.8093217015266418\n",
      "step 3569, loss: 0.875866174697876\n",
      "step 3570, loss: 1.0603783130645752\n",
      "step 3571, loss: 0.9451276659965515\n",
      "step 3572, loss: 1.715211033821106\n",
      "step 3573, loss: 1.8656952381134033\n",
      "step 3574, loss: 1.923545241355896\n",
      "step 3575, loss: 2.0177347660064697\n",
      "step 3576, loss: 1.9672698974609375\n",
      "step 3577, loss: 1.8809645175933838\n",
      "step 3578, loss: 1.82718026638031\n",
      "step 3579, loss: 1.966914176940918\n",
      "step 3580, loss: 1.9294936656951904\n",
      "step 3581, loss: 2.037081480026245\n",
      "step 3582, loss: 1.9864073991775513\n",
      "step 3583, loss: 2.095557689666748\n",
      "step 3584, loss: 2.2442445755004883\n",
      "step 3585, loss: 2.139176845550537\n",
      "step 3586, loss: 2.284156084060669\n",
      "step 3587, loss: 2.3059098720550537\n",
      "step 3588, loss: 2.157165050506592\n",
      "step 3589, loss: 2.135748863220215\n",
      "step 3590, loss: 2.066066026687622\n",
      "step 3591, loss: 2.0956263542175293\n",
      "step 3592, loss: 2.120944023132324\n",
      "step 3593, loss: 1.93228018283844\n",
      "step 3594, loss: 1.880048155784607\n",
      "step 3595, loss: 1.9626820087432861\n",
      "step 3596, loss: 2.1129860877990723\n",
      "step 3597, loss: 1.8279048204421997\n",
      "step 3598, loss: 1.8961619138717651\n",
      "step 3599, loss: 1.9427281618118286\n",
      "step 3600, loss: 1.884310007095337\n",
      "step 3601, loss: 1.7794736623764038\n",
      "step 3602, loss: 1.9227302074432373\n",
      "step 3603, loss: 1.9434738159179688\n",
      "step 3604, loss: 2.1443967819213867\n",
      "step 3605, loss: 1.7386548519134521\n",
      "step 3606, loss: 1.7720446586608887\n",
      "step 3607, loss: 1.9449448585510254\n",
      "step 3608, loss: 2.1496458053588867\n",
      "step 3609, loss: 1.9622327089309692\n",
      "step 3610, loss: 2.193974494934082\n",
      "step 3611, loss: 1.8737037181854248\n",
      "step 3612, loss: 1.86582612991333\n",
      "step 3613, loss: 2.1022205352783203\n",
      "step 3614, loss: 1.8862709999084473\n",
      "step 3615, loss: 1.85044527053833\n",
      "step 3616, loss: 1.777531623840332\n",
      "step 3617, loss: 1.9246495962142944\n",
      "step 3618, loss: 1.9026954174041748\n",
      "step 3619, loss: 2.0401902198791504\n",
      "step 3620, loss: 1.7655587196350098\n",
      "step 3621, loss: 1.8126332759857178\n",
      "step 3622, loss: 1.8586821556091309\n",
      "step 3623, loss: 1.8078337907791138\n",
      "step 3624, loss: 1.7223082780838013\n",
      "step 3625, loss: 2.128021717071533\n",
      "step 3626, loss: 2.1114108562469482\n",
      "step 3627, loss: 1.927171230316162\n",
      "step 3628, loss: 1.8359694480895996\n",
      "step 3629, loss: 1.996353030204773\n",
      "step 3630, loss: 1.9425268173217773\n",
      "step 3631, loss: 2.0108072757720947\n",
      "step 3632, loss: 1.9896953105926514\n",
      "step 3633, loss: 2.0887508392333984\n",
      "step 3634, loss: 1.8784946203231812\n",
      "step 3635, loss: 1.973872184753418\n",
      "step 3636, loss: 1.8010931015014648\n",
      "step 3637, loss: 1.742339015007019\n",
      "step 3638, loss: 1.8953293561935425\n",
      "step 3639, loss: 2.04516339302063\n",
      "step 3640, loss: 2.0193169116973877\n",
      "step 3641, loss: 2.008667469024658\n",
      "step 3642, loss: 1.9490848779678345\n",
      "step 3643, loss: 2.0261781215667725\n",
      "step 3644, loss: 2.053068161010742\n",
      "step 3645, loss: 2.0372424125671387\n",
      "step 3646, loss: 2.14483642578125\n",
      "step 3647, loss: 1.8819797039031982\n",
      "step 3648, loss: 2.0322601795196533\n",
      "step 3649, loss: 1.9785856008529663\n",
      "step 3650, loss: 1.851021647453308\n",
      "step 3651, loss: 1.962687373161316\n",
      "step 3652, loss: 1.8638354539871216\n",
      "step 3653, loss: 1.9084162712097168\n",
      "step 3654, loss: 2.0724098682403564\n",
      "step 3655, loss: 2.0003368854522705\n",
      "step 3656, loss: 1.9324818849563599\n",
      "step 3657, loss: 2.007542133331299\n",
      "step 3658, loss: 2.014519691467285\n",
      "step 3659, loss: 1.9680190086364746\n",
      "step 3660, loss: 2.1780502796173096\n",
      "step 3661, loss: 2.0939276218414307\n",
      "step 3662, loss: 2.038766622543335\n",
      "step 3663, loss: 2.0844101905822754\n",
      "step 3664, loss: 1.8731580972671509\n",
      "step 3665, loss: 1.8524609804153442\n",
      "step 3666, loss: 1.7640684843063354\n",
      "step 3667, loss: 1.843440055847168\n",
      "step 3668, loss: 1.36512291431427\n",
      "step 3669, loss: 1.2709285020828247\n",
      "step 3670, loss: 1.2467817068099976\n",
      "step 3671, loss: 1.221200942993164\n",
      "step 3672, loss: 1.338728666305542\n",
      "step 3673, loss: 1.3865188360214233\n",
      "step 3674, loss: 1.2202695608139038\n",
      "step 3675, loss: 1.171614646911621\n",
      "step 3676, loss: 1.2878198623657227\n",
      "step 3677, loss: 0.9522550106048584\n",
      "step 3678, loss: 1.050350308418274\n",
      "step 3679, loss: 1.0621843338012695\n",
      "step 3680, loss: 1.0643205642700195\n",
      "step 3681, loss: 0.894010603427887\n",
      "step 3682, loss: 0.9199099540710449\n",
      "step 3683, loss: 0.7903475761413574\n",
      "step 3684, loss: 1.047446370124817\n",
      "step 3685, loss: 1.3424503803253174\n",
      "step 3686, loss: 1.0953335762023926\n",
      "step 3687, loss: 0.9789396524429321\n",
      "step 3688, loss: 0.8587939739227295\n",
      "step 3689, loss: 0.7943387627601624\n",
      "step 3690, loss: 1.705219030380249\n",
      "step 3691, loss: 1.7557525634765625\n",
      "step 3692, loss: 1.6345983743667603\n",
      "step 3693, loss: 1.6996324062347412\n",
      "step 3694, loss: 2.011702060699463\n",
      "step 3695, loss: 1.8021724224090576\n",
      "step 3696, loss: 1.6309258937835693\n",
      "step 3697, loss: 1.7072536945343018\n",
      "step 3698, loss: 1.8277674913406372\n",
      "step 3699, loss: 1.812575101852417\n",
      "step 3700, loss: 1.7383896112442017\n",
      "step 3701, loss: 1.7030882835388184\n",
      "step 3702, loss: 1.754555106163025\n",
      "step 3703, loss: 2.342031955718994\n",
      "step 3704, loss: 2.1520771980285645\n",
      "step 3705, loss: 2.1892683506011963\n",
      "step 3706, loss: 2.107388496398926\n",
      "step 3707, loss: 2.274946928024292\n",
      "step 3708, loss: 2.2317452430725098\n",
      "step 3709, loss: 2.261458396911621\n",
      "step 3710, loss: 2.2919600009918213\n",
      "step 3711, loss: 2.236844301223755\n",
      "step 3712, loss: 2.128103733062744\n",
      "step 3713, loss: 2.155172109603882\n",
      "step 3714, loss: 2.004803419113159\n",
      "step 3715, loss: 2.045018434524536\n",
      "step 3716, loss: 1.8409157991409302\n",
      "step 3717, loss: 1.9447753429412842\n",
      "step 3718, loss: 2.211099624633789\n",
      "step 3719, loss: 2.172656774520874\n",
      "step 3720, loss: 1.941245198249817\n",
      "step 3721, loss: 2.1426546573638916\n",
      "step 3722, loss: 2.059110641479492\n",
      "step 3723, loss: 2.396340847015381\n",
      "step 3724, loss: 2.028254747390747\n",
      "step 3725, loss: 2.355402946472168\n",
      "step 3726, loss: 2.2978832721710205\n",
      "step 3727, loss: 2.2222847938537598\n",
      "step 3728, loss: 2.3458404541015625\n",
      "step 3729, loss: 2.356231212615967\n",
      "step 3730, loss: 2.1428842544555664\n",
      "step 3731, loss: 2.1131021976470947\n",
      "step 3732, loss: 2.064136028289795\n",
      "step 3733, loss: 2.333526849746704\n",
      "step 3734, loss: 2.199484348297119\n",
      "step 3735, loss: 2.2712926864624023\n",
      "step 3736, loss: 2.1509878635406494\n",
      "step 3737, loss: 2.225503921508789\n",
      "step 3738, loss: 2.094350814819336\n",
      "step 3739, loss: 1.7378592491149902\n",
      "step 3740, loss: 1.671640157699585\n",
      "step 3741, loss: 2.0198144912719727\n",
      "step 3742, loss: 1.9621671438217163\n",
      "step 3743, loss: 2.228722095489502\n",
      "step 3744, loss: 1.8130863904953003\n",
      "step 3745, loss: 1.7340775728225708\n",
      "step 3746, loss: 2.1115059852600098\n",
      "step 3747, loss: 1.9227347373962402\n",
      "step 3748, loss: 2.1765129566192627\n",
      "step 3749, loss: 2.197176694869995\n",
      "step 3750, loss: 1.9407299757003784\n",
      "step 3751, loss: 2.2666773796081543\n",
      "step 3752, loss: 2.250365734100342\n",
      "step 3753, loss: 2.341144323348999\n",
      "step 3754, loss: 2.2372682094573975\n",
      "step 3755, loss: 2.186643362045288\n",
      "step 3756, loss: 2.3566465377807617\n",
      "step 3757, loss: 2.389801025390625\n",
      "step 3758, loss: 2.3394083976745605\n",
      "step 3759, loss: 2.518425226211548\n",
      "step 3760, loss: 2.393291473388672\n",
      "step 3761, loss: 2.339691162109375\n",
      "step 3762, loss: 2.4043545722961426\n",
      "step 3763, loss: 2.2715516090393066\n",
      "step 3764, loss: 2.140075206756592\n",
      "step 3765, loss: 2.034947395324707\n",
      "step 3766, loss: 2.168261766433716\n",
      "step 3767, loss: 2.4593636989593506\n",
      "step 3768, loss: 1.925252914428711\n",
      "step 3769, loss: 2.083928108215332\n",
      "step 3770, loss: 2.1405553817749023\n",
      "step 3771, loss: 1.9176392555236816\n",
      "step 3772, loss: 2.4149506092071533\n",
      "step 3773, loss: 2.2287237644195557\n",
      "step 3774, loss: 2.32025408744812\n",
      "step 3775, loss: 2.3963756561279297\n",
      "step 3776, loss: 2.2853965759277344\n",
      "step 3777, loss: 2.2978408336639404\n",
      "step 3778, loss: 2.092144727706909\n",
      "step 3779, loss: 2.0567891597747803\n",
      "step 3780, loss: 1.3973623514175415\n",
      "step 3781, loss: 1.2244895696640015\n",
      "step 3782, loss: 1.3081285953521729\n",
      "step 3783, loss: 1.0861666202545166\n",
      "step 3784, loss: 1.1878256797790527\n",
      "step 3785, loss: 0.9687583446502686\n",
      "step 3786, loss: 0.9956031441688538\n",
      "step 3787, loss: 0.9722313284873962\n",
      "step 3788, loss: 0.901918888092041\n",
      "step 3789, loss: 0.9515679478645325\n",
      "step 3790, loss: 1.0602940320968628\n",
      "step 3791, loss: 0.8940486311912537\n",
      "step 3792, loss: 0.7832024693489075\n",
      "step 3793, loss: 0.7590792775154114\n",
      "step 3794, loss: 0.737671434879303\n",
      "step 3795, loss: 0.9204466342926025\n",
      "step 3796, loss: 0.9033834338188171\n",
      "step 3797, loss: 0.9475702047348022\n",
      "step 3798, loss: 1.6637903451919556\n",
      "step 3799, loss: 2.1157162189483643\n",
      "step 3800, loss: 1.944579839706421\n",
      "step 3801, loss: 2.0650293827056885\n",
      "step 3802, loss: 1.8821996450424194\n",
      "step 3803, loss: 2.0333149433135986\n",
      "step 3804, loss: 1.8389629125595093\n",
      "step 3805, loss: 1.8719866275787354\n",
      "step 3806, loss: 1.8539066314697266\n",
      "step 3807, loss: 2.1023881435394287\n",
      "step 3808, loss: 2.0467302799224854\n",
      "step 3809, loss: 2.283290147781372\n",
      "step 3810, loss: 2.189774990081787\n",
      "step 3811, loss: 2.0798733234405518\n",
      "step 3812, loss: 2.168321132659912\n",
      "step 3813, loss: 2.01810622215271\n",
      "step 3814, loss: 2.0712215900421143\n",
      "step 3815, loss: 1.9813681840896606\n",
      "step 3816, loss: 1.9418866634368896\n",
      "step 3817, loss: 2.2026753425598145\n",
      "step 3818, loss: 2.06375789642334\n",
      "step 3819, loss: 1.9801360368728638\n",
      "step 3820, loss: 2.1921310424804688\n",
      "step 3821, loss: 1.9971952438354492\n",
      "step 3822, loss: 2.0686209201812744\n",
      "step 3823, loss: 2.2254765033721924\n",
      "step 3824, loss: 2.1568868160247803\n",
      "step 3825, loss: 2.1006109714508057\n",
      "step 3826, loss: 2.108844041824341\n",
      "step 3827, loss: 2.0690484046936035\n",
      "step 3828, loss: 2.255765676498413\n",
      "step 3829, loss: 2.3565673828125\n",
      "step 3830, loss: 2.033020257949829\n",
      "step 3831, loss: 2.113658905029297\n",
      "step 3832, loss: 2.2327194213867188\n",
      "step 3833, loss: 1.8489569425582886\n",
      "step 3834, loss: 2.126743793487549\n",
      "step 3835, loss: 2.0716748237609863\n",
      "step 3836, loss: 2.4473655223846436\n",
      "step 3837, loss: 2.4755053520202637\n",
      "step 3838, loss: 1.9951053857803345\n",
      "step 3839, loss: 2.0127222537994385\n",
      "step 3840, loss: 2.084728479385376\n",
      "step 3841, loss: 1.7286099195480347\n",
      "step 3842, loss: 1.684548020362854\n",
      "step 3843, loss: 1.0154531002044678\n",
      "step 3844, loss: 1.0234217643737793\n",
      "step 3845, loss: 0.9179388284683228\n",
      "step 3846, loss: 0.9490346908569336\n",
      "step 3847, loss: 0.7966887950897217\n",
      "step 3848, loss: 0.6434634327888489\n",
      "step 3849, loss: 0.6162051558494568\n",
      "step 3850, loss: 0.7879281044006348\n",
      "step 3851, loss: 0.6842331886291504\n",
      "step 3852, loss: 0.8387846946716309\n",
      "step 3853, loss: 0.8226848244667053\n",
      "step 3854, loss: 1.061374306678772\n",
      "step 3855, loss: 0.9181227087974548\n",
      "step 3856, loss: 1.023298740386963\n",
      "step 3857, loss: 1.1274482011795044\n",
      "step 3858, loss: 1.870492935180664\n",
      "step 3859, loss: 1.9371411800384521\n",
      "step 3860, loss: 1.914413571357727\n",
      "step 3861, loss: 1.8230150938034058\n",
      "step 3862, loss: 1.8937628269195557\n",
      "step 3863, loss: 1.838645339012146\n",
      "step 3864, loss: 1.6737585067749023\n",
      "step 3865, loss: 2.174159049987793\n",
      "step 3866, loss: 1.7814116477966309\n",
      "step 3867, loss: 1.796860933303833\n",
      "step 3868, loss: 1.7153123617172241\n",
      "step 3869, loss: 1.7988686561584473\n",
      "step 3870, loss: 1.960349678993225\n",
      "step 3871, loss: 1.9406142234802246\n",
      "step 3872, loss: 2.111834764480591\n",
      "step 3873, loss: 2.099703311920166\n",
      "step 3874, loss: 2.456249713897705\n",
      "step 3875, loss: 1.8523908853530884\n",
      "step 3876, loss: 1.7564619779586792\n",
      "step 3877, loss: 1.9297962188720703\n",
      "step 3878, loss: 2.124897003173828\n",
      "step 3879, loss: 1.925634503364563\n",
      "step 3880, loss: 2.1957879066467285\n",
      "step 3881, loss: 2.0414278507232666\n",
      "step 3882, loss: 2.040287733078003\n",
      "step 3883, loss: 2.0771970748901367\n",
      "step 3884, loss: 2.228158473968506\n",
      "step 3885, loss: 2.295656681060791\n",
      "step 3886, loss: 2.14097261428833\n",
      "step 3887, loss: 2.3871757984161377\n",
      "step 3888, loss: 2.1801748275756836\n",
      "step 3889, loss: 2.0940771102905273\n",
      "step 3890, loss: 2.0522992610931396\n",
      "step 3891, loss: 2.0695877075195312\n",
      "step 3892, loss: 2.142737865447998\n",
      "step 3893, loss: 2.213202953338623\n",
      "step 3894, loss: 2.189938545227051\n",
      "step 3895, loss: 2.3879873752593994\n",
      "step 3896, loss: 2.3812921047210693\n",
      "step 3897, loss: 2.398820400238037\n",
      "step 3898, loss: 2.4124176502227783\n",
      "step 3899, loss: 2.1083810329437256\n",
      "step 3900, loss: 1.9773768186569214\n",
      "step 3901, loss: 2.3346331119537354\n",
      "step 3902, loss: 2.141636371612549\n",
      "step 3903, loss: 2.249828338623047\n",
      "step 3904, loss: 2.1975481510162354\n",
      "step 3905, loss: 2.148965835571289\n",
      "step 3906, loss: 2.0991404056549072\n",
      "step 3907, loss: 2.296165704727173\n",
      "step 3908, loss: 1.81818425655365\n",
      "step 3909, loss: 2.416283130645752\n",
      "step 3910, loss: 2.090397596359253\n",
      "step 3911, loss: 2.274902582168579\n",
      "step 3912, loss: 2.1645607948303223\n",
      "step 3913, loss: 2.0187060832977295\n",
      "step 3914, loss: 2.1060807704925537\n",
      "step 3915, loss: 2.0862042903900146\n",
      "step 3916, loss: 1.9661362171173096\n",
      "step 3917, loss: 1.9922481775283813\n",
      "step 3918, loss: 2.491285562515259\n",
      "step 3919, loss: 2.2385172843933105\n",
      "step 3920, loss: 2.367581367492676\n",
      "step 3921, loss: 1.9960559606552124\n",
      "step 3922, loss: 2.212691307067871\n",
      "step 3923, loss: 2.2230746746063232\n",
      "step 3924, loss: 2.196162462234497\n",
      "step 3925, loss: 2.2672832012176514\n",
      "step 3926, loss: 2.0889463424682617\n",
      "step 3927, loss: 2.2262847423553467\n",
      "step 3928, loss: 2.203425407409668\n",
      "step 3929, loss: 2.0964927673339844\n",
      "step 3930, loss: 2.3438470363616943\n",
      "step 3931, loss: 2.108816146850586\n",
      "step 3932, loss: 2.305969715118408\n",
      "step 3933, loss: 2.0016489028930664\n",
      "step 3934, loss: 2.022516965866089\n",
      "step 3935, loss: 1.9175766706466675\n",
      "step 3936, loss: 2.221895694732666\n",
      "step 3937, loss: 1.9886538982391357\n",
      "step 3938, loss: 2.110924243927002\n",
      "step 3939, loss: 1.7853033542633057\n",
      "step 3940, loss: 2.0431950092315674\n",
      "step 3941, loss: 2.020826816558838\n",
      "step 3942, loss: 1.9599827527999878\n",
      "step 3943, loss: 2.0123178958892822\n",
      "step 3944, loss: 1.9796876907348633\n",
      "step 3945, loss: 2.116807222366333\n",
      "step 3946, loss: 1.9202669858932495\n",
      "step 3947, loss: 1.330269694328308\n",
      "step 3948, loss: 0.9836046695709229\n",
      "step 3949, loss: 0.9125205278396606\n",
      "step 3950, loss: 1.0137434005737305\n",
      "step 3951, loss: 0.9020805358886719\n",
      "step 3952, loss: 0.8195692896842957\n",
      "step 3953, loss: 0.8689061403274536\n",
      "step 3954, loss: 0.8721365928649902\n",
      "step 3955, loss: 0.80181485414505\n",
      "step 3956, loss: 0.9083492755889893\n",
      "step 3957, loss: 0.7823448777198792\n",
      "step 3958, loss: 0.7334674596786499\n",
      "step 3959, loss: 1.0391196012496948\n",
      "step 3960, loss: 0.8720455765724182\n",
      "step 3961, loss: 0.845471203327179\n",
      "step 3962, loss: 0.8129286766052246\n",
      "step 3963, loss: 0.82016521692276\n",
      "step 3964, loss: 0.8151448369026184\n",
      "step 3965, loss: 0.9152523279190063\n",
      "step 3966, loss: 0.8238736391067505\n",
      "step 3967, loss: 0.7902193665504456\n",
      "step 3968, loss: 1.087253212928772\n",
      "step 3969, loss: 1.9140859842300415\n",
      "step 3970, loss: 1.9115519523620605\n",
      "step 3971, loss: 1.9337605237960815\n",
      "step 3972, loss: 1.6442348957061768\n",
      "step 3973, loss: 2.010115623474121\n",
      "step 3974, loss: 2.055659770965576\n",
      "step 3975, loss: 2.141209125518799\n",
      "step 3976, loss: 1.623558521270752\n",
      "step 3977, loss: 1.7636209726333618\n",
      "step 3978, loss: 1.8191413879394531\n",
      "step 3979, loss: 1.806434154510498\n",
      "step 3980, loss: 1.874900221824646\n",
      "step 3981, loss: 1.748148798942566\n",
      "step 3982, loss: 2.1415507793426514\n",
      "step 3983, loss: 1.770363211631775\n",
      "step 3984, loss: 1.8385703563690186\n",
      "step 3985, loss: 1.9249541759490967\n",
      "step 3986, loss: 1.870223879814148\n",
      "step 3987, loss: 1.9984698295593262\n",
      "step 3988, loss: 2.025158405303955\n",
      "step 3989, loss: 1.8286054134368896\n",
      "step 3990, loss: 1.9766157865524292\n",
      "step 3991, loss: 1.8964509963989258\n",
      "step 3992, loss: 2.0153462886810303\n",
      "step 3993, loss: 1.8020983934402466\n",
      "step 3994, loss: 1.8865197896957397\n",
      "step 3995, loss: 2.1278469562530518\n",
      "step 3996, loss: 1.8887330293655396\n",
      "step 3997, loss: 2.0129287242889404\n",
      "step 3998, loss: 1.9091386795043945\n",
      "step 3999, loss: 1.970844030380249\n",
      "step 4000, loss: 2.0160973072052\n",
      "step 4001, loss: 2.0173635482788086\n",
      "step 4002, loss: 1.9035389423370361\n",
      "step 4003, loss: 1.718501091003418\n",
      "step 4004, loss: 1.8840638399124146\n",
      "step 4005, loss: 2.0070645809173584\n",
      "step 4006, loss: 2.2461819648742676\n",
      "step 4007, loss: 2.11336350440979\n",
      "step 4008, loss: 1.6409491300582886\n",
      "step 4009, loss: 1.9340014457702637\n",
      "step 4010, loss: 1.8416662216186523\n",
      "step 4011, loss: 1.8199081420898438\n",
      "step 4012, loss: 1.7593860626220703\n",
      "step 4013, loss: 1.9504010677337646\n",
      "step 4014, loss: 2.0808799266815186\n",
      "step 4015, loss: 2.038604497909546\n",
      "step 4016, loss: 1.8843938112258911\n",
      "step 4017, loss: 1.8917627334594727\n",
      "step 4018, loss: 1.9841269254684448\n",
      "step 4019, loss: 1.802351474761963\n",
      "step 4020, loss: 2.1221089363098145\n",
      "step 4021, loss: 1.9528007507324219\n",
      "step 4022, loss: 2.4319941997528076\n",
      "step 4023, loss: 2.103929281234741\n",
      "step 4024, loss: 1.8242279291152954\n",
      "step 4025, loss: 2.2368223667144775\n",
      "step 4026, loss: 2.062352418899536\n",
      "step 4027, loss: 1.7812259197235107\n",
      "step 4028, loss: 1.827080249786377\n",
      "step 4029, loss: 1.9053936004638672\n",
      "step 4030, loss: 1.9498858451843262\n",
      "step 4031, loss: 2.023103713989258\n",
      "step 4032, loss: 1.9713366031646729\n",
      "step 4033, loss: 2.000732898712158\n",
      "step 4034, loss: 2.1642212867736816\n",
      "step 4035, loss: 2.0537874698638916\n",
      "step 4036, loss: 2.0089168548583984\n",
      "step 4037, loss: 2.1055634021759033\n",
      "step 4038, loss: 1.8545136451721191\n",
      "step 4039, loss: 1.990578055381775\n",
      "step 4040, loss: 2.0528454780578613\n",
      "step 4041, loss: 2.0222373008728027\n",
      "step 4042, loss: 2.3357503414154053\n",
      "step 4043, loss: 1.811030387878418\n",
      "step 4044, loss: 1.836234211921692\n",
      "step 4045, loss: 2.113478660583496\n",
      "step 4046, loss: 1.9785090684890747\n",
      "step 4047, loss: 1.9993329048156738\n",
      "step 4048, loss: 1.952143669128418\n",
      "step 4049, loss: 2.1836323738098145\n",
      "step 4050, loss: 2.033994674682617\n",
      "step 4051, loss: 2.0455057621002197\n",
      "step 4052, loss: 1.8178445100784302\n",
      "step 4053, loss: 1.879411220550537\n",
      "step 4054, loss: 1.7438859939575195\n",
      "step 4055, loss: 1.7683311700820923\n",
      "step 4056, loss: 1.7763174772262573\n",
      "step 4057, loss: 2.049783229827881\n",
      "step 4058, loss: 2.1010873317718506\n",
      "step 4059, loss: 1.857454538345337\n",
      "step 4060, loss: 1.88803231716156\n",
      "step 4061, loss: 2.0576224327087402\n",
      "step 4062, loss: 2.040024995803833\n",
      "step 4063, loss: 1.8916161060333252\n",
      "step 4064, loss: 2.1922380924224854\n",
      "step 4065, loss: 2.0608012676239014\n",
      "step 4066, loss: 2.0059475898742676\n",
      "step 4067, loss: 2.241093397140503\n",
      "step 4068, loss: 1.9282838106155396\n",
      "step 4069, loss: 2.112272262573242\n",
      "step 4070, loss: 2.086942434310913\n",
      "step 4071, loss: 2.0738673210144043\n",
      "step 4072, loss: 2.010449171066284\n",
      "step 4073, loss: 1.695345163345337\n",
      "step 4074, loss: 1.7746919393539429\n",
      "step 4075, loss: 1.9227417707443237\n",
      "step 4076, loss: 2.074185848236084\n",
      "step 4077, loss: 2.017719268798828\n",
      "step 4078, loss: 1.7614409923553467\n",
      "step 4079, loss: 0.9899765849113464\n",
      "step 4080, loss: 0.8975590467453003\n",
      "step 4081, loss: 0.8968408107757568\n",
      "step 4082, loss: 0.7461274862289429\n",
      "step 4083, loss: 0.9897220134735107\n",
      "step 4084, loss: 1.0377416610717773\n",
      "step 4085, loss: 0.9385792016983032\n",
      "step 4086, loss: 1.0422089099884033\n",
      "step 4087, loss: 0.8714019656181335\n",
      "step 4088, loss: 0.8991454243659973\n",
      "step 4089, loss: 0.7824075818061829\n",
      "step 4090, loss: 0.8255779147148132\n",
      "step 4091, loss: 0.9088531136512756\n",
      "step 4092, loss: 0.9737790822982788\n",
      "step 4093, loss: 0.641698956489563\n",
      "step 4094, loss: 0.6197240352630615\n",
      "step 4095, loss: 0.6652714014053345\n",
      "step 4096, loss: 0.5848971009254456\n",
      "step 4097, loss: 0.6859229803085327\n",
      "step 4098, loss: 0.733708381652832\n",
      "step 4099, loss: 0.7512654066085815\n",
      "step 4100, loss: 0.7194185853004456\n",
      "step 4101, loss: 0.9099463224411011\n",
      "step 4102, loss: 0.853564977645874\n",
      "step 4103, loss: 0.6181589365005493\n",
      "step 4104, loss: 0.7385005950927734\n",
      "step 4105, loss: 0.8096623420715332\n",
      "step 4106, loss: 0.8385413289070129\n",
      "step 4107, loss: 0.6646154522895813\n",
      "step 4108, loss: 1.5077160596847534\n",
      "step 4109, loss: 1.8000341653823853\n",
      "step 4110, loss: 1.702390193939209\n",
      "step 4111, loss: 1.4929181337356567\n",
      "step 4112, loss: 1.5041959285736084\n",
      "step 4113, loss: 1.794180154800415\n",
      "step 4114, loss: 1.886720895767212\n",
      "step 4115, loss: 2.090242624282837\n",
      "step 4116, loss: 1.9628607034683228\n",
      "step 4117, loss: 1.984907627105713\n",
      "step 4118, loss: 2.2214717864990234\n",
      "step 4119, loss: 1.8402513265609741\n",
      "step 4120, loss: 1.9907400608062744\n",
      "step 4121, loss: 1.9195371866226196\n",
      "step 4122, loss: 2.064117431640625\n",
      "step 4123, loss: 1.9585503339767456\n",
      "step 4124, loss: 1.8637458086013794\n",
      "step 4125, loss: 1.7775076627731323\n",
      "step 4126, loss: 1.7793420553207397\n",
      "step 4127, loss: 1.6800264120101929\n",
      "step 4128, loss: 1.7795960903167725\n",
      "step 4129, loss: 1.9991106986999512\n",
      "step 4130, loss: 1.7920480966567993\n",
      "step 4131, loss: 1.9628087282180786\n",
      "step 4132, loss: 1.7206400632858276\n",
      "step 4133, loss: 1.922409176826477\n",
      "step 4134, loss: 1.9147729873657227\n",
      "step 4135, loss: 1.9662070274353027\n",
      "step 4136, loss: 2.0477027893066406\n",
      "step 4137, loss: 1.675930142402649\n",
      "step 4138, loss: 2.323866128921509\n",
      "step 4139, loss: 1.954685926437378\n",
      "step 4140, loss: 1.931321620941162\n",
      "step 4141, loss: 2.282301425933838\n",
      "step 4142, loss: 2.063192367553711\n",
      "step 4143, loss: 2.126082420349121\n",
      "step 4144, loss: 2.0687763690948486\n",
      "step 4145, loss: 2.3118536472320557\n",
      "step 4146, loss: 2.2103586196899414\n",
      "step 4147, loss: 2.0614209175109863\n",
      "step 4148, loss: 2.1731019020080566\n",
      "step 4149, loss: 2.2945480346679688\n",
      "step 4150, loss: 2.305297613143921\n",
      "step 4151, loss: 2.131678342819214\n",
      "step 4152, loss: 2.046837091445923\n",
      "step 4153, loss: 2.145646810531616\n",
      "step 4154, loss: 1.950662612915039\n",
      "step 4155, loss: 2.1413493156433105\n",
      "step 4156, loss: 2.0512845516204834\n",
      "step 4157, loss: 1.925804615020752\n",
      "step 4158, loss: 1.9236369132995605\n",
      "step 4159, loss: 2.041219472885132\n",
      "step 4160, loss: 2.037564754486084\n",
      "step 4161, loss: 1.8598644733428955\n",
      "step 4162, loss: 2.186054229736328\n",
      "step 4163, loss: 2.0659255981445312\n",
      "step 4164, loss: 1.8040881156921387\n",
      "step 4165, loss: 1.7644555568695068\n",
      "step 4166, loss: 1.8885711431503296\n",
      "step 4167, loss: 1.9959957599639893\n",
      "step 4168, loss: 1.8922390937805176\n",
      "step 4169, loss: 2.088831901550293\n",
      "step 4170, loss: 1.8927702903747559\n",
      "step 4171, loss: 2.3410274982452393\n",
      "step 4172, loss: 1.758370041847229\n",
      "step 4173, loss: 1.8930156230926514\n",
      "step 4174, loss: 2.078312635421753\n",
      "step 4175, loss: 1.8471733331680298\n",
      "step 4176, loss: 1.9549963474273682\n",
      "step 4177, loss: 1.9482786655426025\n",
      "step 4178, loss: 2.1993653774261475\n",
      "step 4179, loss: 2.085041046142578\n",
      "step 4180, loss: 1.9685378074645996\n",
      "step 4181, loss: 1.6941174268722534\n",
      "step 4182, loss: 2.0766870975494385\n",
      "step 4183, loss: 1.9302994012832642\n",
      "step 4184, loss: 1.8386462926864624\n",
      "step 4185, loss: 1.8889963626861572\n",
      "step 4186, loss: 2.1493844985961914\n",
      "step 4187, loss: 2.165156841278076\n",
      "step 4188, loss: 2.188920259475708\n",
      "step 4189, loss: 2.026789903640747\n",
      "step 4190, loss: 1.8970036506652832\n",
      "step 4191, loss: 1.9944345951080322\n",
      "step 4192, loss: 2.193920135498047\n",
      "step 4193, loss: 2.1703128814697266\n",
      "step 4194, loss: 2.3215372562408447\n",
      "step 4195, loss: 2.378216505050659\n",
      "step 4196, loss: 1.973039150238037\n",
      "step 4197, loss: 1.9636703729629517\n",
      "step 4198, loss: 2.0695319175720215\n",
      "step 4199, loss: 2.0982978343963623\n",
      "step 4200, loss: 1.8537429571151733\n",
      "step 4201, loss: 2.0769786834716797\n",
      "step 4202, loss: 2.2185637950897217\n",
      "step 4203, loss: 1.9196382761001587\n",
      "step 4204, loss: 2.0332980155944824\n",
      "step 4205, loss: 2.1842381954193115\n",
      "step 4206, loss: 2.1304476261138916\n",
      "step 4207, loss: 1.846434235572815\n",
      "step 4208, loss: 1.9450409412384033\n",
      "step 4209, loss: 2.2046947479248047\n",
      "step 4210, loss: 2.1899211406707764\n",
      "step 4211, loss: 1.7698924541473389\n",
      "step 4212, loss: 2.0131494998931885\n",
      "step 4213, loss: 2.1706960201263428\n",
      "step 4214, loss: 2.2102057933807373\n",
      "step 4215, loss: 2.003385066986084\n",
      "step 4216, loss: 1.9091112613677979\n",
      "step 4217, loss: 2.254438877105713\n",
      "step 4218, loss: 2.3468620777130127\n",
      "step 4219, loss: 2.2255170345306396\n",
      "step 4220, loss: 1.808652400970459\n",
      "step 4221, loss: 2.262890577316284\n",
      "step 4222, loss: 2.170334815979004\n",
      "step 4223, loss: 2.204847574234009\n",
      "step 4224, loss: 1.8371272087097168\n",
      "step 4225, loss: 2.0100033283233643\n",
      "step 4226, loss: 2.0472335815429688\n",
      "step 4227, loss: 2.1523172855377197\n",
      "step 4228, loss: 2.0863726139068604\n",
      "step 4229, loss: 2.269775629043579\n",
      "step 4230, loss: 2.1542859077453613\n",
      "step 4231, loss: 2.1568212509155273\n",
      "step 4232, loss: 2.324524164199829\n",
      "step 4233, loss: 2.195566177368164\n",
      "step 4234, loss: 2.3307418823242188\n",
      "step 4235, loss: 2.3607585430145264\n",
      "step 4236, loss: 1.9708526134490967\n",
      "step 4237, loss: 2.1520586013793945\n",
      "step 4238, loss: 2.0920090675354004\n",
      "step 4239, loss: 1.956184983253479\n",
      "step 4240, loss: 2.239201068878174\n",
      "step 4241, loss: 1.9233169555664062\n",
      "step 4242, loss: 2.2299818992614746\n",
      "step 4243, loss: 1.9396830797195435\n",
      "step 4244, loss: 2.23134708404541\n",
      "step 4245, loss: 2.0776522159576416\n",
      "step 4246, loss: 1.9229952096939087\n",
      "step 4247, loss: 1.8456008434295654\n",
      "step 4248, loss: 2.1923632621765137\n",
      "step 4249, loss: 2.147141456604004\n",
      "step 4250, loss: 1.9763648509979248\n",
      "step 4251, loss: 2.2387170791625977\n",
      "step 4252, loss: 2.1778934001922607\n",
      "step 4253, loss: 1.8864573240280151\n",
      "step 4254, loss: 2.1968276500701904\n",
      "step 4255, loss: 2.2200491428375244\n",
      "step 4256, loss: 2.046523332595825\n",
      "step 4257, loss: 2.2063310146331787\n",
      "step 4258, loss: 2.290147066116333\n",
      "step 4259, loss: 2.2297163009643555\n",
      "step 4260, loss: 2.3794937133789062\n",
      "step 4261, loss: 2.05460524559021\n",
      "step 4262, loss: 2.05301833152771\n",
      "step 4263, loss: 2.050752639770508\n",
      "step 4264, loss: 2.248248338699341\n",
      "step 4265, loss: 2.0158538818359375\n",
      "step 4266, loss: 1.8809843063354492\n",
      "step 4267, loss: 1.612843632698059\n",
      "step 4268, loss: 1.814309000968933\n",
      "step 4269, loss: 1.8556886911392212\n",
      "step 4270, loss: 1.997408151626587\n",
      "step 4271, loss: 1.7889976501464844\n",
      "step 4272, loss: 2.053483486175537\n",
      "step 4273, loss: 1.7127702236175537\n",
      "step 4274, loss: 1.9227285385131836\n",
      "step 4275, loss: 1.7704753875732422\n",
      "step 4276, loss: 2.0083045959472656\n",
      "step 4277, loss: 1.8822370767593384\n",
      "step 4278, loss: 1.7672748565673828\n",
      "step 4279, loss: 1.6140090227127075\n",
      "step 4280, loss: 1.7260265350341797\n",
      "step 4281, loss: 2.08280086517334\n",
      "step 4282, loss: 2.044327735900879\n",
      "step 4283, loss: 1.9633017778396606\n",
      "step 4284, loss: 2.0765750408172607\n",
      "step 4285, loss: 1.8700885772705078\n",
      "step 4286, loss: 1.9550117254257202\n",
      "step 4287, loss: 1.8505090475082397\n",
      "step 4288, loss: 1.9116876125335693\n",
      "step 4289, loss: 1.7953729629516602\n",
      "step 4290, loss: 1.8853368759155273\n",
      "step 4291, loss: 1.7031950950622559\n",
      "step 4292, loss: 1.0113835334777832\n",
      "step 4293, loss: 1.0686631202697754\n",
      "step 4294, loss: 1.0137109756469727\n",
      "step 4295, loss: 0.9054454565048218\n",
      "step 4296, loss: 1.0050979852676392\n",
      "step 4297, loss: 0.9835049510002136\n",
      "step 4298, loss: 0.8964530229568481\n",
      "step 4299, loss: 0.9112966656684875\n",
      "step 4300, loss: 0.8971323370933533\n",
      "step 4301, loss: 0.818523645401001\n",
      "step 4302, loss: 0.7599887251853943\n",
      "step 4303, loss: 0.6398718357086182\n",
      "step 4304, loss: 0.7982960939407349\n",
      "step 4305, loss: 0.6213042736053467\n",
      "step 4306, loss: 0.6929843425750732\n",
      "step 4307, loss: 0.9282664656639099\n",
      "step 4308, loss: 0.9811175465583801\n",
      "step 4309, loss: 0.6850343942642212\n",
      "step 4310, loss: 0.8002718687057495\n",
      "step 4311, loss: 0.7440358996391296\n",
      "step 4312, loss: 0.581927478313446\n",
      "step 4313, loss: 0.6166529655456543\n",
      "step 4314, loss: 0.8623559474945068\n",
      "step 4315, loss: 0.8211303353309631\n",
      "step 4316, loss: 0.787031888961792\n",
      "step 4317, loss: 0.7298315763473511\n",
      "step 4318, loss: 0.8693021535873413\n",
      "step 4319, loss: 1.0067622661590576\n",
      "step 4320, loss: 0.8008387088775635\n",
      "step 4321, loss: 0.8675218820571899\n",
      "step 4322, loss: 0.972797155380249\n",
      "step 4323, loss: 1.9174697399139404\n",
      "step 4324, loss: 1.9722944498062134\n",
      "step 4325, loss: 1.7879458665847778\n",
      "step 4326, loss: 1.8148951530456543\n",
      "step 4327, loss: 1.7534070014953613\n",
      "step 4328, loss: 1.7842692136764526\n",
      "step 4329, loss: 2.0298941135406494\n",
      "step 4330, loss: 1.8748505115509033\n",
      "step 4331, loss: 1.7365434169769287\n",
      "step 4332, loss: 1.6722315549850464\n",
      "step 4333, loss: 1.7672194242477417\n",
      "step 4334, loss: 1.821401596069336\n",
      "step 4335, loss: 1.5376356840133667\n",
      "step 4336, loss: 1.8252286911010742\n",
      "step 4337, loss: 1.9401566982269287\n",
      "step 4338, loss: 1.8003783226013184\n",
      "step 4339, loss: 1.9335315227508545\n",
      "step 4340, loss: 1.7508788108825684\n",
      "step 4341, loss: 2.131376266479492\n",
      "step 4342, loss: 1.8578996658325195\n",
      "step 4343, loss: 1.7994678020477295\n",
      "step 4344, loss: 1.6305367946624756\n",
      "step 4345, loss: 1.64255952835083\n",
      "step 4346, loss: 1.8657158613204956\n",
      "step 4347, loss: 1.749061107635498\n",
      "step 4348, loss: 1.9043821096420288\n",
      "step 4349, loss: 1.653723955154419\n",
      "step 4350, loss: 1.6027165651321411\n",
      "step 4351, loss: 1.6613423824310303\n",
      "step 4352, loss: 1.588135838508606\n",
      "step 4353, loss: 2.084993362426758\n",
      "step 4354, loss: 1.7047505378723145\n",
      "step 4355, loss: 1.7061148881912231\n",
      "step 4356, loss: 1.6677708625793457\n",
      "step 4357, loss: 1.8629727363586426\n",
      "step 4358, loss: 1.6328052282333374\n",
      "step 4359, loss: 1.4641823768615723\n",
      "step 4360, loss: 1.4722496271133423\n",
      "step 4361, loss: 1.4779030084609985\n",
      "step 4362, loss: 1.070539951324463\n",
      "step 4363, loss: 1.0925260782241821\n",
      "step 4364, loss: 0.9933632612228394\n",
      "step 4365, loss: 1.0658059120178223\n",
      "step 4366, loss: 0.9808201789855957\n",
      "step 4367, loss: 0.8774827718734741\n",
      "step 4368, loss: 0.9082830548286438\n",
      "step 4369, loss: 0.8974099159240723\n",
      "step 4370, loss: 1.1740288734436035\n",
      "step 4371, loss: 0.9436628818511963\n",
      "step 4372, loss: 0.7285805344581604\n",
      "step 4373, loss: 0.7022944688796997\n",
      "step 4374, loss: 0.7067084908485413\n",
      "step 4375, loss: 0.5273038148880005\n",
      "step 4376, loss: 1.5830079317092896\n",
      "step 4377, loss: 1.7374556064605713\n",
      "step 4378, loss: 1.8208768367767334\n",
      "step 4379, loss: 2.092144250869751\n",
      "step 4380, loss: 1.7647242546081543\n",
      "step 4381, loss: 1.9474120140075684\n",
      "step 4382, loss: 1.8282171487808228\n",
      "step 4383, loss: 1.934282898902893\n",
      "step 4384, loss: 1.817270278930664\n",
      "step 4385, loss: 1.9818073511123657\n",
      "step 4386, loss: 1.8797543048858643\n",
      "step 4387, loss: 2.0920143127441406\n",
      "step 4388, loss: 1.9433436393737793\n",
      "step 4389, loss: 2.021167755126953\n",
      "step 4390, loss: 2.2766032218933105\n",
      "step 4391, loss: 2.1164486408233643\n",
      "step 4392, loss: 1.8314847946166992\n",
      "step 4393, loss: 1.944931149482727\n",
      "step 4394, loss: 2.1193909645080566\n",
      "step 4395, loss: 1.8650593757629395\n",
      "step 4396, loss: 1.9347392320632935\n",
      "step 4397, loss: 1.8534057140350342\n",
      "step 4398, loss: 1.9872758388519287\n",
      "step 4399, loss: 1.9637411832809448\n",
      "step 4400, loss: 2.0984320640563965\n",
      "step 4401, loss: 1.890742301940918\n",
      "step 4402, loss: 1.9972771406173706\n",
      "step 4403, loss: 1.9657514095306396\n",
      "step 4404, loss: 1.9323118925094604\n",
      "step 4405, loss: 1.700130581855774\n",
      "step 4406, loss: 1.7152788639068604\n",
      "step 4407, loss: 1.990237832069397\n",
      "step 4408, loss: 1.8047319650650024\n",
      "step 4409, loss: 1.8117843866348267\n",
      "step 4410, loss: 1.9928992986679077\n",
      "step 4411, loss: 2.0130181312561035\n",
      "step 4412, loss: 1.7502304315567017\n",
      "step 4413, loss: 1.9048928022384644\n",
      "step 4414, loss: 1.893227458000183\n",
      "step 4415, loss: 1.6174410581588745\n",
      "step 4416, loss: 1.737221360206604\n",
      "step 4417, loss: 1.6665549278259277\n",
      "step 4418, loss: 1.9267562627792358\n",
      "step 4419, loss: 1.886372447013855\n",
      "step 4420, loss: 1.5997291803359985\n",
      "step 4421, loss: 1.7586153745651245\n",
      "step 4422, loss: 1.774003267288208\n",
      "step 4423, loss: 1.9018027782440186\n",
      "step 4424, loss: 1.7789632081985474\n",
      "step 4425, loss: 1.6881588697433472\n",
      "step 4426, loss: 1.7074896097183228\n",
      "step 4427, loss: 1.9114341735839844\n",
      "step 4428, loss: 1.7861554622650146\n",
      "step 4429, loss: 1.7787209749221802\n",
      "step 4430, loss: 1.9775514602661133\n",
      "step 4431, loss: 1.9639233350753784\n",
      "step 4432, loss: 1.7503465414047241\n",
      "step 4433, loss: 1.9368808269500732\n",
      "step 4434, loss: 1.9738425016403198\n",
      "step 4435, loss: 1.8250069618225098\n",
      "step 4436, loss: 1.912304162979126\n",
      "step 4437, loss: 1.8505358695983887\n",
      "step 4438, loss: 1.9019719362258911\n",
      "step 4439, loss: 2.042325735092163\n",
      "step 4440, loss: 1.9910221099853516\n",
      "step 4441, loss: 1.82968008518219\n",
      "step 4442, loss: 1.9014948606491089\n",
      "step 4443, loss: 1.9280694723129272\n",
      "step 4444, loss: 1.9966615438461304\n",
      "step 4445, loss: 1.938691258430481\n",
      "step 4446, loss: 2.101470947265625\n",
      "step 4447, loss: 1.8837155103683472\n",
      "step 4448, loss: 1.897133469581604\n",
      "step 4449, loss: 1.9491961002349854\n",
      "step 4450, loss: 2.060185194015503\n",
      "step 4451, loss: 1.9595553874969482\n",
      "step 4452, loss: 1.8650954961776733\n",
      "step 4453, loss: 1.942185401916504\n",
      "step 4454, loss: 2.1523118019104004\n",
      "step 4455, loss: 2.129149913787842\n",
      "step 4456, loss: 1.965248465538025\n",
      "step 4457, loss: 1.873299479484558\n",
      "step 4458, loss: 2.0596230030059814\n",
      "step 4459, loss: 1.9000093936920166\n",
      "step 4460, loss: 2.017368793487549\n",
      "step 4461, loss: 1.6535316705703735\n",
      "step 4462, loss: 1.8810981512069702\n",
      "step 4463, loss: 2.0124616622924805\n",
      "step 4464, loss: 1.591015100479126\n",
      "step 4465, loss: 1.6318440437316895\n",
      "step 4466, loss: 1.0793853998184204\n",
      "step 4467, loss: 1.1260697841644287\n",
      "step 4468, loss: 1.1570544242858887\n",
      "step 4469, loss: 1.129974603652954\n",
      "step 4470, loss: 0.9912559986114502\n",
      "step 4471, loss: 0.9354472756385803\n",
      "step 4472, loss: 0.8271414041519165\n",
      "step 4473, loss: 0.8847200274467468\n",
      "step 4474, loss: 0.9880216717720032\n",
      "step 4475, loss: 0.9929112195968628\n",
      "step 4476, loss: 0.9168500304222107\n",
      "step 4477, loss: 0.9190889596939087\n",
      "step 4478, loss: 0.8893967866897583\n",
      "step 4479, loss: 1.095080018043518\n",
      "step 4480, loss: 1.2632650136947632\n",
      "step 4481, loss: 1.6170995235443115\n",
      "step 4482, loss: 1.6407999992370605\n",
      "step 4483, loss: 1.686049222946167\n",
      "step 4484, loss: 1.598605990409851\n",
      "step 4485, loss: 1.6100560426712036\n",
      "step 4486, loss: 1.5254275798797607\n",
      "step 4487, loss: 1.4282840490341187\n",
      "step 4488, loss: 1.618951678276062\n",
      "step 4489, loss: 1.6428507566452026\n",
      "step 4490, loss: 1.6098042726516724\n",
      "step 4491, loss: 1.5916733741760254\n",
      "step 4492, loss: 1.7633445262908936\n",
      "step 4493, loss: 1.6491856575012207\n",
      "step 4494, loss: 1.8245155811309814\n",
      "step 4495, loss: 1.8596806526184082\n",
      "step 4496, loss: 1.4683603048324585\n",
      "step 4497, loss: 1.5833191871643066\n",
      "step 4498, loss: 1.6050350666046143\n",
      "step 4499, loss: 1.5218583345413208\n",
      "step 4500, loss: 1.4714888334274292\n",
      "step 4501, loss: 1.882198691368103\n",
      "step 4502, loss: 1.7796406745910645\n",
      "step 4503, loss: 1.6328340768814087\n",
      "step 4504, loss: 1.7388343811035156\n",
      "step 4505, loss: 1.8910118341445923\n",
      "step 4506, loss: 1.655542016029358\n",
      "step 4507, loss: 1.794013500213623\n",
      "step 4508, loss: 1.866698980331421\n",
      "step 4509, loss: 1.6948615312576294\n",
      "step 4510, loss: 1.543229103088379\n",
      "step 4511, loss: 1.4357248544692993\n",
      "step 4512, loss: 1.717343807220459\n",
      "step 4513, loss: 1.5440794229507446\n",
      "step 4514, loss: 1.7459365129470825\n",
      "step 4515, loss: 1.608272671699524\n",
      "step 4516, loss: 1.6843066215515137\n",
      "step 4517, loss: 1.4742534160614014\n",
      "step 4518, loss: 1.6873244047164917\n",
      "step 4519, loss: 1.7492775917053223\n",
      "step 4520, loss: 1.4544329643249512\n",
      "step 4521, loss: 1.5522551536560059\n",
      "step 4522, loss: 1.4565225839614868\n",
      "step 4523, loss: 1.6004421710968018\n",
      "step 4524, loss: 1.406686782836914\n",
      "step 4525, loss: 1.7029410600662231\n",
      "step 4526, loss: 1.5110524892807007\n",
      "step 4527, loss: 1.409995675086975\n",
      "step 4528, loss: 1.6920245885849\n",
      "step 4529, loss: 1.3276240825653076\n",
      "step 4530, loss: 1.5748037099838257\n",
      "step 4531, loss: 1.6512579917907715\n",
      "step 4532, loss: 1.744893193244934\n",
      "step 4533, loss: 1.6333032846450806\n",
      "step 4534, loss: 1.5918009281158447\n",
      "step 4535, loss: 1.5080785751342773\n",
      "step 4536, loss: 1.4697262048721313\n",
      "step 4537, loss: 1.8607121706008911\n",
      "step 4538, loss: 1.2523640394210815\n",
      "step 4539, loss: 0.996612548828125\n",
      "step 4540, loss: 0.8766800761222839\n",
      "step 4541, loss: 0.790169358253479\n",
      "step 4542, loss: 0.8344114422798157\n",
      "step 4543, loss: 0.8829293251037598\n",
      "step 4544, loss: 0.7797204256057739\n",
      "step 4545, loss: 0.6825759410858154\n",
      "step 4546, loss: 0.8125121593475342\n",
      "step 4547, loss: 0.8555779457092285\n",
      "step 4548, loss: 0.712628185749054\n",
      "step 4549, loss: 0.6278210878372192\n",
      "step 4550, loss: 0.6639646291732788\n",
      "step 4551, loss: 0.9897634387016296\n",
      "step 4552, loss: 1.1885805130004883\n",
      "step 4553, loss: 1.6629060506820679\n",
      "step 4554, loss: 1.5073782205581665\n",
      "step 4555, loss: 1.5397332906723022\n",
      "step 4556, loss: 1.3148144483566284\n",
      "step 4557, loss: 1.403070092201233\n",
      "step 4558, loss: 1.3332839012145996\n",
      "step 4559, loss: 1.6456350088119507\n",
      "step 4560, loss: 1.3865272998809814\n",
      "step 4561, loss: 1.5751662254333496\n",
      "step 4562, loss: 1.5766539573669434\n",
      "step 4563, loss: 1.692936658859253\n",
      "step 4564, loss: 1.4264769554138184\n",
      "step 4565, loss: 1.6377309560775757\n",
      "step 4566, loss: 1.628941535949707\n",
      "step 4567, loss: 1.6393555402755737\n",
      "step 4568, loss: 1.7164416313171387\n",
      "step 4569, loss: 1.822016716003418\n",
      "step 4570, loss: 1.7991081476211548\n",
      "step 4571, loss: 1.7828813791275024\n",
      "step 4572, loss: 1.7107630968093872\n",
      "step 4573, loss: 1.7384073734283447\n",
      "step 4574, loss: 1.8515135049819946\n",
      "step 4575, loss: 1.5317009687423706\n",
      "step 4576, loss: 1.6648342609405518\n",
      "step 4577, loss: 1.8068029880523682\n",
      "step 4578, loss: 1.5531562566757202\n",
      "step 4579, loss: 1.760074257850647\n",
      "step 4580, loss: 1.8447113037109375\n",
      "step 4581, loss: 1.7647470235824585\n",
      "step 4582, loss: 1.5394457578659058\n",
      "step 4583, loss: 1.9757994413375854\n",
      "step 4584, loss: 1.6830151081085205\n",
      "step 4585, loss: 1.637335181236267\n",
      "step 4586, loss: 1.527193307876587\n",
      "step 4587, loss: 1.6733704805374146\n",
      "step 4588, loss: 1.6034923791885376\n",
      "step 4589, loss: 1.788108229637146\n",
      "step 4590, loss: 1.575395941734314\n",
      "step 4591, loss: 1.694907784461975\n",
      "step 4592, loss: 1.8956005573272705\n",
      "step 4593, loss: 1.6478127241134644\n",
      "step 4594, loss: 1.6806923151016235\n",
      "step 4595, loss: 1.8546279668807983\n",
      "step 4596, loss: 1.6684119701385498\n",
      "step 4597, loss: 1.7131433486938477\n",
      "step 4598, loss: 1.9149930477142334\n",
      "step 4599, loss: 1.684078335762024\n",
      "step 4600, loss: 1.7511330842971802\n",
      "step 4601, loss: 1.8871475458145142\n",
      "step 4602, loss: 1.6964224576950073\n",
      "step 4603, loss: 1.628629446029663\n",
      "step 4604, loss: 1.5492202043533325\n",
      "step 4605, loss: 1.7696123123168945\n",
      "step 4606, loss: 1.53069269657135\n",
      "step 4607, loss: 1.6299960613250732\n",
      "step 4608, loss: 1.5925250053405762\n",
      "step 4609, loss: 1.737719178199768\n",
      "step 4610, loss: 1.7694569826126099\n",
      "step 4611, loss: 1.522576093673706\n",
      "step 4612, loss: 1.4394513368606567\n",
      "step 4613, loss: 1.5221645832061768\n",
      "step 4614, loss: 1.8804324865341187\n",
      "step 4615, loss: 1.7008695602416992\n",
      "step 4616, loss: 1.6189574003219604\n",
      "step 4617, loss: 1.5809251070022583\n",
      "step 4618, loss: 1.6909327507019043\n",
      "step 4619, loss: 1.594893455505371\n",
      "step 4620, loss: 1.6501328945159912\n",
      "step 4621, loss: 1.5939979553222656\n",
      "step 4622, loss: 1.5464853048324585\n",
      "step 4623, loss: 1.655653476715088\n",
      "step 4624, loss: 1.626835584640503\n",
      "step 4625, loss: 1.5275230407714844\n",
      "step 4626, loss: 1.706482172012329\n",
      "step 4627, loss: 1.5258194208145142\n",
      "step 4628, loss: 1.305107593536377\n",
      "step 4629, loss: 1.6887741088867188\n",
      "step 4630, loss: 1.766442894935608\n",
      "step 4631, loss: 1.7972338199615479\n",
      "step 4632, loss: 1.6405411958694458\n",
      "step 4633, loss: 1.729030966758728\n",
      "step 4634, loss: 1.8732854127883911\n",
      "step 4635, loss: 1.7380166053771973\n",
      "step 4636, loss: 1.6714073419570923\n",
      "step 4637, loss: 1.5709854364395142\n",
      "step 4638, loss: 1.5063575506210327\n",
      "step 4639, loss: 1.6347306966781616\n",
      "step 4640, loss: 1.739611029624939\n",
      "step 4641, loss: 1.810750961303711\n",
      "step 4642, loss: 1.9059035778045654\n",
      "step 4643, loss: 1.6842457056045532\n",
      "step 4644, loss: 1.9690428972244263\n",
      "step 4645, loss: 1.6980135440826416\n",
      "step 4646, loss: 1.7414655685424805\n",
      "step 4647, loss: 1.6136269569396973\n",
      "step 4648, loss: 1.6381034851074219\n",
      "step 4649, loss: 1.8573614358901978\n",
      "step 4650, loss: 1.6399635076522827\n",
      "step 4651, loss: 1.7253901958465576\n",
      "step 4652, loss: 1.7955422401428223\n",
      "step 4653, loss: 1.6046491861343384\n",
      "step 4654, loss: 1.5724457502365112\n",
      "step 4655, loss: 1.5412673950195312\n",
      "step 4656, loss: 1.6898016929626465\n",
      "step 4657, loss: 1.6647202968597412\n",
      "step 4658, loss: 1.4258620738983154\n",
      "step 4659, loss: 1.7933716773986816\n",
      "step 4660, loss: 1.3249058723449707\n",
      "step 4661, loss: 1.381760835647583\n",
      "step 4662, loss: 1.6459767818450928\n",
      "step 4663, loss: 1.4516576528549194\n",
      "step 4664, loss: 1.0160263776779175\n",
      "step 4665, loss: 0.8861915469169617\n",
      "step 4666, loss: 0.9855974912643433\n",
      "step 4667, loss: 0.6457529664039612\n",
      "step 4668, loss: 0.6561187505722046\n",
      "step 4669, loss: 0.7994683980941772\n",
      "step 4670, loss: 0.6539918780326843\n",
      "step 4671, loss: 0.49908480048179626\n",
      "step 4672, loss: 0.781029462814331\n",
      "step 4673, loss: 0.6783123016357422\n",
      "step 4674, loss: 0.645761251449585\n",
      "step 4675, loss: 0.7921128869056702\n",
      "step 4676, loss: 0.8562738299369812\n",
      "step 4677, loss: 0.6607861518859863\n",
      "step 4678, loss: 0.7295626401901245\n",
      "step 4679, loss: 0.8609157800674438\n",
      "step 4680, loss: 0.6801055669784546\n",
      "step 4681, loss: 0.6004459261894226\n",
      "step 4682, loss: 0.7355941534042358\n",
      "step 4683, loss: 0.5347481966018677\n",
      "step 4684, loss: 0.8996164202690125\n",
      "step 4685, loss: 0.9248132109642029\n",
      "step 4686, loss: 0.7408568859100342\n",
      "step 4687, loss: 0.791549801826477\n",
      "step 4688, loss: 0.6807326078414917\n",
      "step 4689, loss: 0.7266388535499573\n",
      "step 4690, loss: 0.8771461248397827\n",
      "step 4691, loss: 0.7684379816055298\n",
      "step 4692, loss: 1.3866268396377563\n",
      "step 4693, loss: 1.4638649225234985\n",
      "step 4694, loss: 1.56996750831604\n",
      "step 4695, loss: 1.4780030250549316\n",
      "step 4696, loss: 1.6211744546890259\n",
      "step 4697, loss: 1.6229604482650757\n",
      "step 4698, loss: 1.5599220991134644\n",
      "step 4699, loss: 1.563016653060913\n",
      "step 4700, loss: 1.521502137184143\n",
      "step 4701, loss: 1.4942244291305542\n",
      "step 4702, loss: 1.466068983078003\n",
      "step 4703, loss: 1.66213858127594\n",
      "step 4704, loss: 1.577148199081421\n",
      "step 4705, loss: 1.8598273992538452\n",
      "step 4706, loss: 1.781689167022705\n",
      "step 4707, loss: 1.706056833267212\n",
      "step 4708, loss: 1.6102687120437622\n",
      "step 4709, loss: 1.7022889852523804\n",
      "step 4710, loss: 1.5448499917984009\n",
      "step 4711, loss: 1.7155675888061523\n",
      "step 4712, loss: 1.8054275512695312\n",
      "step 4713, loss: 1.6760063171386719\n",
      "step 4714, loss: 1.6658788919448853\n",
      "step 4715, loss: 1.7002545595169067\n",
      "step 4716, loss: 1.6065359115600586\n",
      "step 4717, loss: 1.498228669166565\n",
      "step 4718, loss: 1.5396199226379395\n",
      "step 4719, loss: 1.4184573888778687\n",
      "step 4720, loss: 1.5857083797454834\n",
      "step 4721, loss: 1.4106090068817139\n",
      "step 4722, loss: 1.4806649684906006\n",
      "step 4723, loss: 1.6946181058883667\n",
      "step 4724, loss: 1.6497480869293213\n",
      "step 4725, loss: 1.2656078338623047\n",
      "step 4726, loss: 1.4208056926727295\n",
      "step 4727, loss: 1.5335766077041626\n",
      "step 4728, loss: 1.639362096786499\n",
      "step 4729, loss: 1.4793603420257568\n",
      "step 4730, loss: 1.6870371103286743\n",
      "step 4731, loss: 1.597398042678833\n",
      "step 4732, loss: 1.6571168899536133\n",
      "step 4733, loss: 1.9828888177871704\n",
      "step 4734, loss: 1.4423682689666748\n",
      "step 4735, loss: 1.2841122150421143\n",
      "step 4736, loss: 1.3534795045852661\n",
      "step 4737, loss: 1.6273622512817383\n",
      "step 4738, loss: 1.6554421186447144\n",
      "step 4739, loss: 1.714206337928772\n",
      "step 4740, loss: 1.3521265983581543\n",
      "step 4741, loss: 1.5410475730895996\n",
      "step 4742, loss: 1.7556952238082886\n",
      "step 4743, loss: 1.3214476108551025\n",
      "step 4744, loss: 1.2065056562423706\n",
      "step 4745, loss: 1.6286201477050781\n",
      "step 4746, loss: 1.5604274272918701\n",
      "step 4747, loss: 1.6565744876861572\n",
      "step 4748, loss: 1.5177420377731323\n",
      "step 4749, loss: 1.5518637895584106\n",
      "step 4750, loss: 1.4687020778656006\n",
      "step 4751, loss: 1.747247576713562\n",
      "step 4752, loss: 1.5478148460388184\n",
      "step 4753, loss: 1.7312475442886353\n",
      "step 4754, loss: 1.520673155784607\n",
      "step 4755, loss: 1.5405468940734863\n",
      "step 4756, loss: 1.344914436340332\n",
      "step 4757, loss: 1.4367029666900635\n",
      "step 4758, loss: 1.575707197189331\n",
      "step 4759, loss: 1.7227030992507935\n",
      "step 4760, loss: 1.6591821908950806\n",
      "step 4761, loss: 1.5832923650741577\n",
      "step 4762, loss: 1.4921786785125732\n",
      "step 4763, loss: 1.6776351928710938\n",
      "step 4764, loss: 1.693342924118042\n",
      "step 4765, loss: 1.8056445121765137\n",
      "step 4766, loss: 1.7898876667022705\n",
      "step 4767, loss: 1.611849069595337\n",
      "step 4768, loss: 1.582060694694519\n",
      "step 4769, loss: 1.7180118560791016\n",
      "step 4770, loss: 1.4122862815856934\n",
      "step 4771, loss: 1.5454607009887695\n",
      "step 4772, loss: 1.3663169145584106\n",
      "step 4773, loss: 1.5625431537628174\n",
      "step 4774, loss: 1.5755198001861572\n",
      "step 4775, loss: 1.5110063552856445\n",
      "step 4776, loss: 1.5652110576629639\n",
      "step 4777, loss: 1.4558236598968506\n",
      "step 4778, loss: 1.4978002309799194\n",
      "step 4779, loss: 1.586333155632019\n",
      "step 4780, loss: 1.7968591451644897\n",
      "step 4781, loss: 1.5526306629180908\n",
      "step 4782, loss: 1.6068003177642822\n",
      "step 4783, loss: 1.6929882764816284\n",
      "step 4784, loss: 1.5097707509994507\n",
      "step 4785, loss: 1.6233173608779907\n",
      "step 4786, loss: 1.2663143873214722\n",
      "step 4787, loss: 1.3430233001708984\n",
      "step 4788, loss: 0.958757758140564\n",
      "step 4789, loss: 0.9630850553512573\n",
      "step 4790, loss: 0.9332553744316101\n",
      "step 4791, loss: 0.9672701954841614\n",
      "step 4792, loss: 1.048987865447998\n",
      "step 4793, loss: 1.1222914457321167\n",
      "step 4794, loss: 0.9678354859352112\n",
      "step 4795, loss: 0.8819247484207153\n",
      "step 4796, loss: 0.9085586071014404\n",
      "step 4797, loss: 0.7965930104255676\n",
      "step 4798, loss: 0.8397315740585327\n",
      "step 4799, loss: 0.9288505911827087\n",
      "step 4800, loss: 0.8562878966331482\n",
      "step 4801, loss: 0.786998450756073\n",
      "step 4802, loss: 0.6811611652374268\n",
      "step 4803, loss: 0.5135351419448853\n",
      "step 4804, loss: 0.7678406238555908\n",
      "step 4805, loss: 0.7844814658164978\n",
      "step 4806, loss: 0.832516074180603\n",
      "step 4807, loss: 0.6347013115882874\n",
      "step 4808, loss: 0.8117067217826843\n",
      "step 4809, loss: 0.6979484558105469\n",
      "step 4810, loss: 1.2475091218948364\n",
      "step 4811, loss: 1.4669394493103027\n",
      "step 4812, loss: 1.1635267734527588\n",
      "step 4813, loss: 1.304283857345581\n",
      "step 4814, loss: 1.6372333765029907\n",
      "step 4815, loss: 1.384851336479187\n",
      "step 4816, loss: 1.3455679416656494\n",
      "step 4817, loss: 1.5571177005767822\n",
      "step 4818, loss: 1.4378653764724731\n",
      "step 4819, loss: 1.4073667526245117\n",
      "step 4820, loss: 1.5853650569915771\n",
      "step 4821, loss: 1.4206801652908325\n",
      "step 4822, loss: 1.4742804765701294\n",
      "step 4823, loss: 1.6716907024383545\n",
      "step 4824, loss: 1.617569923400879\n",
      "step 4825, loss: 1.6484062671661377\n",
      "step 4826, loss: 1.5783030986785889\n",
      "step 4827, loss: 1.8213750123977661\n",
      "step 4828, loss: 1.8876018524169922\n",
      "step 4829, loss: 1.9750162363052368\n",
      "step 4830, loss: 1.786743402481079\n",
      "step 4831, loss: 1.718297004699707\n",
      "step 4832, loss: 1.836963176727295\n",
      "step 4833, loss: 1.7124501466751099\n",
      "step 4834, loss: 1.5418477058410645\n",
      "step 4835, loss: 1.7387290000915527\n",
      "step 4836, loss: 1.5227384567260742\n",
      "step 4837, loss: 1.5882151126861572\n",
      "step 4838, loss: 1.831843614578247\n",
      "step 4839, loss: 1.72077476978302\n",
      "step 4840, loss: 1.5166594982147217\n",
      "step 4841, loss: 1.648859977722168\n",
      "step 4842, loss: 1.7274714708328247\n",
      "step 4843, loss: 1.6716169118881226\n",
      "step 4844, loss: 1.5703632831573486\n",
      "step 4845, loss: 1.7124369144439697\n",
      "step 4846, loss: 1.5304746627807617\n",
      "step 4847, loss: 1.6243667602539062\n",
      "step 4848, loss: 1.869604229927063\n",
      "step 4849, loss: 1.9094688892364502\n",
      "step 4850, loss: 1.8651752471923828\n",
      "step 4851, loss: 1.6389832496643066\n",
      "step 4852, loss: 1.7912250757217407\n",
      "step 4853, loss: 1.8779075145721436\n",
      "step 4854, loss: 1.8811676502227783\n",
      "step 4855, loss: 2.018561601638794\n",
      "step 4856, loss: 1.8936915397644043\n",
      "step 4857, loss: 1.9935133457183838\n",
      "step 4858, loss: 1.8618674278259277\n",
      "step 4859, loss: 1.674781084060669\n",
      "step 4860, loss: 1.5558748245239258\n",
      "step 4861, loss: 1.6754705905914307\n",
      "step 4862, loss: 1.5802215337753296\n",
      "step 4863, loss: 1.6602787971496582\n",
      "step 4864, loss: 1.475355863571167\n",
      "step 4865, loss: 1.3621680736541748\n",
      "step 4866, loss: 1.663646936416626\n",
      "step 4867, loss: 1.6473240852355957\n",
      "step 4868, loss: 1.7394694089889526\n",
      "step 4869, loss: 1.5734593868255615\n",
      "step 4870, loss: 1.5993846654891968\n",
      "step 4871, loss: 1.6917973756790161\n",
      "step 4872, loss: 1.80959153175354\n",
      "step 4873, loss: 1.7979724407196045\n",
      "step 4874, loss: 1.779284119606018\n",
      "step 4875, loss: 1.6974860429763794\n",
      "step 4876, loss: 1.8676687479019165\n",
      "step 4877, loss: 1.875095248222351\n",
      "step 4878, loss: 2.002443790435791\n",
      "step 4879, loss: 2.3147757053375244\n",
      "step 4880, loss: 2.0255537033081055\n",
      "step 4881, loss: 2.0327162742614746\n",
      "step 4882, loss: 2.118349552154541\n",
      "step 4883, loss: 1.9260776042938232\n",
      "step 4884, loss: 1.706978678703308\n",
      "step 4885, loss: 1.7532678842544556\n",
      "step 4886, loss: 1.7983338832855225\n",
      "step 4887, loss: 1.9722317457199097\n",
      "step 4888, loss: 1.5164636373519897\n",
      "step 4889, loss: 1.5710833072662354\n",
      "step 4890, loss: 1.7841976881027222\n",
      "step 4891, loss: 1.6379072666168213\n",
      "step 4892, loss: 1.8589762449264526\n",
      "step 4893, loss: 1.8327593803405762\n",
      "step 4894, loss: 1.917490005493164\n",
      "step 4895, loss: 1.9139745235443115\n",
      "step 4896, loss: 1.9900730848312378\n",
      "step 4897, loss: 1.7322971820831299\n",
      "step 4898, loss: 1.726296305656433\n",
      "step 4899, loss: 1.5174418687820435\n",
      "step 4900, loss: 0.9424899816513062\n",
      "step 4901, loss: 0.8409510850906372\n",
      "step 4902, loss: 1.0171159505844116\n",
      "step 4903, loss: 0.9012138247489929\n",
      "step 4904, loss: 0.8680290579795837\n",
      "step 4905, loss: 0.8972828984260559\n",
      "step 4906, loss: 0.8371566534042358\n",
      "step 4907, loss: 0.794563889503479\n",
      "step 4908, loss: 0.8013115525245667\n",
      "step 4909, loss: 0.8603163957595825\n",
      "step 4910, loss: 1.2269562482833862\n",
      "step 4911, loss: 0.7694022059440613\n",
      "step 4912, loss: 0.72811359167099\n",
      "step 4913, loss: 0.6835955381393433\n",
      "step 4914, loss: 0.5733882188796997\n",
      "step 4915, loss: 0.7304648160934448\n",
      "step 4916, loss: 0.7020704746246338\n",
      "step 4917, loss: 0.922012984752655\n",
      "step 4918, loss: 1.4346545934677124\n",
      "step 4919, loss: 1.6985291242599487\n",
      "step 4920, loss: 1.7378835678100586\n",
      "step 4921, loss: 1.5095525979995728\n",
      "step 4922, loss: 1.4721038341522217\n",
      "step 4923, loss: 1.622736930847168\n",
      "step 4924, loss: 1.491506814956665\n",
      "step 4925, loss: 1.4662253856658936\n",
      "step 4926, loss: 1.3306409120559692\n",
      "step 4927, loss: 1.5666581392288208\n",
      "step 4928, loss: 1.5024280548095703\n",
      "step 4929, loss: 1.6732261180877686\n",
      "step 4930, loss: 1.5211341381072998\n",
      "step 4931, loss: 1.5666348934173584\n",
      "step 4932, loss: 1.5898289680480957\n",
      "step 4933, loss: 1.5542227029800415\n",
      "step 4934, loss: 1.5817090272903442\n",
      "step 4935, loss: 1.4621992111206055\n",
      "step 4936, loss: 1.5663917064666748\n",
      "step 4937, loss: 1.7157319784164429\n",
      "step 4938, loss: 1.8523918390274048\n",
      "step 4939, loss: 1.7044923305511475\n",
      "step 4940, loss: 1.7811051607131958\n",
      "step 4941, loss: 1.7217177152633667\n",
      "step 4942, loss: 1.7049410343170166\n",
      "step 4943, loss: 1.7806400060653687\n",
      "step 4944, loss: 1.6885509490966797\n",
      "step 4945, loss: 1.7566018104553223\n",
      "step 4946, loss: 1.553767204284668\n",
      "step 4947, loss: 1.5879722833633423\n",
      "step 4948, loss: 1.713240146636963\n",
      "step 4949, loss: 1.816615104675293\n",
      "step 4950, loss: 1.5802559852600098\n",
      "step 4951, loss: 1.7541117668151855\n",
      "step 4952, loss: 1.7242504358291626\n",
      "step 4953, loss: 1.4448961019515991\n",
      "step 4954, loss: 1.5448404550552368\n",
      "step 4955, loss: 1.499874472618103\n",
      "step 4956, loss: 1.8662842512130737\n",
      "step 4957, loss: 1.8183660507202148\n",
      "step 4958, loss: 1.6614161729812622\n",
      "step 4959, loss: 1.5049268007278442\n",
      "step 4960, loss: 1.5362651348114014\n",
      "step 4961, loss: 1.4204617738723755\n",
      "step 4962, loss: 1.0811734199523926\n",
      "step 4963, loss: 0.7881541848182678\n",
      "step 4964, loss: 0.8533931970596313\n",
      "step 4965, loss: 0.766364336013794\n",
      "step 4966, loss: 0.7591727375984192\n",
      "step 4967, loss: 0.7345907092094421\n",
      "step 4968, loss: 0.8110021352767944\n",
      "step 4969, loss: 0.40912583470344543\n",
      "step 4970, loss: 0.6204354166984558\n",
      "step 4971, loss: 0.5807479023933411\n",
      "step 4972, loss: 0.857751727104187\n",
      "step 4973, loss: 0.6817869544029236\n",
      "step 4974, loss: 0.9052444696426392\n",
      "step 4975, loss: 0.9823419451713562\n",
      "step 4976, loss: 0.7588048577308655\n",
      "step 4977, loss: 0.836734414100647\n",
      "step 4978, loss: 1.4842325448989868\n",
      "step 4979, loss: 1.4972658157348633\n",
      "step 4980, loss: 1.2760816812515259\n",
      "step 4981, loss: 1.4935803413391113\n",
      "step 4982, loss: 1.55231511592865\n",
      "step 4983, loss: 1.3623977899551392\n",
      "step 4984, loss: 1.3777217864990234\n",
      "step 4985, loss: 1.6420150995254517\n",
      "step 4986, loss: 1.322535753250122\n",
      "step 4987, loss: 1.4840099811553955\n",
      "step 4988, loss: 1.2270445823669434\n",
      "step 4989, loss: 1.2821102142333984\n",
      "step 4990, loss: 1.4051063060760498\n",
      "step 4991, loss: 1.4157381057739258\n",
      "step 4992, loss: 1.547629952430725\n",
      "step 4993, loss: 1.6410566568374634\n",
      "step 4994, loss: 1.9726974964141846\n",
      "step 4995, loss: 1.4784411191940308\n",
      "step 4996, loss: 1.5446444749832153\n",
      "step 4997, loss: 1.42121160030365\n",
      "step 4998, loss: 1.6523642539978027\n",
      "step 4999, loss: 1.6193280220031738\n",
      "step 5000, loss: 1.7256050109863281\n",
      "step 5001, loss: 1.6157546043395996\n",
      "step 5002, loss: 1.4859586954116821\n",
      "step 5003, loss: 1.7612957954406738\n",
      "step 5004, loss: 1.8992856740951538\n",
      "step 5005, loss: 1.851988673210144\n",
      "step 5006, loss: 1.7090914249420166\n",
      "step 5007, loss: 1.7963175773620605\n",
      "step 5008, loss: 1.7243504524230957\n",
      "step 5009, loss: 1.7107067108154297\n",
      "step 5010, loss: 1.8316212892532349\n",
      "step 5011, loss: 1.6981170177459717\n",
      "step 5012, loss: 1.798629641532898\n",
      "step 5013, loss: 1.6442099809646606\n",
      "step 5014, loss: 1.6584240198135376\n",
      "step 5015, loss: 1.983750343322754\n",
      "step 5016, loss: 1.997987151145935\n",
      "step 5017, loss: 1.9354387521743774\n",
      "step 5018, loss: 1.9305496215820312\n",
      "step 5019, loss: 1.7150530815124512\n",
      "step 5020, loss: 1.5872836112976074\n",
      "step 5021, loss: 1.865176796913147\n",
      "step 5022, loss: 1.7598026990890503\n",
      "step 5023, loss: 1.6364004611968994\n",
      "step 5024, loss: 1.764725923538208\n",
      "step 5025, loss: 1.71298348903656\n",
      "step 5026, loss: 1.7298048734664917\n",
      "step 5027, loss: 1.8717217445373535\n",
      "step 5028, loss: 1.4192297458648682\n",
      "step 5029, loss: 1.8378218412399292\n",
      "step 5030, loss: 1.5546371936798096\n",
      "step 5031, loss: 1.6502362489700317\n",
      "step 5032, loss: 1.596862554550171\n",
      "step 5033, loss: 1.4752341508865356\n",
      "step 5034, loss: 1.660990595817566\n",
      "step 5035, loss: 1.6485939025878906\n",
      "step 5036, loss: 1.5796103477478027\n",
      "step 5037, loss: 1.7313778400421143\n",
      "step 5038, loss: 1.778180718421936\n",
      "step 5039, loss: 1.7319034337997437\n",
      "step 5040, loss: 1.8819224834442139\n",
      "step 5041, loss: 1.5068128108978271\n",
      "step 5042, loss: 1.7986963987350464\n",
      "step 5043, loss: 1.7746881246566772\n",
      "step 5044, loss: 1.8163279294967651\n",
      "step 5045, loss: 1.6114153861999512\n",
      "step 5046, loss: 1.6634345054626465\n",
      "step 5047, loss: 1.7582026720046997\n",
      "step 5048, loss: 1.5284065008163452\n",
      "step 5049, loss: 1.7845308780670166\n",
      "step 5050, loss: 1.9087756872177124\n",
      "step 5051, loss: 1.679360270500183\n",
      "step 5052, loss: 1.8343206644058228\n",
      "step 5053, loss: 1.6699827909469604\n",
      "step 5054, loss: 1.5255464315414429\n",
      "step 5055, loss: 1.598433256149292\n",
      "step 5056, loss: 1.6393615007400513\n",
      "step 5057, loss: 1.5904704332351685\n",
      "step 5058, loss: 1.7482558488845825\n",
      "step 5059, loss: 1.296436071395874\n",
      "step 5060, loss: 1.5905249118804932\n",
      "step 5061, loss: 1.5152621269226074\n",
      "step 5062, loss: 1.4401392936706543\n",
      "step 5063, loss: 1.3629487752914429\n",
      "step 5064, loss: 1.4536064863204956\n",
      "step 5065, loss: 1.5425304174423218\n",
      "step 5066, loss: 1.458557367324829\n",
      "step 5067, loss: 0.9675925970077515\n",
      "step 5068, loss: 0.8358308672904968\n",
      "step 5069, loss: 0.7529157400131226\n",
      "step 5070, loss: 0.8377991914749146\n",
      "step 5071, loss: 0.6478466987609863\n",
      "step 5072, loss: 0.557693362236023\n",
      "step 5073, loss: 0.645465075969696\n",
      "step 5074, loss: 0.673137366771698\n",
      "step 5075, loss: 0.7200241684913635\n",
      "step 5076, loss: 0.723473072052002\n",
      "step 5077, loss: 0.6578273773193359\n",
      "step 5078, loss: 0.7048206925392151\n",
      "step 5079, loss: 0.865068793296814\n",
      "step 5080, loss: 0.7769067883491516\n",
      "step 5081, loss: 0.7736618518829346\n",
      "step 5082, loss: 0.7461647987365723\n",
      "step 5083, loss: 0.7619280815124512\n",
      "step 5084, loss: 0.5925394892692566\n",
      "step 5085, loss: 0.7020755410194397\n",
      "step 5086, loss: 0.7546286582946777\n",
      "step 5087, loss: 0.5500826239585876\n",
      "step 5088, loss: 0.9558790922164917\n",
      "step 5089, loss: 1.4925596714019775\n",
      "step 5090, loss: 1.570816159248352\n",
      "step 5091, loss: 1.5607833862304688\n",
      "step 5092, loss: 1.2402350902557373\n",
      "step 5093, loss: 1.391456127166748\n",
      "step 5094, loss: 1.6123614311218262\n",
      "step 5095, loss: 1.9985696077346802\n",
      "step 5096, loss: 1.3595741987228394\n",
      "step 5097, loss: 1.55604887008667\n",
      "step 5098, loss: 1.370169997215271\n",
      "step 5099, loss: 1.4151233434677124\n",
      "step 5100, loss: 1.5113083124160767\n",
      "step 5101, loss: 1.3839375972747803\n",
      "step 5102, loss: 1.607286810874939\n",
      "step 5103, loss: 1.2416325807571411\n",
      "step 5104, loss: 1.3170522451400757\n",
      "step 5105, loss: 1.5164618492126465\n",
      "step 5106, loss: 1.377004861831665\n",
      "step 5107, loss: 1.4286211729049683\n",
      "step 5108, loss: 1.5254982709884644\n",
      "step 5109, loss: 1.4627413749694824\n",
      "step 5110, loss: 1.6267441511154175\n",
      "step 5111, loss: 1.386040210723877\n",
      "step 5112, loss: 1.613491177558899\n",
      "step 5113, loss: 1.5003714561462402\n",
      "step 5114, loss: 1.5575435161590576\n",
      "step 5115, loss: 1.4782449007034302\n",
      "step 5116, loss: 1.5096664428710938\n",
      "step 5117, loss: 1.5638916492462158\n",
      "step 5118, loss: 1.5037086009979248\n",
      "step 5119, loss: 1.584485650062561\n",
      "step 5120, loss: 1.5951168537139893\n",
      "step 5121, loss: 1.5267902612686157\n",
      "step 5122, loss: 1.5452698469161987\n",
      "step 5123, loss: 1.3972657918930054\n",
      "step 5124, loss: 1.5421206951141357\n",
      "step 5125, loss: 1.6553658246994019\n",
      "step 5126, loss: 1.678682804107666\n",
      "step 5127, loss: 1.6318607330322266\n",
      "step 5128, loss: 1.250516653060913\n",
      "step 5129, loss: 1.5397443771362305\n",
      "step 5130, loss: 1.4567571878433228\n",
      "step 5131, loss: 1.4469009637832642\n",
      "step 5132, loss: 1.3721206188201904\n",
      "step 5133, loss: 1.735052466392517\n",
      "step 5134, loss: 1.6401177644729614\n",
      "step 5135, loss: 1.6576578617095947\n",
      "step 5136, loss: 1.4888875484466553\n",
      "step 5137, loss: 1.4438385963439941\n",
      "step 5138, loss: 1.6108773946762085\n",
      "step 5139, loss: 1.3946678638458252\n",
      "step 5140, loss: 1.6586713790893555\n",
      "step 5141, loss: 1.4294604063034058\n",
      "step 5142, loss: 2.2178988456726074\n",
      "step 5143, loss: 1.673397183418274\n",
      "step 5144, loss: 1.515677809715271\n",
      "step 5145, loss: 1.776930332183838\n",
      "step 5146, loss: 1.4030067920684814\n",
      "step 5147, loss: 1.4449152946472168\n",
      "step 5148, loss: 1.5098859071731567\n",
      "step 5149, loss: 1.4421221017837524\n",
      "step 5150, loss: 1.4563342332839966\n",
      "step 5151, loss: 1.5618786811828613\n",
      "step 5152, loss: 1.5567487478256226\n",
      "step 5153, loss: 1.6102228164672852\n",
      "step 5154, loss: 1.6520243883132935\n",
      "step 5155, loss: 1.3788022994995117\n",
      "step 5156, loss: 1.662821650505066\n",
      "step 5157, loss: 1.6310315132141113\n",
      "step 5158, loss: 1.2451248168945312\n",
      "step 5159, loss: 1.4027721881866455\n",
      "step 5160, loss: 1.619099497795105\n",
      "step 5161, loss: 1.5133423805236816\n",
      "step 5162, loss: 1.6935957670211792\n",
      "step 5163, loss: 1.4101624488830566\n",
      "step 5164, loss: 1.5470541715621948\n",
      "step 5165, loss: 1.568641185760498\n",
      "step 5166, loss: 1.5806822776794434\n",
      "step 5167, loss: 1.6074227094650269\n",
      "step 5168, loss: 1.5765682458877563\n",
      "step 5169, loss: 1.6781026124954224\n",
      "step 5170, loss: 1.7076538801193237\n",
      "step 5171, loss: 1.5653941631317139\n",
      "step 5172, loss: 1.3978155851364136\n",
      "step 5173, loss: 1.5993878841400146\n",
      "step 5174, loss: 1.4120118618011475\n",
      "step 5175, loss: 1.436312198638916\n",
      "step 5176, loss: 1.4042874574661255\n",
      "step 5177, loss: 1.7026652097702026\n",
      "step 5178, loss: 1.482099175453186\n",
      "step 5179, loss: 1.3888436555862427\n",
      "step 5180, loss: 1.3457725048065186\n",
      "step 5181, loss: 1.5485347509384155\n",
      "step 5182, loss: 1.4892162084579468\n",
      "step 5183, loss: 1.515655279159546\n",
      "step 5184, loss: 1.687580943107605\n",
      "step 5185, loss: 1.5909557342529297\n",
      "step 5186, loss: 1.4074225425720215\n",
      "step 5187, loss: 1.6479723453521729\n",
      "step 5188, loss: 1.4977633953094482\n",
      "step 5189, loss: 1.4747300148010254\n",
      "step 5190, loss: 1.3510843515396118\n",
      "step 5191, loss: 1.5253243446350098\n",
      "step 5192, loss: 1.4118176698684692\n",
      "step 5193, loss: 1.3270753622055054\n",
      "step 5194, loss: 1.499760389328003\n",
      "step 5195, loss: 1.529417872428894\n",
      "step 5196, loss: 1.4604753255844116\n",
      "step 5197, loss: 1.38821280002594\n",
      "step 5198, loss: 1.2442141771316528\n",
      "step 5199, loss: 0.8469924926757812\n",
      "step 5200, loss: 0.7251562476158142\n",
      "step 5201, loss: 0.7150477170944214\n",
      "step 5202, loss: 0.5773147940635681\n",
      "step 5203, loss: 0.8996599912643433\n",
      "step 5204, loss: 0.721070408821106\n",
      "step 5205, loss: 0.7689370512962341\n",
      "step 5206, loss: 0.8401448130607605\n",
      "step 5207, loss: 0.621364176273346\n",
      "step 5208, loss: 0.7430185675621033\n",
      "step 5209, loss: 0.6229677796363831\n",
      "step 5210, loss: 0.7334041595458984\n",
      "step 5211, loss: 0.7385748624801636\n",
      "step 5212, loss: 0.8056178689002991\n",
      "step 5213, loss: 0.537493109703064\n",
      "step 5214, loss: 0.6070294976234436\n",
      "step 5215, loss: 0.4749836325645447\n",
      "step 5216, loss: 0.5322631001472473\n",
      "step 5217, loss: 0.6275376081466675\n",
      "step 5218, loss: 0.6166590452194214\n",
      "step 5219, loss: 0.7610443830490112\n",
      "step 5220, loss: 0.619590699672699\n",
      "step 5221, loss: 0.6270607113838196\n",
      "step 5222, loss: 0.5903199315071106\n",
      "step 5223, loss: 0.6516236662864685\n",
      "step 5224, loss: 0.6812637448310852\n",
      "step 5225, loss: 0.6109723448753357\n",
      "step 5226, loss: 0.6928833723068237\n",
      "step 5227, loss: 0.5606144666671753\n",
      "step 5228, loss: 1.2540441751480103\n",
      "step 5229, loss: 1.264211654663086\n",
      "step 5230, loss: 1.3337856531143188\n",
      "step 5231, loss: 1.1920907497406006\n",
      "step 5232, loss: 1.285223364830017\n",
      "step 5233, loss: 1.5534881353378296\n",
      "step 5234, loss: 1.5249898433685303\n",
      "step 5235, loss: 1.6878188848495483\n",
      "step 5236, loss: 1.4714343547821045\n",
      "step 5237, loss: 1.6531875133514404\n",
      "step 5238, loss: 1.6272485256195068\n",
      "step 5239, loss: 1.431673288345337\n",
      "step 5240, loss: 1.6215139627456665\n",
      "step 5241, loss: 1.4760403633117676\n",
      "step 5242, loss: 1.7508265972137451\n",
      "step 5243, loss: 1.5948753356933594\n",
      "step 5244, loss: 1.5411635637283325\n",
      "step 5245, loss: 1.2945830821990967\n",
      "step 5246, loss: 1.3071810007095337\n",
      "step 5247, loss: 1.2531349658966064\n",
      "step 5248, loss: 1.4267425537109375\n",
      "step 5249, loss: 1.5436068773269653\n",
      "step 5250, loss: 1.5724881887435913\n",
      "step 5251, loss: 1.5616661310195923\n",
      "step 5252, loss: 1.4541500806808472\n",
      "step 5253, loss: 1.4144631624221802\n",
      "step 5254, loss: 1.5352108478546143\n",
      "step 5255, loss: 1.6762712001800537\n",
      "step 5256, loss: 1.4666213989257812\n",
      "step 5257, loss: 1.3832783699035645\n",
      "step 5258, loss: 1.7095109224319458\n",
      "step 5259, loss: 1.5488070249557495\n",
      "step 5260, loss: 1.5583776235580444\n",
      "step 5261, loss: 1.7743784189224243\n",
      "step 5262, loss: 1.694862723350525\n",
      "step 5263, loss: 1.7486201524734497\n",
      "step 5264, loss: 1.6311737298965454\n",
      "step 5265, loss: 1.6600291728973389\n",
      "step 5266, loss: 1.672766923904419\n",
      "step 5267, loss: 1.7111854553222656\n",
      "step 5268, loss: 1.8226381540298462\n",
      "step 5269, loss: 1.7845873832702637\n",
      "step 5270, loss: 1.8648409843444824\n",
      "step 5271, loss: 1.6527544260025024\n",
      "step 5272, loss: 1.5413998365402222\n",
      "step 5273, loss: 1.573122262954712\n",
      "step 5274, loss: 1.5668977499008179\n",
      "step 5275, loss: 1.5752830505371094\n",
      "step 5276, loss: 1.6732348203659058\n",
      "step 5277, loss: 1.6144134998321533\n",
      "step 5278, loss: 1.6355704069137573\n",
      "step 5279, loss: 1.7106775045394897\n",
      "step 5280, loss: 1.536968469619751\n",
      "step 5281, loss: 1.4209891557693481\n",
      "step 5282, loss: 1.5799968242645264\n",
      "step 5283, loss: 1.4039344787597656\n",
      "step 5284, loss: 1.2267545461654663\n",
      "step 5285, loss: 1.3510199785232544\n",
      "step 5286, loss: 1.5668609142303467\n",
      "step 5287, loss: 1.543097734451294\n",
      "step 5288, loss: 1.6579701900482178\n",
      "step 5289, loss: 1.5935370922088623\n",
      "step 5290, loss: 1.49376380443573\n",
      "step 5291, loss: 1.5938698053359985\n",
      "step 5292, loss: 1.4275078773498535\n",
      "step 5293, loss: 1.5512468814849854\n",
      "step 5294, loss: 1.5830804109573364\n",
      "step 5295, loss: 1.2940170764923096\n",
      "step 5296, loss: 1.365921974182129\n",
      "step 5297, loss: 1.3220657110214233\n",
      "step 5298, loss: 1.5426511764526367\n",
      "step 5299, loss: 1.430069923400879\n",
      "step 5300, loss: 1.5392746925354004\n",
      "step 5301, loss: 1.484041690826416\n",
      "step 5302, loss: 1.5338375568389893\n",
      "step 5303, loss: 1.5388710498809814\n",
      "step 5304, loss: 1.4865220785140991\n",
      "step 5305, loss: 1.4125868082046509\n",
      "step 5306, loss: 1.4496309757232666\n",
      "step 5307, loss: 1.6078095436096191\n",
      "step 5308, loss: 1.5492899417877197\n",
      "step 5309, loss: 1.6817681789398193\n",
      "step 5310, loss: 1.5488094091415405\n",
      "step 5311, loss: 1.4639359712600708\n",
      "step 5312, loss: 1.6247965097427368\n",
      "step 5313, loss: 1.6564912796020508\n",
      "step 5314, loss: 1.6635757684707642\n",
      "step 5315, loss: 1.854175329208374\n",
      "step 5316, loss: 1.615991473197937\n",
      "step 5317, loss: 1.529778003692627\n",
      "step 5318, loss: 1.621839165687561\n",
      "step 5319, loss: 1.7734160423278809\n",
      "step 5320, loss: 1.509919285774231\n",
      "step 5321, loss: 1.5567421913146973\n",
      "step 5322, loss: 1.5573396682739258\n",
      "step 5323, loss: 1.615190029144287\n",
      "step 5324, loss: 1.610458493232727\n",
      "step 5325, loss: 1.7162842750549316\n",
      "step 5326, loss: 1.8336251974105835\n",
      "step 5327, loss: 1.5815260410308838\n",
      "step 5328, loss: 1.7477833032608032\n",
      "step 5329, loss: 1.7290838956832886\n",
      "step 5330, loss: 1.6808290481567383\n",
      "step 5331, loss: 1.326460599899292\n",
      "step 5332, loss: 1.7292494773864746\n",
      "step 5333, loss: 1.6077656745910645\n",
      "step 5334, loss: 1.5179990530014038\n",
      "step 5335, loss: 1.610314965248108\n",
      "step 5336, loss: 1.5016157627105713\n",
      "step 5337, loss: 1.669938325881958\n",
      "step 5338, loss: 1.833114504814148\n",
      "step 5339, loss: 1.7126431465148926\n",
      "step 5340, loss: 1.3716269731521606\n",
      "step 5341, loss: 1.5676616430282593\n",
      "step 5342, loss: 1.744140625\n",
      "step 5343, loss: 1.876268982887268\n",
      "step 5344, loss: 1.4928861856460571\n",
      "step 5345, loss: 1.6943418979644775\n",
      "step 5346, loss: 1.673993706703186\n",
      "step 5347, loss: 1.6767020225524902\n",
      "step 5348, loss: 1.4779284000396729\n",
      "step 5349, loss: 1.6920270919799805\n",
      "step 5350, loss: 1.6914151906967163\n",
      "step 5351, loss: 1.8121851682662964\n",
      "step 5352, loss: 1.700163722038269\n",
      "step 5353, loss: 1.5937933921813965\n",
      "step 5354, loss: 1.6317627429962158\n",
      "step 5355, loss: 1.8215153217315674\n",
      "step 5356, loss: 1.4811270236968994\n",
      "step 5357, loss: 1.8677802085876465\n",
      "step 5358, loss: 1.8580862283706665\n",
      "step 5359, loss: 1.3715497255325317\n",
      "step 5360, loss: 1.6411933898925781\n",
      "step 5361, loss: 1.520986795425415\n",
      "step 5362, loss: 1.8552132844924927\n",
      "step 5363, loss: 1.5630524158477783\n",
      "step 5364, loss: 1.8183175325393677\n",
      "step 5365, loss: 1.6600602865219116\n",
      "step 5366, loss: 1.652215838432312\n",
      "step 5367, loss: 1.6342220306396484\n",
      "step 5368, loss: 1.738828420639038\n",
      "step 5369, loss: 1.7896575927734375\n",
      "step 5370, loss: 1.5296220779418945\n",
      "step 5371, loss: 1.843956470489502\n",
      "step 5372, loss: 1.7608330249786377\n",
      "step 5373, loss: 1.5272657871246338\n",
      "step 5374, loss: 1.634591817855835\n",
      "step 5375, loss: 1.5274821519851685\n",
      "step 5376, loss: 1.591677188873291\n",
      "step 5377, loss: 1.821211814880371\n",
      "step 5378, loss: 1.7020143270492554\n",
      "step 5379, loss: 1.7694478034973145\n",
      "step 5380, loss: 1.8895595073699951\n",
      "step 5381, loss: 1.5154616832733154\n",
      "step 5382, loss: 1.7749614715576172\n",
      "step 5383, loss: 1.4670737981796265\n",
      "step 5384, loss: 1.64274263381958\n",
      "step 5385, loss: 1.6411278247833252\n",
      "step 5386, loss: 1.4966728687286377\n",
      "step 5387, loss: 1.1331795454025269\n",
      "step 5388, loss: 1.3601948022842407\n",
      "step 5389, loss: 1.6033371686935425\n",
      "step 5390, loss: 1.5964385271072388\n",
      "step 5391, loss: 1.3136831521987915\n",
      "step 5392, loss: 1.6007623672485352\n",
      "step 5393, loss: 1.4346587657928467\n",
      "step 5394, loss: 1.4645476341247559\n",
      "step 5395, loss: 1.6754505634307861\n",
      "step 5396, loss: 1.5045684576034546\n",
      "step 5397, loss: 1.5137343406677246\n",
      "step 5398, loss: 1.3846814632415771\n",
      "step 5399, loss: 1.3279386758804321\n",
      "step 5400, loss: 1.3611268997192383\n",
      "step 5401, loss: 1.4820383787155151\n",
      "step 5402, loss: 1.4342796802520752\n",
      "step 5403, loss: 1.3538272380828857\n",
      "step 5404, loss: 1.754652500152588\n",
      "step 5405, loss: 1.487439513206482\n",
      "step 5406, loss: 1.3822119235992432\n",
      "step 5407, loss: 1.407638669013977\n",
      "step 5408, loss: 1.5305861234664917\n",
      "step 5409, loss: 1.435546875\n",
      "step 5410, loss: 1.4690977334976196\n",
      "step 5411, loss: 1.1522974967956543\n",
      "step 5412, loss: 0.610523521900177\n",
      "step 5413, loss: 0.8072139620780945\n",
      "step 5414, loss: 0.8880768418312073\n",
      "step 5415, loss: 0.7084862589836121\n",
      "step 5416, loss: 0.6858006715774536\n",
      "step 5417, loss: 0.5608130097389221\n",
      "step 5418, loss: 0.8156080842018127\n",
      "step 5419, loss: 0.6900021433830261\n",
      "step 5420, loss: 0.8048901557922363\n",
      "step 5421, loss: 0.722805917263031\n",
      "step 5422, loss: 0.6291909217834473\n",
      "step 5423, loss: 0.5617638230323792\n",
      "step 5424, loss: 0.6582782864570618\n",
      "step 5425, loss: 0.542777955532074\n",
      "step 5426, loss: 0.7098943591117859\n",
      "step 5427, loss: 0.6984562873840332\n",
      "step 5428, loss: 0.5652218461036682\n",
      "step 5429, loss: 0.5704160928726196\n",
      "step 5430, loss: 0.7474644184112549\n",
      "step 5431, loss: 0.6529402732849121\n",
      "step 5432, loss: 0.5155102014541626\n",
      "step 5433, loss: 0.477508544921875\n",
      "step 5434, loss: 0.7138749361038208\n",
      "step 5435, loss: 0.5872514247894287\n",
      "step 5436, loss: 0.6497163772583008\n",
      "step 5437, loss: 0.41437748074531555\n",
      "step 5438, loss: 0.5724568963050842\n",
      "step 5439, loss: 0.8059816956520081\n",
      "step 5440, loss: 0.5860285758972168\n",
      "step 5441, loss: 0.6383382678031921\n",
      "step 5442, loss: 0.6187855005264282\n",
      "step 5443, loss: 1.5270029306411743\n",
      "step 5444, loss: 1.4714992046356201\n",
      "step 5445, loss: 1.4270316362380981\n",
      "step 5446, loss: 1.3210712671279907\n",
      "step 5447, loss: 1.5848960876464844\n",
      "step 5448, loss: 1.3907890319824219\n",
      "step 5449, loss: 1.4240347146987915\n",
      "step 5450, loss: 1.4647369384765625\n",
      "step 5451, loss: 1.398026943206787\n",
      "step 5452, loss: 1.4285664558410645\n",
      "step 5453, loss: 1.3486812114715576\n",
      "step 5454, loss: 1.4075398445129395\n",
      "step 5455, loss: 1.3905922174453735\n",
      "step 5456, loss: 1.4428800344467163\n",
      "step 5457, loss: 1.4384666681289673\n",
      "step 5458, loss: 1.143212080001831\n",
      "step 5459, loss: 1.3020949363708496\n",
      "step 5460, loss: 1.2796604633331299\n",
      "step 5461, loss: 1.5161771774291992\n",
      "step 5462, loss: 1.3668394088745117\n",
      "step 5463, loss: 1.3723087310791016\n",
      "step 5464, loss: 1.348764181137085\n",
      "step 5465, loss: 1.3709672689437866\n",
      "step 5466, loss: 1.433147668838501\n",
      "step 5467, loss: 1.3041094541549683\n",
      "step 5468, loss: 1.2402710914611816\n",
      "step 5469, loss: 1.165021300315857\n",
      "step 5470, loss: 1.3067412376403809\n",
      "step 5471, loss: 1.1434831619262695\n",
      "step 5472, loss: 1.223090648651123\n",
      "step 5473, loss: 1.4702523946762085\n",
      "step 5474, loss: 1.3866872787475586\n",
      "step 5475, loss: 1.3352447748184204\n",
      "step 5476, loss: 1.3254905939102173\n",
      "step 5477, loss: 1.4795554876327515\n",
      "step 5478, loss: 1.3011490106582642\n",
      "step 5479, loss: 1.0158385038375854\n",
      "step 5480, loss: 0.979560136795044\n",
      "step 5481, loss: 0.9657130241394043\n",
      "step 5482, loss: 0.8576784133911133\n",
      "step 5483, loss: 0.8229097127914429\n",
      "step 5484, loss: 0.8637733459472656\n",
      "step 5485, loss: 0.8314349055290222\n",
      "step 5486, loss: 0.7595905065536499\n",
      "step 5487, loss: 0.7224288582801819\n",
      "step 5488, loss: 0.8126653432846069\n",
      "step 5489, loss: 0.7096418142318726\n",
      "step 5490, loss: 0.8756822943687439\n",
      "step 5491, loss: 0.753942608833313\n",
      "step 5492, loss: 0.8734276294708252\n",
      "step 5493, loss: 0.7062517404556274\n",
      "step 5494, loss: 0.6387122869491577\n",
      "step 5495, loss: 0.42282456159591675\n",
      "step 5496, loss: 1.325582504272461\n",
      "step 5497, loss: 1.448058009147644\n",
      "step 5498, loss: 1.4539048671722412\n",
      "step 5499, loss: 1.6413222551345825\n",
      "step 5500, loss: 1.4411706924438477\n",
      "step 5501, loss: 1.5490158796310425\n",
      "step 5502, loss: 1.4045820236206055\n",
      "step 5503, loss: 1.6821258068084717\n",
      "step 5504, loss: 1.4012295007705688\n",
      "step 5505, loss: 1.693074107170105\n",
      "step 5506, loss: 1.4765055179595947\n",
      "step 5507, loss: 1.6919511556625366\n",
      "step 5508, loss: 1.5915789604187012\n",
      "step 5509, loss: 1.5386388301849365\n",
      "step 5510, loss: 1.7116118669509888\n",
      "step 5511, loss: 1.7828993797302246\n",
      "step 5512, loss: 1.6860578060150146\n",
      "step 5513, loss: 1.5555413961410522\n",
      "step 5514, loss: 1.6746755838394165\n",
      "step 5515, loss: 1.5205743312835693\n",
      "step 5516, loss: 1.5848551988601685\n",
      "step 5517, loss: 1.5329580307006836\n",
      "step 5518, loss: 1.376907229423523\n",
      "step 5519, loss: 1.6645697355270386\n",
      "step 5520, loss: 1.5731747150421143\n",
      "step 5521, loss: 1.5342750549316406\n",
      "step 5522, loss: 1.4296287298202515\n",
      "step 5523, loss: 1.4587724208831787\n",
      "step 5524, loss: 1.502566933631897\n",
      "step 5525, loss: 1.2804949283599854\n",
      "step 5526, loss: 1.4868433475494385\n",
      "step 5527, loss: 1.6980788707733154\n",
      "step 5528, loss: 1.3238804340362549\n",
      "step 5529, loss: 1.3949445486068726\n",
      "step 5530, loss: 1.3521909713745117\n",
      "step 5531, loss: 1.5970103740692139\n",
      "step 5532, loss: 1.416434407234192\n",
      "step 5533, loss: 1.4870994091033936\n",
      "step 5534, loss: 1.411149024963379\n",
      "step 5535, loss: 1.2721751928329468\n",
      "step 5536, loss: 1.276206612586975\n",
      "step 5537, loss: 1.4353220462799072\n",
      "step 5538, loss: 1.6344517469406128\n",
      "step 5539, loss: 1.4186012744903564\n",
      "step 5540, loss: 1.3586242198944092\n",
      "step 5541, loss: 1.3351988792419434\n",
      "step 5542, loss: 1.3506406545639038\n",
      "step 5543, loss: 1.3969063758850098\n",
      "step 5544, loss: 1.4199485778808594\n",
      "step 5545, loss: 1.290345311164856\n",
      "step 5546, loss: 1.3182995319366455\n",
      "step 5547, loss: 1.6103768348693848\n",
      "step 5548, loss: 1.427184820175171\n",
      "step 5549, loss: 1.3800864219665527\n",
      "step 5550, loss: 1.5974334478378296\n",
      "step 5551, loss: 1.5630316734313965\n",
      "step 5552, loss: 1.474994421005249\n",
      "step 5553, loss: 1.379784345626831\n",
      "step 5554, loss: 1.6057331562042236\n",
      "step 5555, loss: 1.5847818851470947\n",
      "step 5556, loss: 1.5611120462417603\n",
      "step 5557, loss: 1.4967694282531738\n",
      "step 5558, loss: 1.6122599840164185\n",
      "step 5559, loss: 1.6494174003601074\n",
      "step 5560, loss: 1.5446559190750122\n",
      "step 5561, loss: 1.4057244062423706\n",
      "step 5562, loss: 1.6650770902633667\n",
      "step 5563, loss: 1.5927928686141968\n",
      "step 5564, loss: 1.6190581321716309\n",
      "step 5565, loss: 1.6135690212249756\n",
      "step 5566, loss: 1.6907228231430054\n",
      "step 5567, loss: 1.4891648292541504\n",
      "step 5568, loss: 1.6318601369857788\n",
      "step 5569, loss: 1.6851979494094849\n",
      "step 5570, loss: 1.5488637685775757\n",
      "step 5571, loss: 1.6854369640350342\n",
      "step 5572, loss: 1.434179663658142\n",
      "step 5573, loss: 1.5143513679504395\n",
      "step 5574, loss: 1.630996823310852\n",
      "step 5575, loss: 1.4315563440322876\n",
      "step 5576, loss: 1.413802146911621\n",
      "step 5577, loss: 1.4476287364959717\n",
      "step 5578, loss: 1.494040846824646\n",
      "step 5579, loss: 1.4597586393356323\n",
      "step 5580, loss: 1.5380761623382568\n",
      "step 5581, loss: 1.2646827697753906\n",
      "step 5582, loss: 1.4556851387023926\n",
      "step 5583, loss: 1.371287226676941\n",
      "step 5584, loss: 1.108002781867981\n",
      "step 5585, loss: 1.2309906482696533\n",
      "step 5586, loss: 0.8331125378608704\n",
      "step 5587, loss: 0.8146804571151733\n",
      "step 5588, loss: 0.9035661816596985\n",
      "step 5589, loss: 0.9404765963554382\n",
      "step 5590, loss: 0.8773516416549683\n",
      "step 5591, loss: 0.6822628378868103\n",
      "step 5592, loss: 0.6628907918930054\n",
      "step 5593, loss: 0.6926808953285217\n",
      "step 5594, loss: 0.5629475116729736\n",
      "step 5595, loss: 0.7185994982719421\n",
      "step 5596, loss: 0.768441915512085\n",
      "step 5597, loss: 0.6922217011451721\n",
      "step 5598, loss: 0.7416628003120422\n",
      "step 5599, loss: 0.7442336678504944\n",
      "step 5600, loss: 1.0334651470184326\n",
      "step 5601, loss: 1.3403488397598267\n",
      "step 5602, loss: 1.382051706314087\n",
      "step 5603, loss: 1.2577074766159058\n",
      "step 5604, loss: 1.0817391872406006\n",
      "step 5605, loss: 1.1945277452468872\n",
      "step 5606, loss: 1.2161946296691895\n",
      "step 5607, loss: 1.1518325805664062\n",
      "step 5608, loss: 1.3079851865768433\n",
      "step 5609, loss: 1.2664216756820679\n",
      "step 5610, loss: 1.343396544456482\n",
      "step 5611, loss: 1.3422164916992188\n",
      "step 5612, loss: 1.4056159257888794\n",
      "step 5613, loss: 1.2758463621139526\n",
      "step 5614, loss: 1.327147126197815\n",
      "step 5615, loss: 1.4748841524124146\n",
      "step 5616, loss: 1.1544984579086304\n",
      "step 5617, loss: 1.295424222946167\n",
      "step 5618, loss: 1.417370080947876\n",
      "step 5619, loss: 1.2761324644088745\n",
      "step 5620, loss: 1.2198237180709839\n",
      "step 5621, loss: 1.4726120233535767\n",
      "step 5622, loss: 1.5457109212875366\n",
      "step 5623, loss: 1.280476689338684\n",
      "step 5624, loss: 1.3931643962860107\n",
      "step 5625, loss: 1.4383612871170044\n",
      "step 5626, loss: 1.1855629682540894\n",
      "step 5627, loss: 1.1003358364105225\n",
      "step 5628, loss: 1.3662177324295044\n",
      "step 5629, loss: 1.2930022478103638\n",
      "step 5630, loss: 1.2739357948303223\n",
      "step 5631, loss: 1.2282687425613403\n",
      "step 5632, loss: 1.4158016443252563\n",
      "step 5633, loss: 1.2359669208526611\n",
      "step 5634, loss: 1.4186161756515503\n",
      "step 5635, loss: 1.2318854331970215\n",
      "step 5636, loss: 1.45102059841156\n",
      "step 5637, loss: 1.1703351736068726\n",
      "step 5638, loss: 1.330083966255188\n",
      "step 5639, loss: 1.4259028434753418\n",
      "step 5640, loss: 1.1165688037872314\n",
      "step 5641, loss: 1.1701127290725708\n",
      "step 5642, loss: 1.172304391860962\n",
      "step 5643, loss: 1.4169909954071045\n",
      "step 5644, loss: 1.344921588897705\n",
      "step 5645, loss: 1.314771056175232\n",
      "step 5646, loss: 1.2814527750015259\n",
      "step 5647, loss: 1.2652604579925537\n",
      "step 5648, loss: 1.3163596391677856\n",
      "step 5649, loss: 1.2077603340148926\n",
      "step 5650, loss: 1.1828685998916626\n",
      "step 5651, loss: 1.2004163265228271\n",
      "step 5652, loss: 1.2810425758361816\n",
      "step 5653, loss: 1.2476571798324585\n",
      "step 5654, loss: 1.1819597482681274\n",
      "step 5655, loss: 1.214434266090393\n",
      "step 5656, loss: 1.2524148225784302\n",
      "step 5657, loss: 1.3498669862747192\n",
      "step 5658, loss: 1.1732228994369507\n",
      "step 5659, loss: 0.6748312711715698\n",
      "step 5660, loss: 0.7769015431404114\n",
      "step 5661, loss: 0.6142271757125854\n",
      "step 5662, loss: 0.7410756349563599\n",
      "step 5663, loss: 0.7219565510749817\n",
      "step 5664, loss: 0.6811190247535706\n",
      "step 5665, loss: 0.6792281270027161\n",
      "step 5666, loss: 0.7486992478370667\n",
      "step 5667, loss: 0.7745964527130127\n",
      "step 5668, loss: 0.601879358291626\n",
      "step 5669, loss: 0.6337639093399048\n",
      "step 5670, loss: 0.8651663661003113\n",
      "step 5671, loss: 0.8263311386108398\n",
      "step 5672, loss: 1.1045291423797607\n",
      "step 5673, loss: 1.3624883890151978\n",
      "step 5674, loss: 1.2317957878112793\n",
      "step 5675, loss: 1.1801073551177979\n",
      "step 5676, loss: 1.191577672958374\n",
      "step 5677, loss: 1.1563770771026611\n",
      "step 5678, loss: 1.0808539390563965\n",
      "step 5679, loss: 1.241976261138916\n",
      "step 5680, loss: 1.188966155052185\n",
      "step 5681, loss: 1.2153470516204834\n",
      "step 5682, loss: 1.2535992860794067\n",
      "step 5683, loss: 1.2381634712219238\n",
      "step 5684, loss: 1.1811162233352661\n",
      "step 5685, loss: 1.3894959688186646\n",
      "step 5686, loss: 1.349137306213379\n",
      "step 5687, loss: 1.1618038415908813\n",
      "step 5688, loss: 1.618519902229309\n",
      "step 5689, loss: 1.4821021556854248\n",
      "step 5690, loss: 1.3973716497421265\n",
      "step 5691, loss: 1.3550705909729004\n",
      "step 5692, loss: 1.2629588842391968\n",
      "step 5693, loss: 1.4405901432037354\n",
      "step 5694, loss: 1.5473501682281494\n",
      "step 5695, loss: 1.231276035308838\n",
      "step 5696, loss: 1.3857157230377197\n",
      "step 5697, loss: 1.6349810361862183\n",
      "step 5698, loss: 1.446646809577942\n",
      "step 5699, loss: 1.4941169023513794\n",
      "step 5700, loss: 1.3750078678131104\n",
      "step 5701, loss: 1.4548765420913696\n",
      "step 5702, loss: 1.3470383882522583\n",
      "step 5703, loss: 1.5208770036697388\n",
      "step 5704, loss: 1.417015790939331\n",
      "step 5705, loss: 1.2028844356536865\n",
      "step 5706, loss: 1.2963535785675049\n",
      "step 5707, loss: 1.3894234895706177\n",
      "step 5708, loss: 1.4801740646362305\n",
      "step 5709, loss: 1.3440909385681152\n",
      "step 5710, loss: 1.335056185722351\n",
      "step 5711, loss: 1.3639376163482666\n",
      "step 5712, loss: 1.634607195854187\n",
      "step 5713, loss: 1.2823868989944458\n",
      "step 5714, loss: 1.212268352508545\n",
      "step 5715, loss: 1.5301258563995361\n",
      "step 5716, loss: 1.3405097723007202\n",
      "step 5717, loss: 1.225045084953308\n",
      "step 5718, loss: 1.4892239570617676\n",
      "step 5719, loss: 1.3077713251113892\n",
      "step 5720, loss: 1.3696010112762451\n",
      "step 5721, loss: 1.5701104402542114\n",
      "step 5722, loss: 1.3547031879425049\n",
      "step 5723, loss: 1.3972113132476807\n",
      "step 5724, loss: 1.4787259101867676\n",
      "step 5725, loss: 1.2483233213424683\n",
      "step 5726, loss: 1.373971939086914\n",
      "step 5727, loss: 1.1992030143737793\n",
      "step 5728, loss: 1.3117471933364868\n",
      "step 5729, loss: 1.4886276721954346\n",
      "step 5730, loss: 1.321763515472412\n",
      "step 5731, loss: 1.1986167430877686\n",
      "step 5732, loss: 1.2881563901901245\n",
      "step 5733, loss: 1.2283337116241455\n",
      "step 5734, loss: 1.6040645837783813\n",
      "step 5735, loss: 1.3111413717269897\n",
      "step 5736, loss: 1.3761475086212158\n",
      "step 5737, loss: 1.1962286233901978\n",
      "step 5738, loss: 1.1314759254455566\n",
      "step 5739, loss: 1.4145528078079224\n",
      "step 5740, loss: 1.1382704973220825\n",
      "step 5741, loss: 1.1839485168457031\n",
      "step 5742, loss: 1.3402836322784424\n",
      "step 5743, loss: 1.3574719429016113\n",
      "step 5744, loss: 1.4263108968734741\n",
      "step 5745, loss: 1.4066287279129028\n",
      "step 5746, loss: 1.2657546997070312\n",
      "step 5747, loss: 1.2201528549194336\n",
      "step 5748, loss: 0.9468092322349548\n",
      "step 5749, loss: 1.2671536207199097\n",
      "step 5750, loss: 1.1822444200515747\n",
      "step 5751, loss: 1.259702444076538\n",
      "step 5752, loss: 1.2672957181930542\n",
      "step 5753, loss: 1.429550051689148\n",
      "step 5754, loss: 1.566014051437378\n",
      "step 5755, loss: 1.2634344100952148\n",
      "step 5756, loss: 1.3475269079208374\n",
      "step 5757, loss: 1.3160419464111328\n",
      "step 5758, loss: 1.366154432296753\n",
      "step 5759, loss: 1.2276707887649536\n",
      "step 5760, loss: 1.3992009162902832\n",
      "step 5761, loss: 1.4946320056915283\n",
      "step 5762, loss: 1.60019052028656\n",
      "step 5763, loss: 1.3770321607589722\n",
      "step 5764, loss: 1.4480539560317993\n",
      "step 5765, loss: 1.3601495027542114\n",
      "step 5766, loss: 1.2157267332077026\n",
      "step 5767, loss: 1.4087722301483154\n",
      "step 5768, loss: 1.338526964187622\n",
      "step 5769, loss: 1.490313172340393\n",
      "step 5770, loss: 1.4014054536819458\n",
      "step 5771, loss: 1.2177906036376953\n",
      "step 5772, loss: 1.4425122737884521\n",
      "step 5773, loss: 1.3555142879486084\n",
      "step 5774, loss: 1.2386900186538696\n",
      "step 5775, loss: 1.2645331621170044\n",
      "step 5776, loss: 1.3076766729354858\n",
      "step 5777, loss: 1.160262942314148\n",
      "step 5778, loss: 1.0820704698562622\n",
      "step 5779, loss: 1.3219166994094849\n",
      "step 5780, loss: 1.1528401374816895\n",
      "step 5781, loss: 1.0638365745544434\n",
      "step 5782, loss: 1.0745476484298706\n",
      "step 5783, loss: 1.1018402576446533\n",
      "step 5784, loss: 0.7315829396247864\n",
      "step 5785, loss: 0.6482898592948914\n",
      "step 5786, loss: 0.825782060623169\n",
      "step 5787, loss: 0.598058819770813\n",
      "step 5788, loss: 0.5479632019996643\n",
      "step 5789, loss: 0.6366318464279175\n",
      "step 5790, loss: 0.6330620646476746\n",
      "step 5791, loss: 0.5257614254951477\n",
      "step 5792, loss: 0.6069225668907166\n",
      "step 5793, loss: 0.521726667881012\n",
      "step 5794, loss: 0.6403452157974243\n",
      "step 5795, loss: 0.5398104190826416\n",
      "step 5796, loss: 0.7667328715324402\n",
      "step 5797, loss: 0.46184754371643066\n",
      "step 5798, loss: 0.5194064378738403\n",
      "step 5799, loss: 0.7286657094955444\n",
      "step 5800, loss: 0.5525765419006348\n",
      "step 5801, loss: 0.6267622709274292\n",
      "step 5802, loss: 0.634281575679779\n",
      "step 5803, loss: 0.47848543524742126\n",
      "step 5804, loss: 0.7285154461860657\n",
      "step 5805, loss: 0.698091447353363\n",
      "step 5806, loss: 0.7684967517852783\n",
      "step 5807, loss: 0.6428771018981934\n",
      "step 5808, loss: 0.6807889938354492\n",
      "step 5809, loss: 0.6050437688827515\n",
      "step 5810, loss: 0.6159127950668335\n",
      "step 5811, loss: 0.6035895347595215\n",
      "step 5812, loss: 1.281820297241211\n",
      "step 5813, loss: 1.3270015716552734\n",
      "step 5814, loss: 1.0928996801376343\n",
      "step 5815, loss: 1.254336953163147\n",
      "step 5816, loss: 1.238635778427124\n",
      "step 5817, loss: 1.1873267889022827\n",
      "step 5818, loss: 1.1377614736557007\n",
      "step 5819, loss: 1.2412986755371094\n",
      "step 5820, loss: 1.3030054569244385\n",
      "step 5821, loss: 1.0410442352294922\n",
      "step 5822, loss: 1.233100414276123\n",
      "step 5823, loss: 1.2683290243148804\n",
      "step 5824, loss: 1.0439605712890625\n",
      "step 5825, loss: 1.27803635597229\n",
      "step 5826, loss: 1.4004528522491455\n",
      "step 5827, loss: 1.2715224027633667\n",
      "step 5828, loss: 1.194193959236145\n",
      "step 5829, loss: 1.3677903413772583\n",
      "step 5830, loss: 1.38810396194458\n",
      "step 5831, loss: 1.335221767425537\n",
      "step 5832, loss: 1.4915833473205566\n",
      "step 5833, loss: 1.3500908613204956\n",
      "step 5834, loss: 1.3352994918823242\n",
      "step 5835, loss: 1.5269651412963867\n",
      "step 5836, loss: 1.4019811153411865\n",
      "step 5837, loss: 1.3195523023605347\n",
      "step 5838, loss: 1.1949923038482666\n",
      "step 5839, loss: 1.1176379919052124\n",
      "step 5840, loss: 1.307202935218811\n",
      "step 5841, loss: 1.1087199449539185\n",
      "step 5842, loss: 1.1671315431594849\n",
      "step 5843, loss: 1.1378028392791748\n",
      "step 5844, loss: 1.3865922689437866\n",
      "step 5845, loss: 1.1904035806655884\n",
      "step 5846, loss: 1.4092236757278442\n",
      "step 5847, loss: 1.321872591972351\n",
      "step 5848, loss: 1.367465615272522\n",
      "step 5849, loss: 1.288076639175415\n",
      "step 5850, loss: 1.326653242111206\n",
      "step 5851, loss: 1.3079622983932495\n",
      "step 5852, loss: 1.176310420036316\n",
      "step 5853, loss: 1.1617339849472046\n",
      "step 5854, loss: 1.1954039335250854\n",
      "step 5855, loss: 0.9713863730430603\n",
      "step 5856, loss: 1.1777898073196411\n",
      "step 5857, loss: 1.376292109489441\n",
      "step 5858, loss: 1.6130051612854004\n",
      "step 5859, loss: 1.196834921836853\n",
      "step 5860, loss: 1.2368264198303223\n",
      "step 5861, loss: 1.2777460813522339\n",
      "step 5862, loss: 1.2633217573165894\n",
      "step 5863, loss: 1.2041091918945312\n",
      "step 5864, loss: 1.1558735370635986\n",
      "step 5865, loss: 1.344500184059143\n",
      "step 5866, loss: 1.2486835718154907\n",
      "step 5867, loss: 1.1545617580413818\n",
      "step 5868, loss: 1.1457042694091797\n",
      "step 5869, loss: 1.3076515197753906\n",
      "step 5870, loss: 1.3395750522613525\n",
      "step 5871, loss: 1.4156336784362793\n",
      "step 5872, loss: 1.227533221244812\n",
      "step 5873, loss: 1.4468237161636353\n",
      "step 5874, loss: 1.3233458995819092\n",
      "step 5875, loss: 1.1046266555786133\n",
      "step 5876, loss: 1.0144776105880737\n",
      "step 5877, loss: 1.0751852989196777\n",
      "step 5878, loss: 1.307278037071228\n",
      "step 5879, loss: 1.2716450691223145\n",
      "step 5880, loss: 1.3211719989776611\n",
      "step 5881, loss: 1.3166536092758179\n",
      "step 5882, loss: 1.1430296897888184\n",
      "step 5883, loss: 1.3077354431152344\n",
      "step 5884, loss: 1.3717106580734253\n",
      "step 5885, loss: 1.3313090801239014\n",
      "step 5886, loss: 1.3308680057525635\n",
      "step 5887, loss: 1.2277476787567139\n",
      "step 5888, loss: 1.1607568264007568\n",
      "step 5889, loss: 1.3950574398040771\n",
      "step 5890, loss: 1.1563389301300049\n",
      "step 5891, loss: 1.1435110569000244\n",
      "step 5892, loss: 1.0600425004959106\n",
      "step 5893, loss: 1.1600518226623535\n",
      "step 5894, loss: 1.2903625965118408\n",
      "step 5895, loss: 1.2153403759002686\n",
      "step 5896, loss: 1.1094355583190918\n",
      "step 5897, loss: 1.1546331644058228\n",
      "step 5898, loss: 1.000327706336975\n",
      "step 5899, loss: 1.178071141242981\n",
      "step 5900, loss: 1.452565312385559\n",
      "step 5901, loss: 1.182855486869812\n",
      "step 5902, loss: 1.2867988348007202\n",
      "step 5903, loss: 1.3129881620407104\n",
      "step 5904, loss: 1.345868468284607\n",
      "step 5905, loss: 1.2767443656921387\n",
      "step 5906, loss: 1.0479764938354492\n",
      "step 5907, loss: 1.058057188987732\n",
      "step 5908, loss: 0.8258355259895325\n",
      "step 5909, loss: 0.8107762932777405\n",
      "step 5910, loss: 0.6740519404411316\n",
      "step 5911, loss: 0.8131344318389893\n",
      "step 5912, loss: 0.7332627177238464\n",
      "step 5913, loss: 0.8573907017707825\n",
      "step 5914, loss: 0.6967347860336304\n",
      "step 5915, loss: 0.5864254832267761\n",
      "step 5916, loss: 0.8913156390190125\n",
      "step 5917, loss: 0.5822983384132385\n",
      "step 5918, loss: 0.6024982929229736\n",
      "step 5919, loss: 0.6795833110809326\n",
      "step 5920, loss: 0.6441420912742615\n",
      "step 5921, loss: 0.5388026833534241\n",
      "step 5922, loss: 0.574682354927063\n",
      "step 5923, loss: 0.485051691532135\n",
      "step 5924, loss: 0.5710507035255432\n",
      "step 5925, loss: 0.5580844283103943\n",
      "step 5926, loss: 0.6441619396209717\n",
      "step 5927, loss: 0.6687934398651123\n",
      "step 5928, loss: 0.6291125416755676\n",
      "step 5929, loss: 0.6426272392272949\n",
      "step 5930, loss: 1.2714297771453857\n",
      "step 5931, loss: 0.9991530179977417\n",
      "step 5932, loss: 1.1897773742675781\n",
      "step 5933, loss: 1.1668678522109985\n",
      "step 5934, loss: 1.1014111042022705\n",
      "step 5935, loss: 1.0973373651504517\n",
      "step 5936, loss: 1.020146131515503\n",
      "step 5937, loss: 1.1646922826766968\n",
      "step 5938, loss: 1.3199050426483154\n",
      "step 5939, loss: 1.2806061506271362\n",
      "step 5940, loss: 1.3815871477127075\n",
      "step 5941, loss: 1.1517326831817627\n",
      "step 5942, loss: 1.1136409044265747\n",
      "step 5943, loss: 1.3252719640731812\n",
      "step 5944, loss: 1.3217853307724\n",
      "step 5945, loss: 1.123416543006897\n",
      "step 5946, loss: 1.1855117082595825\n",
      "step 5947, loss: 1.358464002609253\n",
      "step 5948, loss: 1.2074860334396362\n",
      "step 5949, loss: 1.599260687828064\n",
      "step 5950, loss: 1.2912901639938354\n",
      "step 5951, loss: 1.2043887376785278\n",
      "step 5952, loss: 1.3553216457366943\n",
      "step 5953, loss: 1.2335392236709595\n",
      "step 5954, loss: 1.435983419418335\n",
      "step 5955, loss: 1.320772409439087\n",
      "step 5956, loss: 1.2904828786849976\n",
      "step 5957, loss: 1.2886486053466797\n",
      "step 5958, loss: 1.505575180053711\n",
      "step 5959, loss: 1.37703537940979\n",
      "step 5960, loss: 1.218083143234253\n",
      "step 5961, loss: 1.291994571685791\n",
      "step 5962, loss: 1.3116559982299805\n",
      "step 5963, loss: 1.2599098682403564\n",
      "step 5964, loss: 1.2921490669250488\n",
      "step 5965, loss: 1.4404038190841675\n",
      "step 5966, loss: 1.3770874738693237\n",
      "step 5967, loss: 1.3252243995666504\n",
      "step 5968, loss: 1.4161348342895508\n",
      "step 5969, loss: 1.6233296394348145\n",
      "step 5970, loss: 1.596852421760559\n",
      "step 5971, loss: 1.447446346282959\n",
      "step 5972, loss: 1.4596889019012451\n",
      "step 5973, loss: 1.7459158897399902\n",
      "step 5974, loss: 1.4528905153274536\n",
      "step 5975, loss: 1.7501696348190308\n",
      "step 5976, loss: 1.550978660583496\n",
      "step 5977, loss: 1.6401997804641724\n",
      "step 5978, loss: 1.4750373363494873\n",
      "step 5979, loss: 1.380114197731018\n",
      "step 5980, loss: 1.306735873222351\n",
      "step 5981, loss: 1.3729242086410522\n",
      "step 5982, loss: 1.3029288053512573\n",
      "step 5983, loss: 1.2978514432907104\n",
      "step 5984, loss: 1.0193870067596436\n",
      "step 5985, loss: 1.1850875616073608\n",
      "step 5986, loss: 1.3792834281921387\n",
      "step 5987, loss: 1.2993950843811035\n",
      "step 5988, loss: 1.401108980178833\n",
      "step 5989, loss: 1.2894757986068726\n",
      "step 5990, loss: 1.203779935836792\n",
      "step 5991, loss: 1.3921102285385132\n",
      "step 5992, loss: 1.136853575706482\n",
      "step 5993, loss: 1.357645869255066\n",
      "step 5994, loss: 1.1671838760375977\n",
      "step 5995, loss: 1.1114702224731445\n",
      "step 5996, loss: 1.4903565645217896\n",
      "step 5997, loss: 1.5156038999557495\n",
      "step 5998, loss: 1.5107437372207642\n",
      "step 5999, loss: 1.3912606239318848\n",
      "step 6000, loss: 1.5689538717269897\n",
      "step 6001, loss: 1.6317379474639893\n",
      "step 6002, loss: 1.5610822439193726\n",
      "step 6003, loss: 1.4889098405838013\n",
      "step 6004, loss: 1.3368674516677856\n",
      "step 6005, loss: 1.3983045816421509\n",
      "step 6006, loss: 1.4341771602630615\n",
      "step 6007, loss: 1.5643682479858398\n",
      "step 6008, loss: 1.3321300745010376\n",
      "step 6009, loss: 1.4328241348266602\n",
      "step 6010, loss: 1.2671093940734863\n",
      "step 6011, loss: 1.1953825950622559\n",
      "step 6012, loss: 1.3689792156219482\n",
      "step 6013, loss: 1.2499675750732422\n",
      "step 6014, loss: 1.4515197277069092\n",
      "step 6015, loss: 1.4465008974075317\n",
      "step 6016, loss: 1.47529935836792\n",
      "step 6017, loss: 1.4660228490829468\n",
      "step 6018, loss: 1.3245443105697632\n",
      "step 6019, loss: 1.3067330121994019\n",
      "step 6020, loss: 0.7465820908546448\n",
      "step 6021, loss: 0.7740553617477417\n",
      "step 6022, loss: 0.882486879825592\n",
      "step 6023, loss: 0.7387527227401733\n",
      "step 6024, loss: 0.9114450812339783\n",
      "step 6025, loss: 0.7105739712715149\n",
      "step 6026, loss: 0.6012267470359802\n",
      "step 6027, loss: 0.5763857960700989\n",
      "step 6028, loss: 0.7192557454109192\n",
      "step 6029, loss: 0.6012369394302368\n",
      "step 6030, loss: 0.7001988887786865\n",
      "step 6031, loss: 0.5945554375648499\n",
      "step 6032, loss: 0.5143812894821167\n",
      "step 6033, loss: 0.7281675338745117\n",
      "step 6034, loss: 0.4992731511592865\n",
      "step 6035, loss: 0.6559795141220093\n",
      "step 6036, loss: 0.6398054361343384\n",
      "step 6037, loss: 0.8051162958145142\n",
      "step 6038, loss: 1.1431424617767334\n",
      "step 6039, loss: 1.3166676759719849\n",
      "step 6040, loss: 1.3130806684494019\n",
      "step 6041, loss: 1.2181915044784546\n",
      "step 6042, loss: 1.145786166191101\n",
      "step 6043, loss: 1.369070291519165\n",
      "step 6044, loss: 1.2374248504638672\n",
      "step 6045, loss: 1.19145929813385\n",
      "step 6046, loss: 1.1098554134368896\n",
      "step 6047, loss: 1.292465090751648\n",
      "step 6048, loss: 1.232141137123108\n",
      "step 6049, loss: 1.1689400672912598\n",
      "step 6050, loss: 1.116094708442688\n",
      "step 6051, loss: 1.199502944946289\n",
      "step 6052, loss: 1.234835147857666\n",
      "step 6053, loss: 1.0267372131347656\n",
      "step 6054, loss: 1.147093415260315\n",
      "step 6055, loss: 1.0708141326904297\n",
      "step 6056, loss: 1.3477483987808228\n",
      "step 6057, loss: 1.404586911201477\n",
      "step 6058, loss: 1.3174265623092651\n",
      "step 6059, loss: 1.2674537897109985\n",
      "step 6060, loss: 1.4255553483963013\n",
      "step 6061, loss: 1.2967066764831543\n",
      "step 6062, loss: 1.369826078414917\n",
      "step 6063, loss: 1.3438549041748047\n",
      "step 6064, loss: 1.4658726453781128\n",
      "step 6065, loss: 1.2663326263427734\n",
      "step 6066, loss: 1.263269305229187\n",
      "step 6067, loss: 1.2611573934555054\n",
      "step 6068, loss: 1.2519348859786987\n",
      "step 6069, loss: 1.5206143856048584\n",
      "step 6070, loss: 1.2858970165252686\n",
      "step 6071, loss: 1.3321340084075928\n",
      "step 6072, loss: 1.225056767463684\n",
      "step 6073, loss: 1.173722505569458\n",
      "step 6074, loss: 1.2364033460617065\n",
      "step 6075, loss: 1.1860432624816895\n",
      "step 6076, loss: 1.4833533763885498\n",
      "step 6077, loss: 1.4463979005813599\n",
      "step 6078, loss: 1.2119029760360718\n",
      "step 6079, loss: 1.2066223621368408\n",
      "step 6080, loss: 1.0646135807037354\n",
      "step 6081, loss: 1.0150599479675293\n",
      "step 6082, loss: 0.7821053266525269\n",
      "step 6083, loss: 0.7738897800445557\n",
      "step 6084, loss: 0.5966609120368958\n",
      "step 6085, loss: 0.599906861782074\n",
      "step 6086, loss: 0.6144967675209045\n",
      "step 6087, loss: 0.5617218017578125\n",
      "step 6088, loss: 0.42932790517807007\n",
      "step 6089, loss: 0.47327670454978943\n",
      "step 6090, loss: 0.5427485108375549\n",
      "step 6091, loss: 0.5886654853820801\n",
      "step 6092, loss: 0.7609565258026123\n",
      "step 6093, loss: 0.5739306807518005\n",
      "step 6094, loss: 0.5532702803611755\n",
      "step 6095, loss: 0.8300555944442749\n",
      "step 6096, loss: 0.7526664137840271\n",
      "step 6097, loss: 0.733336865901947\n",
      "step 6098, loss: 1.266591191291809\n",
      "step 6099, loss: 1.2599304914474487\n",
      "step 6100, loss: 1.1896342039108276\n",
      "step 6101, loss: 1.173466682434082\n",
      "step 6102, loss: 1.1490989923477173\n",
      "step 6103, loss: 1.1426230669021606\n",
      "step 6104, loss: 1.1449264287948608\n",
      "step 6105, loss: 1.4182438850402832\n",
      "step 6106, loss: 1.1352397203445435\n",
      "step 6107, loss: 1.0050888061523438\n",
      "step 6108, loss: 1.1437495946884155\n",
      "step 6109, loss: 1.0162708759307861\n",
      "step 6110, loss: 1.161637306213379\n",
      "step 6111, loss: 1.1205686330795288\n",
      "step 6112, loss: 1.2818013429641724\n",
      "step 6113, loss: 1.283573031425476\n",
      "step 6114, loss: 1.376789927482605\n",
      "step 6115, loss: 1.3790124654769897\n",
      "step 6116, loss: 1.138213872909546\n",
      "step 6117, loss: 1.1096469163894653\n",
      "step 6118, loss: 1.2490462064743042\n",
      "step 6119, loss: 1.3586738109588623\n",
      "step 6120, loss: 1.3329850435256958\n",
      "step 6121, loss: 1.2674026489257812\n",
      "step 6122, loss: 1.2240912914276123\n",
      "step 6123, loss: 1.237464189529419\n",
      "step 6124, loss: 1.5522655248641968\n",
      "step 6125, loss: 1.3789349794387817\n",
      "step 6126, loss: 1.2804802656173706\n",
      "step 6127, loss: 1.4552475214004517\n",
      "step 6128, loss: 1.318619728088379\n",
      "step 6129, loss: 1.2720333337783813\n",
      "step 6130, loss: 1.259800672531128\n",
      "step 6131, loss: 1.1684881448745728\n",
      "step 6132, loss: 1.3993993997573853\n",
      "step 6133, loss: 1.186130404472351\n",
      "step 6134, loss: 1.3455417156219482\n",
      "step 6135, loss: 1.4538973569869995\n",
      "step 6136, loss: 1.502351999282837\n",
      "step 6137, loss: 1.5910652875900269\n",
      "step 6138, loss: 1.6254068613052368\n",
      "step 6139, loss: 1.4306578636169434\n",
      "step 6140, loss: 1.3886793851852417\n",
      "step 6141, loss: 1.537227988243103\n",
      "step 6142, loss: 1.6188663244247437\n",
      "step 6143, loss: 1.2728769779205322\n",
      "step 6144, loss: 1.1092079877853394\n",
      "step 6145, loss: 1.4332753419876099\n",
      "step 6146, loss: 1.3168944120407104\n",
      "step 6147, loss: 1.5995904207229614\n",
      "step 6148, loss: 1.2946217060089111\n",
      "step 6149, loss: 1.4823346138000488\n",
      "step 6150, loss: 1.4547460079193115\n",
      "step 6151, loss: 1.2286787033081055\n",
      "step 6152, loss: 1.386802315711975\n",
      "step 6153, loss: 1.0831499099731445\n",
      "step 6154, loss: 1.2305083274841309\n",
      "step 6155, loss: 1.101372241973877\n",
      "step 6156, loss: 1.0668072700500488\n",
      "step 6157, loss: 1.2327823638916016\n",
      "step 6158, loss: 1.4163739681243896\n",
      "step 6159, loss: 1.2467533349990845\n",
      "step 6160, loss: 1.4073774814605713\n",
      "step 6161, loss: 1.0688081979751587\n",
      "step 6162, loss: 1.521040916442871\n",
      "step 6163, loss: 1.3980894088745117\n",
      "step 6164, loss: 1.4483189582824707\n",
      "step 6165, loss: 1.298901915550232\n",
      "step 6166, loss: 1.253064513206482\n",
      "step 6167, loss: 1.3849774599075317\n",
      "step 6168, loss: 1.343410611152649\n",
      "step 6169, loss: 1.2117544412612915\n",
      "step 6170, loss: 1.4874117374420166\n",
      "step 6171, loss: 1.309869647026062\n",
      "step 6172, loss: 1.4529292583465576\n",
      "step 6173, loss: 1.3124642372131348\n",
      "step 6174, loss: 1.255515217781067\n",
      "step 6175, loss: 1.1744778156280518\n",
      "step 6176, loss: 1.3955165147781372\n",
      "step 6177, loss: 1.303699016571045\n",
      "step 6178, loss: 1.261674165725708\n",
      "step 6179, loss: 1.0925642251968384\n",
      "step 6180, loss: 1.4460272789001465\n",
      "step 6181, loss: 1.1779531240463257\n",
      "step 6182, loss: 1.0425119400024414\n",
      "step 6183, loss: 1.1057602167129517\n",
      "step 6184, loss: 1.1346244812011719\n",
      "step 6185, loss: 1.182206630706787\n",
      "step 6186, loss: 1.1398355960845947\n",
      "step 6187, loss: 0.761965811252594\n",
      "step 6188, loss: 0.6151237487792969\n",
      "step 6189, loss: 0.5739701986312866\n",
      "step 6190, loss: 0.60002201795578\n",
      "step 6191, loss: 0.6320493817329407\n",
      "step 6192, loss: 0.5723447799682617\n",
      "step 6193, loss: 0.5656623840332031\n",
      "step 6194, loss: 0.6061443090438843\n",
      "step 6195, loss: 0.56917405128479\n",
      "step 6196, loss: 0.4204496741294861\n",
      "step 6197, loss: 0.5105039477348328\n",
      "step 6198, loss: 0.5257033109664917\n",
      "step 6199, loss: 0.8205130100250244\n",
      "step 6200, loss: 0.7032404541969299\n",
      "step 6201, loss: 0.6114206910133362\n",
      "step 6202, loss: 0.6228774189949036\n",
      "step 6203, loss: 0.6013975143432617\n",
      "step 6204, loss: 0.5467219352722168\n",
      "step 6205, loss: 0.5776658654212952\n",
      "step 6206, loss: 0.44678372144699097\n",
      "step 6207, loss: 0.3414804935455322\n",
      "step 6208, loss: 0.6454012989997864\n",
      "step 6209, loss: 1.3683234453201294\n",
      "step 6210, loss: 1.2550498247146606\n",
      "step 6211, loss: 1.1925678253173828\n",
      "step 6212, loss: 1.030139446258545\n",
      "step 6213, loss: 1.0708876848220825\n",
      "step 6214, loss: 1.258420467376709\n",
      "step 6215, loss: 1.248380184173584\n",
      "step 6216, loss: 1.1089129447937012\n",
      "step 6217, loss: 1.0966932773590088\n",
      "step 6218, loss: 1.2036736011505127\n",
      "step 6219, loss: 1.132446050643921\n",
      "step 6220, loss: 1.183202862739563\n",
      "step 6221, loss: 0.9382225275039673\n",
      "step 6222, loss: 1.0285989046096802\n",
      "step 6223, loss: 0.8964424729347229\n",
      "step 6224, loss: 1.0555766820907593\n",
      "step 6225, loss: 0.983163595199585\n",
      "step 6226, loss: 1.0637657642364502\n",
      "step 6227, loss: 1.097427487373352\n",
      "step 6228, loss: 1.2215278148651123\n",
      "step 6229, loss: 1.2506346702575684\n",
      "step 6230, loss: 1.4094774723052979\n",
      "step 6231, loss: 1.1246349811553955\n",
      "step 6232, loss: 1.0786120891571045\n",
      "step 6233, loss: 1.1145232915878296\n",
      "step 6234, loss: 1.1595042943954468\n",
      "step 6235, loss: 1.35873281955719\n",
      "step 6236, loss: 1.0290168523788452\n",
      "step 6237, loss: 1.1331218481063843\n",
      "step 6238, loss: 1.219355583190918\n",
      "step 6239, loss: 1.2042800188064575\n",
      "step 6240, loss: 1.1359758377075195\n",
      "step 6241, loss: 1.1989543437957764\n",
      "step 6242, loss: 1.1129628419876099\n",
      "step 6243, loss: 1.1587657928466797\n",
      "step 6244, loss: 1.0961068868637085\n",
      "step 6245, loss: 1.2621235847473145\n",
      "step 6246, loss: 1.3834736347198486\n",
      "step 6247, loss: 1.2462657690048218\n",
      "step 6248, loss: 1.1495366096496582\n",
      "step 6249, loss: 1.1809101104736328\n",
      "step 6250, loss: 1.112973690032959\n",
      "step 6251, loss: 1.2663261890411377\n",
      "step 6252, loss: 1.3094040155410767\n",
      "step 6253, loss: 1.3185667991638184\n",
      "step 6254, loss: 1.6458011865615845\n",
      "step 6255, loss: 1.4069355726242065\n",
      "step 6256, loss: 1.2080564498901367\n",
      "step 6257, loss: 1.1583611965179443\n",
      "step 6258, loss: 1.1700996160507202\n",
      "step 6259, loss: 1.0350964069366455\n",
      "step 6260, loss: 1.333027720451355\n",
      "step 6261, loss: 1.1693150997161865\n",
      "step 6262, loss: 1.4269670248031616\n",
      "step 6263, loss: 1.2693769931793213\n",
      "step 6264, loss: 1.2120612859725952\n",
      "step 6265, loss: 1.4167778491973877\n",
      "step 6266, loss: 1.1954779624938965\n",
      "step 6267, loss: 1.0986417531967163\n",
      "step 6268, loss: 1.2291994094848633\n",
      "step 6269, loss: 1.1069097518920898\n",
      "step 6270, loss: 0.8523644804954529\n",
      "step 6271, loss: 1.0975710153579712\n",
      "step 6272, loss: 1.0511149168014526\n",
      "step 6273, loss: 1.0624977350234985\n",
      "step 6274, loss: 1.232258915901184\n",
      "step 6275, loss: 1.202880859375\n",
      "step 6276, loss: 1.2124433517456055\n",
      "step 6277, loss: 1.3675603866577148\n",
      "step 6278, loss: 1.1010231971740723\n",
      "step 6279, loss: 1.1099122762680054\n",
      "step 6280, loss: 1.2413825988769531\n",
      "step 6281, loss: 1.241808533668518\n",
      "step 6282, loss: 1.3592091798782349\n",
      "step 6283, loss: 1.0182740688323975\n",
      "step 6284, loss: 1.1382007598876953\n",
      "step 6285, loss: 1.1537901163101196\n",
      "step 6286, loss: 1.2789548635482788\n",
      "step 6287, loss: 1.366737723350525\n",
      "step 6288, loss: 1.1855835914611816\n",
      "step 6289, loss: 1.371048927307129\n",
      "step 6290, loss: 1.3140476942062378\n",
      "step 6291, loss: 1.1212013959884644\n",
      "step 6292, loss: 1.0686789751052856\n",
      "step 6293, loss: 1.2854816913604736\n",
      "step 6294, loss: 1.1409528255462646\n",
      "step 6295, loss: 1.1358598470687866\n",
      "step 6296, loss: 1.054133653640747\n",
      "step 6297, loss: 1.3792827129364014\n",
      "step 6298, loss: 1.1317391395568848\n",
      "step 6299, loss: 1.0562130212783813\n",
      "step 6300, loss: 1.0735195875167847\n",
      "step 6301, loss: 1.1218069791793823\n",
      "step 6302, loss: 1.083214282989502\n",
      "step 6303, loss: 1.0100655555725098\n",
      "step 6304, loss: 1.3914899826049805\n",
      "step 6305, loss: 1.1658344268798828\n",
      "step 6306, loss: 1.2120583057403564\n",
      "step 6307, loss: 1.3168413639068604\n",
      "step 6308, loss: 1.0834178924560547\n",
      "step 6309, loss: 1.1867637634277344\n",
      "step 6310, loss: 1.1576640605926514\n",
      "step 6311, loss: 1.192373514175415\n",
      "step 6312, loss: 0.9688361883163452\n",
      "step 6313, loss: 0.8763662576675415\n",
      "step 6314, loss: 1.130094051361084\n",
      "step 6315, loss: 1.3677021265029907\n",
      "step 6316, loss: 1.067676067352295\n",
      "step 6317, loss: 0.965935230255127\n",
      "step 6318, loss: 0.8812618255615234\n",
      "step 6319, loss: 0.6503830552101135\n",
      "step 6320, loss: 0.613392174243927\n",
      "step 6321, loss: 0.7324197292327881\n",
      "step 6322, loss: 0.5704929232597351\n",
      "step 6323, loss: 0.529235303401947\n",
      "step 6324, loss: 0.7022697925567627\n",
      "step 6325, loss: 0.7355175018310547\n",
      "step 6326, loss: 0.719925045967102\n",
      "step 6327, loss: 0.5293149948120117\n",
      "step 6328, loss: 0.5168867707252502\n",
      "step 6329, loss: 0.45631033182144165\n",
      "step 6330, loss: 0.639683723449707\n",
      "step 6331, loss: 0.7801676392555237\n",
      "step 6332, loss: 0.5737507343292236\n",
      "step 6333, loss: 0.3741416931152344\n",
      "step 6334, loss: 0.45542478561401367\n",
      "step 6335, loss: 0.5460373759269714\n",
      "step 6336, loss: 0.4421817660331726\n",
      "step 6337, loss: 0.5488536357879639\n",
      "step 6338, loss: 0.5204588174819946\n",
      "step 6339, loss: 0.633929431438446\n",
      "step 6340, loss: 0.5879183411598206\n",
      "step 6341, loss: 0.5901108980178833\n",
      "step 6342, loss: 0.5413876175880432\n",
      "step 6343, loss: 0.494862824678421\n",
      "step 6344, loss: 0.48365652561187744\n",
      "step 6345, loss: 0.4594787061214447\n",
      "step 6346, loss: 0.5626577734947205\n",
      "step 6347, loss: 0.5762027502059937\n",
      "step 6348, loss: 1.0317631959915161\n",
      "step 6349, loss: 1.0695780515670776\n",
      "step 6350, loss: 1.0104329586029053\n",
      "step 6351, loss: 0.9261472821235657\n",
      "step 6352, loss: 0.931951642036438\n",
      "step 6353, loss: 1.36531662940979\n",
      "step 6354, loss: 1.1685656309127808\n",
      "step 6355, loss: 1.2210379838943481\n",
      "step 6356, loss: 0.983709990978241\n",
      "step 6357, loss: 1.2181613445281982\n",
      "step 6358, loss: 1.232948899269104\n",
      "step 6359, loss: 1.062099814414978\n",
      "step 6360, loss: 1.1918867826461792\n",
      "step 6361, loss: 1.1807388067245483\n",
      "step 6362, loss: 1.3363419771194458\n",
      "step 6363, loss: 1.2387775182724\n",
      "step 6364, loss: 1.0561456680297852\n",
      "step 6365, loss: 1.0733240842819214\n",
      "step 6366, loss: 1.1908366680145264\n",
      "step 6367, loss: 1.1324867010116577\n",
      "step 6368, loss: 1.097397804260254\n",
      "step 6369, loss: 1.0866001844406128\n",
      "step 6370, loss: 1.0707218647003174\n",
      "step 6371, loss: 1.1964976787567139\n",
      "step 6372, loss: 1.1534461975097656\n",
      "step 6373, loss: 1.1086111068725586\n",
      "step 6374, loss: 1.1183116436004639\n",
      "step 6375, loss: 1.317225456237793\n",
      "step 6376, loss: 1.0532855987548828\n",
      "step 6377, loss: 0.9038233757019043\n",
      "step 6378, loss: 1.4056402444839478\n",
      "step 6379, loss: 1.151116967201233\n",
      "step 6380, loss: 1.1774897575378418\n",
      "step 6381, loss: 1.256844401359558\n",
      "step 6382, loss: 1.250688910484314\n",
      "step 6383, loss: 1.2855921983718872\n",
      "step 6384, loss: 1.223305106163025\n",
      "step 6385, loss: 1.2762707471847534\n",
      "step 6386, loss: 1.3727092742919922\n",
      "step 6387, loss: 1.3084418773651123\n",
      "step 6388, loss: 1.3883051872253418\n",
      "step 6389, loss: 1.3301982879638672\n",
      "step 6390, loss: 1.5030540227890015\n",
      "step 6391, loss: 1.3157517910003662\n",
      "step 6392, loss: 1.2302554845809937\n",
      "step 6393, loss: 1.2938181161880493\n",
      "step 6394, loss: 1.0801924467086792\n",
      "step 6395, loss: 1.0798434019088745\n",
      "step 6396, loss: 1.3358349800109863\n",
      "step 6397, loss: 1.4244147539138794\n",
      "step 6398, loss: 1.2239478826522827\n",
      "step 6399, loss: 1.288947343826294\n",
      "step 6400, loss: 1.2077956199645996\n",
      "step 6401, loss: 1.1242923736572266\n",
      "step 6402, loss: 1.1643400192260742\n",
      "step 6403, loss: 1.1958234310150146\n",
      "step 6404, loss: 1.0481516122817993\n",
      "step 6405, loss: 0.996825635433197\n",
      "step 6406, loss: 1.2551462650299072\n",
      "step 6407, loss: 1.0868698358535767\n",
      "step 6408, loss: 1.1692370176315308\n",
      "step 6409, loss: 1.2836980819702148\n",
      "step 6410, loss: 1.125807285308838\n",
      "step 6411, loss: 1.1011701822280884\n",
      "step 6412, loss: 1.0064455270767212\n",
      "step 6413, loss: 1.1315252780914307\n",
      "step 6414, loss: 1.4128258228302002\n",
      "step 6415, loss: 1.0701640844345093\n",
      "step 6416, loss: 1.0955921411514282\n",
      "step 6417, loss: 1.216353178024292\n",
      "step 6418, loss: 1.2369784116744995\n",
      "step 6419, loss: 1.3266247510910034\n",
      "step 6420, loss: 1.3025695085525513\n",
      "step 6421, loss: 1.2860667705535889\n",
      "step 6422, loss: 1.1244747638702393\n",
      "step 6423, loss: 1.169588565826416\n",
      "step 6424, loss: 1.093946099281311\n",
      "step 6425, loss: 1.0724595785140991\n",
      "step 6426, loss: 1.3079001903533936\n",
      "step 6427, loss: 1.2951432466506958\n",
      "step 6428, loss: 1.4028512239456177\n",
      "step 6429, loss: 1.1104403734207153\n",
      "step 6430, loss: 1.181243896484375\n",
      "step 6431, loss: 1.2966103553771973\n",
      "step 6432, loss: 1.490035057067871\n",
      "step 6433, loss: 1.466671347618103\n",
      "step 6434, loss: 1.4630887508392334\n",
      "step 6435, loss: 1.3199468851089478\n",
      "step 6436, loss: 1.2364376783370972\n",
      "step 6437, loss: 1.316699743270874\n",
      "step 6438, loss: 1.2326425313949585\n",
      "step 6439, loss: 1.3545517921447754\n",
      "step 6440, loss: 1.1239490509033203\n",
      "step 6441, loss: 1.4083083868026733\n",
      "step 6442, loss: 1.2408421039581299\n",
      "step 6443, loss: 1.0451242923736572\n",
      "step 6444, loss: 1.3152670860290527\n",
      "step 6445, loss: 1.2993329763412476\n",
      "step 6446, loss: 1.1323875188827515\n",
      "step 6447, loss: 1.1734813451766968\n",
      "step 6448, loss: 1.4933632612228394\n",
      "step 6449, loss: 1.2926573753356934\n",
      "step 6450, loss: 1.2447932958602905\n",
      "step 6451, loss: 0.9935106635093689\n",
      "step 6452, loss: 1.2081547975540161\n",
      "step 6453, loss: 1.3854340314865112\n",
      "step 6454, loss: 1.15681791305542\n",
      "step 6455, loss: 1.1985268592834473\n",
      "step 6456, loss: 1.1934592723846436\n",
      "step 6457, loss: 1.3350706100463867\n",
      "step 6458, loss: 1.459230899810791\n",
      "step 6459, loss: 1.2584363222122192\n",
      "step 6460, loss: 1.1788394451141357\n",
      "step 6461, loss: 1.2235157489776611\n",
      "step 6462, loss: 1.2892343997955322\n",
      "step 6463, loss: 1.3766320943832397\n",
      "step 6464, loss: 1.1520386934280396\n",
      "step 6465, loss: 1.1842575073242188\n",
      "step 6466, loss: 1.2940317392349243\n",
      "step 6467, loss: 1.331770420074463\n",
      "step 6468, loss: 1.2654452323913574\n",
      "step 6469, loss: 1.4435932636260986\n",
      "step 6470, loss: 1.3952767848968506\n",
      "step 6471, loss: 1.471680998802185\n",
      "step 6472, loss: 1.2966172695159912\n",
      "step 6473, loss: 1.230218529701233\n",
      "step 6474, loss: 1.1710600852966309\n",
      "step 6475, loss: 1.331311821937561\n",
      "step 6476, loss: 1.2572014331817627\n",
      "step 6477, loss: 1.2548272609710693\n",
      "step 6478, loss: 1.2857791185379028\n",
      "step 6479, loss: 1.1228172779083252\n",
      "step 6480, loss: 1.2635369300842285\n",
      "step 6481, loss: 1.104080319404602\n",
      "step 6482, loss: 1.2577506303787231\n",
      "step 6483, loss: 1.3555399179458618\n",
      "step 6484, loss: 1.323647379875183\n",
      "step 6485, loss: 1.1998298168182373\n",
      "step 6486, loss: 1.1600931882858276\n",
      "step 6487, loss: 1.3085726499557495\n",
      "step 6488, loss: 1.3163526058197021\n",
      "step 6489, loss: 1.4984160661697388\n",
      "step 6490, loss: 1.4280719757080078\n",
      "step 6491, loss: 1.370572805404663\n",
      "step 6492, loss: 1.3240225315093994\n",
      "step 6493, loss: 1.1805198192596436\n",
      "step 6494, loss: 1.0751490592956543\n",
      "step 6495, loss: 1.2568237781524658\n",
      "step 6496, loss: 1.2053303718566895\n",
      "step 6497, loss: 1.3592565059661865\n",
      "step 6498, loss: 1.3392620086669922\n",
      "step 6499, loss: 1.483147144317627\n",
      "step 6500, loss: 1.2408316135406494\n",
      "step 6501, loss: 1.2525544166564941\n",
      "step 6502, loss: 1.1763032674789429\n",
      "step 6503, loss: 1.179919719696045\n",
      "step 6504, loss: 1.2218127250671387\n",
      "step 6505, loss: 1.289838433265686\n",
      "step 6506, loss: 1.289089322090149\n",
      "step 6507, loss: 1.0010671615600586\n",
      "step 6508, loss: 1.074647307395935\n",
      "step 6509, loss: 1.1004375219345093\n",
      "step 6510, loss: 1.2634174823760986\n",
      "step 6511, loss: 1.0486527681350708\n",
      "step 6512, loss: 1.2757360935211182\n",
      "step 6513, loss: 1.1343661546707153\n",
      "step 6514, loss: 1.0884218215942383\n",
      "step 6515, loss: 1.209161400794983\n",
      "step 6516, loss: 1.1291862726211548\n",
      "step 6517, loss: 1.2260832786560059\n",
      "step 6518, loss: 1.065392255783081\n",
      "step 6519, loss: 1.0715975761413574\n",
      "step 6520, loss: 0.9659531116485596\n",
      "step 6521, loss: 1.2198938131332397\n",
      "step 6522, loss: 1.1831433773040771\n",
      "step 6523, loss: 0.9683521389961243\n",
      "step 6524, loss: 1.3181484937667847\n",
      "step 6525, loss: 1.2670565843582153\n",
      "step 6526, loss: 1.215938925743103\n",
      "step 6527, loss: 1.0876349210739136\n",
      "step 6528, loss: 1.1368516683578491\n",
      "step 6529, loss: 1.1267861127853394\n",
      "step 6530, loss: 1.0947237014770508\n",
      "step 6531, loss: 1.020249366760254\n",
      "step 6532, loss: 0.5936025381088257\n",
      "step 6533, loss: 0.5616016983985901\n",
      "step 6534, loss: 0.5307121276855469\n",
      "step 6535, loss: 0.6243073344230652\n",
      "step 6536, loss: 0.5691604018211365\n",
      "step 6537, loss: 0.5452311635017395\n",
      "step 6538, loss: 0.5872907042503357\n",
      "step 6539, loss: 0.5250222086906433\n",
      "step 6540, loss: 0.5256850123405457\n",
      "step 6541, loss: 0.5479761362075806\n",
      "step 6542, loss: 0.5573055148124695\n",
      "step 6543, loss: 0.42783698439598083\n",
      "step 6544, loss: 0.6263101100921631\n",
      "step 6545, loss: 0.555829644203186\n",
      "step 6546, loss: 0.578575074672699\n",
      "step 6547, loss: 0.5775745511054993\n",
      "step 6548, loss: 0.513972282409668\n",
      "step 6549, loss: 0.4115108251571655\n",
      "step 6550, loss: 0.42858925461769104\n",
      "step 6551, loss: 0.4865092933177948\n",
      "step 6552, loss: 0.40005573630332947\n",
      "step 6553, loss: 0.5329577922821045\n",
      "step 6554, loss: 0.7771674990653992\n",
      "step 6555, loss: 0.6104278564453125\n",
      "step 6556, loss: 0.5286110043525696\n",
      "step 6557, loss: 0.41656720638275146\n",
      "step 6558, loss: 0.41671061515808105\n",
      "step 6559, loss: 0.5820308327674866\n",
      "step 6560, loss: 0.46887582540512085\n",
      "step 6561, loss: 0.48446857929229736\n",
      "step 6562, loss: 0.5418683886528015\n",
      "step 6563, loss: 1.1717113256454468\n",
      "step 6564, loss: 1.2643296718597412\n",
      "step 6565, loss: 1.1871013641357422\n",
      "step 6566, loss: 1.0223649740219116\n",
      "step 6567, loss: 1.2052974700927734\n",
      "step 6568, loss: 1.3075456619262695\n",
      "step 6569, loss: 1.0641390085220337\n",
      "step 6570, loss: 1.1449120044708252\n",
      "step 6571, loss: 1.0531291961669922\n",
      "step 6572, loss: 0.9641731977462769\n",
      "step 6573, loss: 0.9676583409309387\n",
      "step 6574, loss: 1.0714588165283203\n",
      "step 6575, loss: 1.0802028179168701\n",
      "step 6576, loss: 1.1773580312728882\n",
      "step 6577, loss: 1.2055968046188354\n",
      "step 6578, loss: 0.9237034320831299\n",
      "step 6579, loss: 1.0484918355941772\n",
      "step 6580, loss: 1.0219286680221558\n",
      "step 6581, loss: 1.3135147094726562\n",
      "step 6582, loss: 1.057518482208252\n",
      "step 6583, loss: 0.9597068428993225\n",
      "step 6584, loss: 1.1363012790679932\n",
      "step 6585, loss: 0.7433203458786011\n",
      "step 6586, loss: 1.0179054737091064\n",
      "step 6587, loss: 0.9923766255378723\n",
      "step 6588, loss: 0.9849523305892944\n",
      "step 6589, loss: 1.0184177160263062\n",
      "step 6590, loss: 0.9892327785491943\n",
      "step 6591, loss: 1.0051486492156982\n",
      "step 6592, loss: 0.9594954252243042\n",
      "step 6593, loss: 1.038208246231079\n",
      "step 6594, loss: 1.1439476013183594\n",
      "step 6595, loss: 0.8830826878547668\n",
      "step 6596, loss: 0.9385746717453003\n",
      "step 6597, loss: 1.0748341083526611\n",
      "step 6598, loss: 1.0573666095733643\n",
      "step 6599, loss: 0.799903154373169\n",
      "step 6600, loss: 0.9555322527885437\n",
      "step 6601, loss: 0.7241183519363403\n",
      "step 6602, loss: 0.6932590007781982\n",
      "step 6603, loss: 0.6748260855674744\n",
      "step 6604, loss: 0.5791426301002502\n",
      "step 6605, loss: 0.5173971056938171\n",
      "step 6606, loss: 0.5289080142974854\n",
      "step 6607, loss: 0.5470200181007385\n",
      "step 6608, loss: 0.6075018048286438\n",
      "step 6609, loss: 0.5808674693107605\n",
      "step 6610, loss: 0.7853820323944092\n",
      "step 6611, loss: 0.6498478651046753\n",
      "step 6612, loss: 0.6676254272460938\n",
      "step 6613, loss: 0.7548514604568481\n",
      "step 6614, loss: 0.47473904490470886\n",
      "step 6615, loss: 0.42454367876052856\n",
      "step 6616, loss: 1.128830075263977\n",
      "step 6617, loss: 1.2513083219528198\n",
      "step 6618, loss: 1.2561759948730469\n",
      "step 6619, loss: 1.3567241430282593\n",
      "step 6620, loss: 1.1331547498703003\n",
      "step 6621, loss: 1.1730315685272217\n",
      "step 6622, loss: 1.170824408531189\n",
      "step 6623, loss: 1.1825376749038696\n",
      "step 6624, loss: 1.0863999128341675\n",
      "step 6625, loss: 1.2240924835205078\n",
      "step 6626, loss: 0.9648330807685852\n",
      "step 6627, loss: 1.2964791059494019\n",
      "step 6628, loss: 1.2657992839813232\n",
      "step 6629, loss: 1.283692479133606\n",
      "step 6630, loss: 1.3148777484893799\n",
      "step 6631, loss: 1.338458776473999\n",
      "step 6632, loss: 1.3729183673858643\n",
      "step 6633, loss: 1.231689214706421\n",
      "step 6634, loss: 1.415130376815796\n",
      "step 6635, loss: 1.3716769218444824\n",
      "step 6636, loss: 1.4383271932601929\n",
      "step 6637, loss: 1.2323060035705566\n",
      "step 6638, loss: 1.2436540126800537\n",
      "step 6639, loss: 1.3084303140640259\n",
      "step 6640, loss: 1.3310763835906982\n",
      "step 6641, loss: 1.2827036380767822\n",
      "step 6642, loss: 1.1472787857055664\n",
      "step 6643, loss: 1.3222917318344116\n",
      "step 6644, loss: 1.2618281841278076\n",
      "step 6645, loss: 1.0733890533447266\n",
      "step 6646, loss: 1.2716057300567627\n",
      "step 6647, loss: 1.3386881351470947\n",
      "step 6648, loss: 0.9557703733444214\n",
      "step 6649, loss: 1.2139294147491455\n",
      "step 6650, loss: 1.0599428415298462\n",
      "step 6651, loss: 1.146140456199646\n",
      "step 6652, loss: 1.0335776805877686\n",
      "step 6653, loss: 1.23563814163208\n",
      "step 6654, loss: 1.2548645734786987\n",
      "step 6655, loss: 1.0713903903961182\n",
      "step 6656, loss: 1.0283256769180298\n",
      "step 6657, loss: 1.125863790512085\n",
      "step 6658, loss: 1.1663732528686523\n",
      "step 6659, loss: 1.15940260887146\n",
      "step 6660, loss: 1.176201343536377\n",
      "step 6661, loss: 1.1899992227554321\n",
      "step 6662, loss: 0.9904050827026367\n",
      "step 6663, loss: 1.0790067911148071\n",
      "step 6664, loss: 1.1633602380752563\n",
      "step 6665, loss: 1.1028099060058594\n",
      "step 6666, loss: 1.023681640625\n",
      "step 6667, loss: 1.0861481428146362\n",
      "step 6668, loss: 1.1255898475646973\n",
      "step 6669, loss: 1.0343148708343506\n",
      "step 6670, loss: 1.3102043867111206\n",
      "step 6671, loss: 1.1520555019378662\n",
      "step 6672, loss: 1.017647624015808\n",
      "step 6673, loss: 1.1233327388763428\n",
      "step 6674, loss: 1.1532005071640015\n",
      "step 6675, loss: 1.0253649950027466\n",
      "step 6676, loss: 1.0299230813980103\n",
      "step 6677, loss: 1.1680827140808105\n",
      "step 6678, loss: 1.1962451934814453\n",
      "step 6679, loss: 1.2315222024917603\n",
      "step 6680, loss: 1.2984850406646729\n",
      "step 6681, loss: 1.1874598264694214\n",
      "step 6682, loss: 1.213742733001709\n",
      "step 6683, loss: 1.1673393249511719\n",
      "step 6684, loss: 1.4029344320297241\n",
      "step 6685, loss: 1.2684087753295898\n",
      "step 6686, loss: 1.261037826538086\n",
      "step 6687, loss: 1.2295178174972534\n",
      "step 6688, loss: 1.3074361085891724\n",
      "step 6689, loss: 1.2314543724060059\n",
      "step 6690, loss: 1.2862452268600464\n",
      "step 6691, loss: 1.405330777168274\n",
      "step 6692, loss: 1.3131812810897827\n",
      "step 6693, loss: 1.0806217193603516\n",
      "step 6694, loss: 1.321586012840271\n",
      "step 6695, loss: 1.2086466550827026\n",
      "step 6696, loss: 1.0993973016738892\n",
      "step 6697, loss: 1.1589514017105103\n",
      "step 6698, loss: 1.2044591903686523\n",
      "step 6699, loss: 1.1249483823776245\n",
      "step 6700, loss: 1.2300529479980469\n",
      "step 6701, loss: 0.9791384935379028\n",
      "step 6702, loss: 0.89178866147995\n",
      "step 6703, loss: 1.1325186491012573\n",
      "step 6704, loss: 0.9306754469871521\n",
      "step 6705, loss: 1.0941596031188965\n",
      "step 6706, loss: 0.5708094835281372\n",
      "step 6707, loss: 0.6239752173423767\n",
      "step 6708, loss: 0.7189062237739563\n",
      "step 6709, loss: 0.5737192034721375\n",
      "step 6710, loss: 0.6279358267784119\n",
      "step 6711, loss: 0.5484042763710022\n",
      "step 6712, loss: 0.5802472829818726\n",
      "step 6713, loss: 0.45604756474494934\n",
      "step 6714, loss: 0.486228346824646\n",
      "step 6715, loss: 0.5889880657196045\n",
      "step 6716, loss: 0.5355634093284607\n",
      "step 6717, loss: 0.5860282182693481\n",
      "step 6718, loss: 0.5279344320297241\n",
      "step 6719, loss: 0.6204206943511963\n",
      "step 6720, loss: 0.8610545992851257\n",
      "step 6721, loss: 1.1181690692901611\n",
      "step 6722, loss: 1.0594580173492432\n",
      "step 6723, loss: 1.1077799797058105\n",
      "step 6724, loss: 1.0110141038894653\n",
      "step 6725, loss: 0.9344462156295776\n",
      "step 6726, loss: 0.9840289950370789\n",
      "step 6727, loss: 0.9328892230987549\n",
      "step 6728, loss: 1.0408869981765747\n",
      "step 6729, loss: 0.9330213665962219\n",
      "step 6730, loss: 0.9824841618537903\n",
      "step 6731, loss: 0.9766305088996887\n",
      "step 6732, loss: 1.0321134328842163\n",
      "step 6733, loss: 1.127915382385254\n",
      "step 6734, loss: 1.0407989025115967\n",
      "step 6735, loss: 1.0710777044296265\n",
      "step 6736, loss: 0.9678665995597839\n",
      "step 6737, loss: 0.9787508249282837\n",
      "step 6738, loss: 1.077697515487671\n",
      "step 6739, loss: 0.9318559765815735\n",
      "step 6740, loss: 1.0192760229110718\n",
      "step 6741, loss: 1.1961182355880737\n",
      "step 6742, loss: 1.1646852493286133\n",
      "step 6743, loss: 0.9971381425857544\n",
      "step 6744, loss: 0.8985294103622437\n",
      "step 6745, loss: 1.095076560974121\n",
      "step 6746, loss: 0.9953625798225403\n",
      "step 6747, loss: 0.8761563897132874\n",
      "step 6748, loss: 1.0752779245376587\n",
      "step 6749, loss: 1.0149409770965576\n",
      "step 6750, loss: 0.9153697490692139\n",
      "step 6751, loss: 1.0363024473190308\n",
      "step 6752, loss: 1.3307852745056152\n",
      "step 6753, loss: 1.0450564622879028\n",
      "step 6754, loss: 1.2662819623947144\n",
      "step 6755, loss: 1.0325369834899902\n",
      "step 6756, loss: 1.0520039796829224\n",
      "step 6757, loss: 1.1057432889938354\n",
      "step 6758, loss: 1.001133918762207\n",
      "step 6759, loss: 0.9461935758590698\n",
      "step 6760, loss: 0.8905743956565857\n",
      "step 6761, loss: 0.973813533782959\n",
      "step 6762, loss: 0.9790629744529724\n",
      "step 6763, loss: 1.234140157699585\n",
      "step 6764, loss: 0.9739359021186829\n",
      "step 6765, loss: 1.1081258058547974\n",
      "step 6766, loss: 0.9763861298561096\n",
      "step 6767, loss: 0.9247794151306152\n",
      "step 6768, loss: 1.0957785844802856\n",
      "step 6769, loss: 0.8278682827949524\n",
      "step 6770, loss: 1.0892431735992432\n",
      "step 6771, loss: 1.122445821762085\n",
      "step 6772, loss: 1.063952088356018\n",
      "step 6773, loss: 1.0012764930725098\n",
      "step 6774, loss: 0.9704170227050781\n",
      "step 6775, loss: 0.8378024101257324\n",
      "step 6776, loss: 0.8229997754096985\n",
      "step 6777, loss: 1.1419048309326172\n",
      "step 6778, loss: 0.7582735419273376\n",
      "step 6779, loss: 0.5734864473342896\n",
      "step 6780, loss: 0.6113158464431763\n",
      "step 6781, loss: 0.5994551181793213\n",
      "step 6782, loss: 0.6008635759353638\n",
      "step 6783, loss: 0.6189131140708923\n",
      "step 6784, loss: 0.5308719873428345\n",
      "step 6785, loss: 0.6111790537834167\n",
      "step 6786, loss: 0.5994269847869873\n",
      "step 6787, loss: 0.611029326915741\n",
      "step 6788, loss: 0.44825074076652527\n",
      "step 6789, loss: 0.4640123248100281\n",
      "step 6790, loss: 0.5211441516876221\n",
      "step 6791, loss: 0.7523589134216309\n",
      "step 6792, loss: 0.8421756029129028\n",
      "step 6793, loss: 1.0428197383880615\n",
      "step 6794, loss: 0.9875511527061462\n",
      "step 6795, loss: 0.8115702867507935\n",
      "step 6796, loss: 0.8765375018119812\n",
      "step 6797, loss: 0.9167614579200745\n",
      "step 6798, loss: 0.8477557897567749\n",
      "step 6799, loss: 0.9899592399597168\n",
      "step 6800, loss: 0.7504153847694397\n",
      "step 6801, loss: 0.9145234823226929\n",
      "step 6802, loss: 1.0620079040527344\n",
      "step 6803, loss: 1.0649052858352661\n",
      "step 6804, loss: 0.9363763332366943\n",
      "step 6805, loss: 0.9744154810905457\n",
      "step 6806, loss: 0.9273912310600281\n",
      "step 6807, loss: 0.8493614792823792\n",
      "step 6808, loss: 1.0279771089553833\n",
      "step 6809, loss: 1.1892188787460327\n",
      "step 6810, loss: 0.9219173789024353\n",
      "step 6811, loss: 1.1495786905288696\n",
      "step 6812, loss: 1.1071408987045288\n",
      "step 6813, loss: 1.1215375661849976\n",
      "step 6814, loss: 1.166014552116394\n",
      "step 6815, loss: 0.9235150814056396\n",
      "step 6816, loss: 1.0166606903076172\n",
      "step 6817, loss: 1.059181571006775\n",
      "step 6818, loss: 1.2658705711364746\n",
      "step 6819, loss: 1.0800888538360596\n",
      "step 6820, loss: 1.1583446264266968\n",
      "step 6821, loss: 1.2290465831756592\n",
      "step 6822, loss: 0.9788995981216431\n",
      "step 6823, loss: 1.2744553089141846\n",
      "step 6824, loss: 1.1640552282333374\n",
      "step 6825, loss: 0.9965670704841614\n",
      "step 6826, loss: 1.3104114532470703\n",
      "step 6827, loss: 1.213884711265564\n",
      "step 6828, loss: 1.2691001892089844\n",
      "step 6829, loss: 1.3043115139007568\n",
      "step 6830, loss: 1.0742051601409912\n",
      "step 6831, loss: 1.2241795063018799\n",
      "step 6832, loss: 1.3557946681976318\n",
      "step 6833, loss: 1.042249083518982\n",
      "step 6834, loss: 1.016023874282837\n",
      "step 6835, loss: 1.3352000713348389\n",
      "step 6836, loss: 1.1306285858154297\n",
      "step 6837, loss: 1.2735826969146729\n",
      "step 6838, loss: 1.083165168762207\n",
      "step 6839, loss: 1.1299018859863281\n",
      "step 6840, loss: 1.1879256963729858\n",
      "step 6841, loss: 1.3863959312438965\n",
      "step 6842, loss: 1.1129926443099976\n",
      "step 6843, loss: 1.323604941368103\n",
      "step 6844, loss: 1.0871541500091553\n",
      "step 6845, loss: 1.3401374816894531\n",
      "step 6846, loss: 1.0147284269332886\n",
      "step 6847, loss: 1.2222142219543457\n",
      "step 6848, loss: 1.1817550659179688\n",
      "step 6849, loss: 1.0363683700561523\n",
      "step 6850, loss: 1.0120031833648682\n",
      "step 6851, loss: 1.0296400785446167\n",
      "step 6852, loss: 0.910545289516449\n",
      "step 6853, loss: 0.9594073295593262\n",
      "step 6854, loss: 1.2844483852386475\n",
      "step 6855, loss: 1.122413992881775\n",
      "step 6856, loss: 1.0747562646865845\n",
      "step 6857, loss: 1.0607153177261353\n",
      "step 6858, loss: 0.9630880355834961\n",
      "step 6859, loss: 1.106123924255371\n",
      "step 6860, loss: 1.0501540899276733\n",
      "step 6861, loss: 1.0110745429992676\n",
      "step 6862, loss: 1.0605177879333496\n",
      "step 6863, loss: 1.0341460704803467\n",
      "step 6864, loss: 1.093036413192749\n",
      "step 6865, loss: 0.9072705507278442\n",
      "step 6866, loss: 1.0739719867706299\n",
      "step 6867, loss: 0.7496696710586548\n",
      "step 6868, loss: 0.9269305467605591\n",
      "step 6869, loss: 1.0611331462860107\n",
      "step 6870, loss: 1.09917414188385\n",
      "step 6871, loss: 1.0808383226394653\n",
      "step 6872, loss: 1.1579880714416504\n",
      "step 6873, loss: 1.1512947082519531\n",
      "step 6874, loss: 1.1989787817001343\n",
      "step 6875, loss: 1.1188488006591797\n",
      "step 6876, loss: 1.0115196704864502\n",
      "step 6877, loss: 1.0341057777404785\n",
      "step 6878, loss: 0.9406111836433411\n",
      "step 6879, loss: 0.8611103296279907\n",
      "step 6880, loss: 1.2472385168075562\n",
      "step 6881, loss: 1.0522557497024536\n",
      "step 6882, loss: 1.134264349937439\n",
      "step 6883, loss: 1.0142661333084106\n",
      "step 6884, loss: 1.0988439321517944\n",
      "step 6885, loss: 1.037843942642212\n",
      "step 6886, loss: 1.0356296300888062\n",
      "step 6887, loss: 1.0635250806808472\n",
      "step 6888, loss: 0.9508751630783081\n",
      "step 6889, loss: 1.1310338973999023\n",
      "step 6890, loss: 1.0756181478500366\n",
      "step 6891, loss: 0.9481726288795471\n",
      "step 6892, loss: 1.1900016069412231\n",
      "step 6893, loss: 1.0553789138793945\n",
      "step 6894, loss: 0.8820076584815979\n",
      "step 6895, loss: 1.020160436630249\n",
      "step 6896, loss: 1.0789133310317993\n",
      "step 6897, loss: 1.0324255228042603\n",
      "step 6898, loss: 1.0250693559646606\n",
      "step 6899, loss: 1.080250859260559\n",
      "step 6900, loss: 0.886807918548584\n",
      "step 6901, loss: 0.8303443789482117\n",
      "step 6902, loss: 1.0038999319076538\n",
      "step 6903, loss: 0.7780622839927673\n",
      "step 6904, loss: 0.65987229347229\n",
      "step 6905, loss: 0.5915253162384033\n",
      "step 6906, loss: 0.7095446586608887\n",
      "step 6907, loss: 0.4615481197834015\n",
      "step 6908, loss: 0.4835047423839569\n",
      "step 6909, loss: 0.4930039048194885\n",
      "step 6910, loss: 0.4958685040473938\n",
      "step 6911, loss: 0.4555642008781433\n",
      "step 6912, loss: 0.4868648052215576\n",
      "step 6913, loss: 0.4802227020263672\n",
      "step 6914, loss: 0.4319007992744446\n",
      "step 6915, loss: 0.4931821823120117\n",
      "step 6916, loss: 0.618768036365509\n",
      "step 6917, loss: 0.5571861863136292\n",
      "step 6918, loss: 0.46869832277297974\n",
      "step 6919, loss: 0.696772038936615\n",
      "step 6920, loss: 0.44582974910736084\n",
      "step 6921, loss: 0.4521158039569855\n",
      "step 6922, loss: 0.5708515048027039\n",
      "step 6923, loss: 0.29224416613578796\n",
      "step 6924, loss: 0.5745510458946228\n",
      "step 6925, loss: 0.7383378148078918\n",
      "step 6926, loss: 0.4984479248523712\n",
      "step 6927, loss: 0.6152352690696716\n",
      "step 6928, loss: 0.48307862877845764\n",
      "step 6929, loss: 0.5268320441246033\n",
      "step 6930, loss: 0.6460989117622375\n",
      "step 6931, loss: 0.6073096990585327\n",
      "step 6932, loss: 0.87722247838974\n",
      "step 6933, loss: 1.21831214427948\n",
      "step 6934, loss: 0.9944707751274109\n",
      "step 6935, loss: 0.9900319576263428\n",
      "step 6936, loss: 1.0394898653030396\n",
      "step 6937, loss: 0.9822065830230713\n",
      "step 6938, loss: 1.0773115158081055\n",
      "step 6939, loss: 1.0893129110336304\n",
      "step 6940, loss: 1.038760781288147\n",
      "step 6941, loss: 0.9334821701049805\n",
      "step 6942, loss: 0.9129140377044678\n",
      "step 6943, loss: 0.9965428709983826\n",
      "step 6944, loss: 0.8275824189186096\n",
      "step 6945, loss: 0.9894116520881653\n",
      "step 6946, loss: 0.9588974714279175\n",
      "step 6947, loss: 1.0482431650161743\n",
      "step 6948, loss: 0.940509021282196\n",
      "step 6949, loss: 1.0650215148925781\n",
      "step 6950, loss: 0.962289571762085\n",
      "step 6951, loss: 0.868602991104126\n",
      "step 6952, loss: 1.0923629999160767\n",
      "step 6953, loss: 0.9137973785400391\n",
      "step 6954, loss: 1.110155463218689\n",
      "step 6955, loss: 1.1390671730041504\n",
      "step 6956, loss: 1.2564674615859985\n",
      "step 6957, loss: 0.9355964660644531\n",
      "step 6958, loss: 1.0847198963165283\n",
      "step 6959, loss: 0.9695019721984863\n",
      "step 6960, loss: 0.9545638561248779\n",
      "step 6961, loss: 1.0376051664352417\n",
      "step 6962, loss: 0.9510138034820557\n",
      "step 6963, loss: 0.9126762747764587\n",
      "step 6964, loss: 1.1061875820159912\n",
      "step 6965, loss: 0.8199770450592041\n",
      "step 6966, loss: 0.9509087204933167\n",
      "step 6967, loss: 1.059869408607483\n",
      "step 6968, loss: 1.1695706844329834\n",
      "step 6969, loss: 1.023459553718567\n",
      "step 6970, loss: 0.9748258590698242\n",
      "step 6971, loss: 0.9733166098594666\n",
      "step 6972, loss: 0.8703555464744568\n",
      "step 6973, loss: 1.134511947631836\n",
      "step 6974, loss: 0.9972206354141235\n",
      "step 6975, loss: 0.8052871227264404\n",
      "step 6976, loss: 1.0584909915924072\n",
      "step 6977, loss: 1.0659096240997314\n",
      "step 6978, loss: 1.148267149925232\n",
      "step 6979, loss: 1.0360965728759766\n",
      "step 6980, loss: 0.9543091058731079\n",
      "step 6981, loss: 0.9707626104354858\n",
      "step 6982, loss: 1.0343239307403564\n",
      "step 6983, loss: 0.8744244575500488\n",
      "step 6984, loss: 1.0156201124191284\n",
      "step 6985, loss: 1.0126922130584717\n",
      "step 6986, loss: 1.1448346376419067\n",
      "step 6987, loss: 1.0161157846450806\n",
      "step 6988, loss: 0.9424930810928345\n",
      "step 6989, loss: 1.1033368110656738\n",
      "step 6990, loss: 1.0127958059310913\n",
      "step 6991, loss: 1.1390751600265503\n",
      "step 6992, loss: 0.8953593373298645\n",
      "step 6993, loss: 1.1481670141220093\n",
      "step 6994, loss: 1.0727486610412598\n",
      "step 6995, loss: 1.0476455688476562\n",
      "step 6996, loss: 1.0459967851638794\n",
      "step 6997, loss: 1.0670284032821655\n",
      "step 6998, loss: 1.1102102994918823\n",
      "step 6999, loss: 1.273179054260254\n",
      "step 7000, loss: 1.0845937728881836\n",
      "step 7001, loss: 1.0387964248657227\n",
      "step 7002, loss: 0.973839521408081\n",
      "step 7003, loss: 0.9691365957260132\n",
      "step 7004, loss: 1.107730746269226\n",
      "step 7005, loss: 1.1193859577178955\n",
      "step 7006, loss: 1.0329577922821045\n",
      "step 7007, loss: 0.9233057498931885\n",
      "step 7008, loss: 1.0903016328811646\n",
      "step 7009, loss: 1.0772813558578491\n",
      "step 7010, loss: 0.9155641794204712\n",
      "step 7011, loss: 0.9381873607635498\n",
      "step 7012, loss: 1.0166966915130615\n",
      "step 7013, loss: 1.0692930221557617\n",
      "step 7014, loss: 0.990659773349762\n",
      "step 7015, loss: 0.9565607309341431\n",
      "step 7016, loss: 0.9478245377540588\n",
      "step 7017, loss: 0.996839702129364\n",
      "step 7018, loss: 1.1032623052597046\n",
      "step 7019, loss: 1.0182511806488037\n",
      "step 7020, loss: 1.1281594038009644\n",
      "step 7021, loss: 0.9218803644180298\n",
      "step 7022, loss: 0.8752628564834595\n",
      "step 7023, loss: 1.2484084367752075\n",
      "step 7024, loss: 0.8833736777305603\n",
      "step 7025, loss: 0.9512873888015747\n",
      "step 7026, loss: 0.816210150718689\n",
      "step 7027, loss: 0.9129965305328369\n",
      "step 7028, loss: 0.6779954433441162\n",
      "step 7029, loss: 0.6971737146377563\n",
      "step 7030, loss: 0.6111299991607666\n",
      "step 7031, loss: 0.6534181833267212\n",
      "step 7032, loss: 0.6335940957069397\n",
      "step 7033, loss: 0.7341692447662354\n",
      "step 7034, loss: 0.7654337286949158\n",
      "step 7035, loss: 0.6284095048904419\n",
      "step 7036, loss: 0.6290386915206909\n",
      "step 7037, loss: 0.5203531384468079\n",
      "step 7038, loss: 0.5888790488243103\n",
      "step 7039, loss: 0.6051369309425354\n",
      "step 7040, loss: 0.54599529504776\n",
      "step 7041, loss: 0.5684818625450134\n",
      "step 7042, loss: 0.5736889243125916\n",
      "step 7043, loss: 0.5243207812309265\n",
      "step 7044, loss: 0.5400748252868652\n",
      "step 7045, loss: 0.6444565057754517\n",
      "step 7046, loss: 0.6038908958435059\n",
      "step 7047, loss: 0.507872462272644\n",
      "step 7048, loss: 0.47458866238594055\n",
      "step 7049, loss: 0.506312370300293\n",
      "step 7050, loss: 0.9185292720794678\n",
      "step 7051, loss: 0.8219822645187378\n",
      "step 7052, loss: 0.9046032428741455\n",
      "step 7053, loss: 0.7257974147796631\n",
      "step 7054, loss: 0.9082889556884766\n",
      "step 7055, loss: 0.7718201279640198\n",
      "step 7056, loss: 0.84524005651474\n",
      "step 7057, loss: 0.8400373458862305\n",
      "step 7058, loss: 0.8276469707489014\n",
      "step 7059, loss: 0.8736200928688049\n",
      "step 7060, loss: 0.8179017305374146\n",
      "step 7061, loss: 0.7766501307487488\n",
      "step 7062, loss: 1.007886290550232\n",
      "step 7063, loss: 1.1597473621368408\n",
      "step 7064, loss: 1.1433541774749756\n",
      "step 7065, loss: 1.0699759721755981\n",
      "step 7066, loss: 1.1057478189468384\n",
      "step 7067, loss: 1.1555051803588867\n",
      "step 7068, loss: 1.0659328699111938\n",
      "step 7069, loss: 1.2038153409957886\n",
      "step 7070, loss: 1.036946177482605\n",
      "step 7071, loss: 1.154126524925232\n",
      "step 7072, loss: 1.057905673980713\n",
      "step 7073, loss: 1.0644125938415527\n",
      "step 7074, loss: 1.1133666038513184\n",
      "step 7075, loss: 1.0748549699783325\n",
      "step 7076, loss: 0.9371796250343323\n",
      "step 7077, loss: 0.9407585859298706\n",
      "step 7078, loss: 1.1760246753692627\n",
      "step 7079, loss: 1.222098708152771\n",
      "step 7080, loss: 0.892452597618103\n",
      "step 7081, loss: 1.012500286102295\n",
      "step 7082, loss: 1.2788618803024292\n",
      "step 7083, loss: 1.0489771366119385\n",
      "step 7084, loss: 0.9862951636314392\n",
      "step 7085, loss: 1.1191308498382568\n",
      "step 7086, loss: 0.9189133644104004\n",
      "step 7087, loss: 1.032641887664795\n",
      "step 7088, loss: 1.1048872470855713\n",
      "step 7089, loss: 1.0964901447296143\n",
      "step 7090, loss: 0.96622234582901\n",
      "step 7091, loss: 1.0717283487319946\n",
      "step 7092, loss: 1.1743292808532715\n",
      "step 7093, loss: 1.164886713027954\n",
      "step 7094, loss: 1.1734254360198975\n",
      "step 7095, loss: 1.4526264667510986\n",
      "step 7096, loss: 1.2757697105407715\n",
      "step 7097, loss: 1.5464662313461304\n",
      "step 7098, loss: 1.200142502784729\n",
      "step 7099, loss: 1.30604088306427\n",
      "step 7100, loss: 1.085633635520935\n",
      "step 7101, loss: 1.2990443706512451\n",
      "step 7102, loss: 0.960382342338562\n",
      "step 7103, loss: 0.8759800791740417\n",
      "step 7104, loss: 0.7840669751167297\n",
      "step 7105, loss: 0.8677348494529724\n",
      "step 7106, loss: 0.9622488021850586\n",
      "step 7107, loss: 1.1407064199447632\n",
      "step 7108, loss: 1.1520460844039917\n",
      "step 7109, loss: 1.2480064630508423\n",
      "step 7110, loss: 1.0153467655181885\n",
      "step 7111, loss: 1.2971595525741577\n",
      "step 7112, loss: 1.0961840152740479\n",
      "step 7113, loss: 1.1331580877304077\n",
      "step 7114, loss: 0.9178980588912964\n",
      "step 7115, loss: 1.0658172369003296\n",
      "step 7116, loss: 1.1730386018753052\n",
      "step 7117, loss: 1.1412262916564941\n",
      "step 7118, loss: 1.1901905536651611\n",
      "step 7119, loss: 1.3545074462890625\n",
      "step 7120, loss: 1.2596304416656494\n",
      "step 7121, loss: 1.2438178062438965\n",
      "step 7122, loss: 1.358913779258728\n",
      "step 7123, loss: 1.2063484191894531\n",
      "step 7124, loss: 0.9595891833305359\n",
      "step 7125, loss: 1.1716536283493042\n",
      "step 7126, loss: 1.1855971813201904\n",
      "step 7127, loss: 1.1020857095718384\n",
      "step 7128, loss: 0.9283391833305359\n",
      "step 7129, loss: 1.113483190536499\n",
      "step 7130, loss: 0.8755268454551697\n",
      "step 7131, loss: 1.0407229661941528\n",
      "step 7132, loss: 1.100936770439148\n",
      "step 7133, loss: 0.965660572052002\n",
      "step 7134, loss: 1.2628241777420044\n",
      "step 7135, loss: 1.1551432609558105\n",
      "step 7136, loss: 1.141204595565796\n",
      "step 7137, loss: 1.077166199684143\n",
      "step 7138, loss: 0.8965778350830078\n",
      "step 7139, loss: 0.856785237789154\n",
      "step 7140, loss: 0.7904334664344788\n",
      "step 7141, loss: 0.8268222808837891\n",
      "step 7142, loss: 0.8017690181732178\n",
      "step 7143, loss: 0.6209535002708435\n",
      "step 7144, loss: 0.5415403246879578\n",
      "step 7145, loss: 0.6306026577949524\n",
      "step 7146, loss: 0.4309682846069336\n",
      "step 7147, loss: 0.5315569043159485\n",
      "step 7148, loss: 0.5976334810256958\n",
      "step 7149, loss: 0.5874258279800415\n",
      "step 7150, loss: 0.6013395190238953\n",
      "step 7151, loss: 0.6053860187530518\n",
      "step 7152, loss: 0.5878922343254089\n",
      "step 7153, loss: 0.5104519724845886\n",
      "step 7154, loss: 0.4861365854740143\n",
      "step 7155, loss: 0.6295510530471802\n",
      "step 7156, loss: 0.5403499603271484\n",
      "step 7157, loss: 0.5326856970787048\n",
      "step 7158, loss: 0.8650655150413513\n",
      "step 7159, loss: 1.0457026958465576\n",
      "step 7160, loss: 1.0593477487564087\n",
      "step 7161, loss: 1.0189577341079712\n",
      "step 7162, loss: 0.9976134300231934\n",
      "step 7163, loss: 1.0165135860443115\n",
      "step 7164, loss: 1.0133225917816162\n",
      "step 7165, loss: 1.0528568029403687\n",
      "step 7166, loss: 0.8573622107505798\n",
      "step 7167, loss: 1.0619004964828491\n",
      "step 7168, loss: 0.9640318155288696\n",
      "step 7169, loss: 0.9489873647689819\n",
      "step 7170, loss: 0.9427740573883057\n",
      "step 7171, loss: 0.9290984272956848\n",
      "step 7172, loss: 1.0162522792816162\n",
      "step 7173, loss: 1.001656413078308\n",
      "step 7174, loss: 0.7681452631950378\n",
      "step 7175, loss: 0.8193100690841675\n",
      "step 7176, loss: 0.9317501783370972\n",
      "step 7177, loss: 1.1383426189422607\n",
      "step 7178, loss: 1.209213137626648\n",
      "step 7179, loss: 1.059098482131958\n",
      "step 7180, loss: 1.072998046875\n",
      "step 7181, loss: 1.0836635828018188\n",
      "step 7182, loss: 1.1181845664978027\n",
      "step 7183, loss: 1.0016144514083862\n",
      "step 7184, loss: 1.1882778406143188\n",
      "step 7185, loss: 1.0178420543670654\n",
      "step 7186, loss: 0.946751058101654\n",
      "step 7187, loss: 0.9501129984855652\n",
      "step 7188, loss: 0.9827185273170471\n",
      "step 7189, loss: 1.0907286405563354\n",
      "step 7190, loss: 1.1210346221923828\n",
      "step 7191, loss: 0.9798792600631714\n",
      "step 7192, loss: 1.029338002204895\n",
      "step 7193, loss: 0.9381499886512756\n",
      "step 7194, loss: 0.8509401679039001\n",
      "step 7195, loss: 0.7633713483810425\n",
      "step 7196, loss: 1.3319834470748901\n",
      "step 7197, loss: 1.1418591737747192\n",
      "step 7198, loss: 0.8979728817939758\n",
      "step 7199, loss: 0.9713131785392761\n",
      "step 7200, loss: 0.9028870463371277\n",
      "step 7201, loss: 0.7724089026451111\n",
      "step 7202, loss: 0.7755038738250732\n",
      "step 7203, loss: 0.3864535987377167\n",
      "step 7204, loss: 0.5455568432807922\n",
      "step 7205, loss: 0.5346671938896179\n",
      "step 7206, loss: 0.4899691939353943\n",
      "step 7207, loss: 0.4224894642829895\n",
      "step 7208, loss: 0.3463082015514374\n",
      "step 7209, loss: 0.36915722489356995\n",
      "step 7210, loss: 0.576845645904541\n",
      "step 7211, loss: 0.40218213200569153\n",
      "step 7212, loss: 0.48424679040908813\n",
      "step 7213, loss: 0.45853638648986816\n",
      "step 7214, loss: 0.5805409550666809\n",
      "step 7215, loss: 0.629710853099823\n",
      "step 7216, loss: 0.6038613319396973\n",
      "step 7217, loss: 0.5717265605926514\n",
      "step 7218, loss: 1.065578818321228\n",
      "step 7219, loss: 0.8759956359863281\n",
      "step 7220, loss: 1.0598936080932617\n",
      "step 7221, loss: 0.9251663088798523\n",
      "step 7222, loss: 1.012429118156433\n",
      "step 7223, loss: 0.9352055788040161\n",
      "step 7224, loss: 0.8447791337966919\n",
      "step 7225, loss: 1.2460931539535522\n",
      "step 7226, loss: 1.064889669418335\n",
      "step 7227, loss: 0.8180937767028809\n",
      "step 7228, loss: 0.8267042636871338\n",
      "step 7229, loss: 0.9092965722084045\n",
      "step 7230, loss: 0.919166088104248\n",
      "step 7231, loss: 0.8551158308982849\n",
      "step 7232, loss: 0.992401123046875\n",
      "step 7233, loss: 1.1198407411575317\n",
      "step 7234, loss: 1.3443357944488525\n",
      "step 7235, loss: 1.063901662826538\n",
      "step 7236, loss: 0.8798388838768005\n",
      "step 7237, loss: 0.8086758852005005\n",
      "step 7238, loss: 1.0415600538253784\n",
      "step 7239, loss: 1.0453968048095703\n",
      "step 7240, loss: 1.095322847366333\n",
      "step 7241, loss: 1.0173395872116089\n",
      "step 7242, loss: 1.119162678718567\n",
      "step 7243, loss: 0.9868279695510864\n",
      "step 7244, loss: 1.147853970527649\n",
      "step 7245, loss: 1.0308042764663696\n",
      "step 7246, loss: 0.9853067994117737\n",
      "step 7247, loss: 1.1037423610687256\n",
      "step 7248, loss: 0.902228832244873\n",
      "step 7249, loss: 1.0654188394546509\n",
      "step 7250, loss: 0.9418991208076477\n",
      "step 7251, loss: 0.9835110902786255\n",
      "step 7252, loss: 1.2160723209381104\n",
      "step 7253, loss: 1.0650144815444946\n",
      "step 7254, loss: 1.1275516748428345\n",
      "step 7255, loss: 1.1939418315887451\n",
      "step 7256, loss: 1.1793100833892822\n",
      "step 7257, loss: 1.1878480911254883\n",
      "step 7258, loss: 1.2681574821472168\n",
      "step 7259, loss: 1.120910406112671\n",
      "step 7260, loss: 0.9929888844490051\n",
      "step 7261, loss: 1.149257779121399\n",
      "step 7262, loss: 1.2329541444778442\n",
      "step 7263, loss: 0.9238652586936951\n",
      "step 7264, loss: 0.9891261458396912\n",
      "step 7265, loss: 1.027885913848877\n",
      "step 7266, loss: 0.9753347635269165\n",
      "step 7267, loss: 1.1697601079940796\n",
      "step 7268, loss: 0.9875518679618835\n",
      "step 7269, loss: 1.2320142984390259\n",
      "step 7270, loss: 1.1122218370437622\n",
      "step 7271, loss: 0.808319091796875\n",
      "step 7272, loss: 1.2295482158660889\n",
      "step 7273, loss: 0.8122540712356567\n",
      "step 7274, loss: 0.9025375247001648\n",
      "step 7275, loss: 0.8495134115219116\n",
      "step 7276, loss: 0.8542580604553223\n",
      "step 7277, loss: 0.8308457136154175\n",
      "step 7278, loss: 0.9602236151695251\n",
      "step 7279, loss: 0.9588268995285034\n",
      "step 7280, loss: 1.2013401985168457\n",
      "step 7281, loss: 0.7435996532440186\n",
      "step 7282, loss: 1.1185559034347534\n",
      "step 7283, loss: 0.8452034592628479\n",
      "step 7284, loss: 1.2231534719467163\n",
      "step 7285, loss: 0.9928929209709167\n",
      "step 7286, loss: 1.0487347841262817\n",
      "step 7287, loss: 0.9298093914985657\n",
      "step 7288, loss: 1.0518547296524048\n",
      "step 7289, loss: 0.9531260132789612\n",
      "step 7290, loss: 1.0820831060409546\n",
      "step 7291, loss: 0.978529155254364\n",
      "step 7292, loss: 0.9868403673171997\n",
      "step 7293, loss: 0.9442341327667236\n",
      "step 7294, loss: 0.8125267028808594\n",
      "step 7295, loss: 0.9347695708274841\n",
      "step 7296, loss: 1.0598156452178955\n",
      "step 7297, loss: 0.9981065392494202\n",
      "step 7298, loss: 1.0573511123657227\n",
      "step 7299, loss: 0.8228374123573303\n",
      "step 7300, loss: 1.0695725679397583\n",
      "step 7301, loss: 0.9530200958251953\n",
      "step 7302, loss: 0.8143897652626038\n",
      "step 7303, loss: 0.8221549391746521\n",
      "step 7304, loss: 0.8406782150268555\n",
      "step 7305, loss: 0.8544649481773376\n",
      "step 7306, loss: 0.820846438407898\n",
      "step 7307, loss: 0.6094064116477966\n",
      "step 7308, loss: 0.5637264251708984\n",
      "step 7309, loss: 0.47397884726524353\n",
      "step 7310, loss: 0.48026347160339355\n",
      "step 7311, loss: 0.4914586842060089\n",
      "step 7312, loss: 0.4866960644721985\n",
      "step 7313, loss: 0.3723105192184448\n",
      "step 7314, loss: 0.49939605593681335\n",
      "step 7315, loss: 0.43903982639312744\n",
      "step 7316, loss: 0.45566752552986145\n",
      "step 7317, loss: 0.42440080642700195\n",
      "step 7318, loss: 0.5068645477294922\n",
      "step 7319, loss: 0.5316432118415833\n",
      "step 7320, loss: 0.4811181128025055\n",
      "step 7321, loss: 0.4074561893939972\n",
      "step 7322, loss: 0.5354530215263367\n",
      "step 7323, loss: 0.39310213923454285\n",
      "step 7324, loss: 0.4995552599430084\n",
      "step 7325, loss: 0.48780393600463867\n",
      "step 7326, loss: 0.3429419994354248\n",
      "step 7327, loss: 0.27919965982437134\n",
      "step 7328, loss: 0.5279986262321472\n",
      "step 7329, loss: 1.0323383808135986\n",
      "step 7330, loss: 0.9959600567817688\n",
      "step 7331, loss: 1.129860281944275\n",
      "step 7332, loss: 0.7557663321495056\n",
      "step 7333, loss: 0.9738874435424805\n",
      "step 7334, loss: 0.938286542892456\n",
      "step 7335, loss: 0.9639580845832825\n",
      "step 7336, loss: 0.8788889646530151\n",
      "step 7337, loss: 0.9749515056610107\n",
      "step 7338, loss: 0.9227827787399292\n",
      "step 7339, loss: 0.8050852417945862\n",
      "step 7340, loss: 0.9533734917640686\n",
      "step 7341, loss: 0.8449419736862183\n",
      "step 7342, loss: 0.9856591820716858\n",
      "step 7343, loss: 0.786824107170105\n",
      "step 7344, loss: 0.7420068383216858\n",
      "step 7345, loss: 1.0563974380493164\n",
      "step 7346, loss: 0.8596550822257996\n",
      "step 7347, loss: 0.8324324488639832\n",
      "step 7348, loss: 0.9616122841835022\n",
      "step 7349, loss: 0.9130876064300537\n",
      "step 7350, loss: 1.079221248626709\n",
      "step 7351, loss: 0.8716399073600769\n",
      "step 7352, loss: 0.952032744884491\n",
      "step 7353, loss: 0.863450288772583\n",
      "step 7354, loss: 0.8738456964492798\n",
      "step 7355, loss: 0.9701314568519592\n",
      "step 7356, loss: 0.9063742756843567\n",
      "step 7357, loss: 0.8990822434425354\n",
      "step 7358, loss: 1.0811482667922974\n",
      "step 7359, loss: 1.0185518264770508\n",
      "step 7360, loss: 0.7385172247886658\n",
      "step 7361, loss: 0.9016512632369995\n",
      "step 7362, loss: 1.0154474973678589\n",
      "step 7363, loss: 0.930751621723175\n",
      "step 7364, loss: 0.912855327129364\n",
      "step 7365, loss: 1.069786787033081\n",
      "step 7366, loss: 1.0581547021865845\n",
      "step 7367, loss: 1.1028242111206055\n",
      "step 7368, loss: 0.7705930471420288\n",
      "step 7369, loss: 0.9700429439544678\n",
      "step 7370, loss: 0.8522640466690063\n",
      "step 7371, loss: 0.824924886226654\n",
      "step 7372, loss: 0.778023362159729\n",
      "step 7373, loss: 1.0306360721588135\n",
      "step 7374, loss: 1.0678777694702148\n",
      "step 7375, loss: 0.9931092858314514\n",
      "step 7376, loss: 0.8806354403495789\n",
      "step 7377, loss: 0.8549564480781555\n",
      "step 7378, loss: 1.171703577041626\n",
      "step 7379, loss: 0.8341101408004761\n",
      "step 7380, loss: 1.035567283630371\n",
      "step 7381, loss: 0.8978537917137146\n",
      "step 7382, loss: 0.9649896025657654\n",
      "step 7383, loss: 0.8221917748451233\n",
      "step 7384, loss: 1.0347585678100586\n",
      "step 7385, loss: 0.989290714263916\n",
      "step 7386, loss: 0.8977519869804382\n",
      "step 7387, loss: 0.8267567157745361\n",
      "step 7388, loss: 0.9830389618873596\n",
      "step 7389, loss: 0.7198159098625183\n",
      "step 7390, loss: 0.7560977339744568\n",
      "step 7391, loss: 0.8479093909263611\n",
      "step 7392, loss: 0.863457977771759\n",
      "step 7393, loss: 1.058735966682434\n",
      "step 7394, loss: 0.8828622698783875\n",
      "step 7395, loss: 0.6818363070487976\n",
      "step 7396, loss: 0.8361139893531799\n",
      "step 7397, loss: 0.9534139633178711\n",
      "step 7398, loss: 0.791298508644104\n",
      "step 7399, loss: 0.8468913435935974\n",
      "step 7400, loss: 0.9771307706832886\n",
      "step 7401, loss: 0.9093222618103027\n",
      "step 7402, loss: 1.000895619392395\n",
      "step 7403, loss: 0.829675018787384\n",
      "step 7404, loss: 0.9042313694953918\n",
      "step 7405, loss: 0.9028245210647583\n",
      "step 7406, loss: 0.8656933903694153\n",
      "step 7407, loss: 1.0118104219436646\n",
      "step 7408, loss: 0.9700833559036255\n",
      "step 7409, loss: 0.9336275458335876\n",
      "step 7410, loss: 0.8571234941482544\n",
      "step 7411, loss: 0.9966294765472412\n",
      "step 7412, loss: 0.8566742539405823\n",
      "step 7413, loss: 1.0158604383468628\n",
      "step 7414, loss: 0.9458356499671936\n",
      "step 7415, loss: 0.888623833656311\n",
      "step 7416, loss: 0.7182028293609619\n",
      "step 7417, loss: 0.9251881837844849\n",
      "step 7418, loss: 0.9265946745872498\n",
      "step 7419, loss: 0.8028920888900757\n",
      "step 7420, loss: 0.9068408012390137\n",
      "step 7421, loss: 0.8067806363105774\n",
      "step 7422, loss: 0.8455614447593689\n",
      "step 7423, loss: 0.9313816428184509\n",
      "step 7424, loss: 1.1846486330032349\n",
      "step 7425, loss: 0.9350689053535461\n",
      "step 7426, loss: 0.8977617025375366\n",
      "step 7427, loss: 1.1976274251937866\n",
      "step 7428, loss: 0.896706223487854\n",
      "step 7429, loss: 0.8478153944015503\n",
      "step 7430, loss: 0.9210808277130127\n",
      "step 7431, loss: 0.8347364664077759\n",
      "step 7432, loss: 0.7985994219779968\n",
      "step 7433, loss: 0.7468084096908569\n",
      "step 7434, loss: 0.8554417490959167\n",
      "step 7435, loss: 1.0565770864486694\n",
      "step 7436, loss: 0.9982320070266724\n",
      "step 7437, loss: 1.005661964416504\n",
      "step 7438, loss: 0.8610529899597168\n",
      "step 7439, loss: 0.5213631987571716\n",
      "step 7440, loss: 0.3947420120239258\n",
      "step 7441, loss: 0.40276825428009033\n",
      "step 7442, loss: 0.4052380323410034\n",
      "step 7443, loss: 0.494951993227005\n",
      "step 7444, loss: 0.4420957565307617\n",
      "step 7445, loss: 0.5640153884887695\n",
      "step 7446, loss: 0.5581386685371399\n",
      "step 7447, loss: 0.466074138879776\n",
      "step 7448, loss: 0.4002027213573456\n",
      "step 7449, loss: 0.4403553307056427\n",
      "step 7450, loss: 0.46757936477661133\n",
      "step 7451, loss: 0.49563857913017273\n",
      "step 7452, loss: 0.4047408401966095\n",
      "step 7453, loss: 0.3292749226093292\n",
      "step 7454, loss: 0.46167925000190735\n",
      "step 7455, loss: 0.4067729413509369\n",
      "step 7456, loss: 0.48630213737487793\n",
      "step 7457, loss: 0.5099974870681763\n",
      "step 7458, loss: 0.4414208233356476\n",
      "step 7459, loss: 0.3771871328353882\n",
      "step 7460, loss: 0.46137887239456177\n",
      "step 7461, loss: 0.42556843161582947\n",
      "step 7462, loss: 0.35686981678009033\n",
      "step 7463, loss: 0.45494312047958374\n",
      "step 7464, loss: 0.3605380654335022\n",
      "step 7465, loss: 0.40534576773643494\n",
      "step 7466, loss: 0.5202012062072754\n",
      "step 7467, loss: 0.3537573516368866\n",
      "step 7468, loss: 0.8934497237205505\n",
      "step 7469, loss: 0.8282817006111145\n",
      "step 7470, loss: 0.7933794856071472\n",
      "step 7471, loss: 0.8973550200462341\n",
      "step 7472, loss: 0.6730085015296936\n",
      "step 7473, loss: 0.9363138675689697\n",
      "step 7474, loss: 0.8645226955413818\n",
      "step 7475, loss: 1.002042293548584\n",
      "step 7476, loss: 1.0151827335357666\n",
      "step 7477, loss: 0.9091070890426636\n",
      "step 7478, loss: 0.8558592200279236\n",
      "step 7479, loss: 0.9537951946258545\n",
      "step 7480, loss: 0.8332998156547546\n",
      "step 7481, loss: 0.8662915825843811\n",
      "step 7482, loss: 0.8891416788101196\n",
      "step 7483, loss: 1.0151660442352295\n",
      "step 7484, loss: 0.9619691371917725\n",
      "step 7485, loss: 0.7097412347793579\n",
      "step 7486, loss: 0.8196777105331421\n",
      "step 7487, loss: 0.7930586338043213\n",
      "step 7488, loss: 0.876715898513794\n",
      "step 7489, loss: 0.9770584106445312\n",
      "step 7490, loss: 0.8842720985412598\n",
      "step 7491, loss: 0.9776724576950073\n",
      "step 7492, loss: 0.8689867258071899\n",
      "step 7493, loss: 0.9752936959266663\n",
      "step 7494, loss: 0.9056501984596252\n",
      "step 7495, loss: 0.8724585175514221\n",
      "step 7496, loss: 0.9513058066368103\n",
      "step 7497, loss: 0.8221522569656372\n",
      "step 7498, loss: 0.9254156351089478\n",
      "step 7499, loss: 0.9254289269447327\n",
      "step 7500, loss: 0.8269264698028564\n",
      "step 7501, loss: 0.9447821378707886\n",
      "step 7502, loss: 0.7880682945251465\n",
      "step 7503, loss: 0.9664625525474548\n",
      "step 7504, loss: 0.9116870760917664\n",
      "step 7505, loss: 0.840338945388794\n",
      "step 7506, loss: 0.9915452599525452\n",
      "step 7507, loss: 1.0644481182098389\n",
      "step 7508, loss: 0.9496088027954102\n",
      "step 7509, loss: 1.0364975929260254\n",
      "step 7510, loss: 1.006943702697754\n",
      "step 7511, loss: 0.9041470885276794\n",
      "step 7512, loss: 1.0825144052505493\n",
      "step 7513, loss: 1.157076120376587\n",
      "step 7514, loss: 1.0858904123306274\n",
      "step 7515, loss: 0.8129823207855225\n",
      "step 7516, loss: 1.0769740343093872\n",
      "step 7517, loss: 0.9724754095077515\n",
      "step 7518, loss: 1.0980360507965088\n",
      "step 7519, loss: 0.8946863412857056\n",
      "step 7520, loss: 0.8046257495880127\n",
      "step 7521, loss: 0.8920223116874695\n",
      "step 7522, loss: 0.9398139715194702\n",
      "step 7523, loss: 0.8861203193664551\n",
      "step 7524, loss: 0.8229710459709167\n",
      "step 7525, loss: 1.0425881147384644\n",
      "step 7526, loss: 1.0822563171386719\n",
      "step 7527, loss: 1.0445713996887207\n",
      "step 7528, loss: 0.9624488949775696\n",
      "step 7529, loss: 0.9945251941680908\n",
      "step 7530, loss: 0.7939079999923706\n",
      "step 7531, loss: 0.9298941493034363\n",
      "step 7532, loss: 1.1273396015167236\n",
      "step 7533, loss: 0.9986664652824402\n",
      "step 7534, loss: 1.0164121389389038\n",
      "step 7535, loss: 0.9885208606719971\n",
      "step 7536, loss: 0.9453234672546387\n",
      "step 7537, loss: 1.0170273780822754\n",
      "step 7538, loss: 0.9987685680389404\n",
      "step 7539, loss: 0.7990739941596985\n",
      "step 7540, loss: 0.8038575053215027\n",
      "step 7541, loss: 0.8868916034698486\n",
      "step 7542, loss: 0.9185411334037781\n",
      "step 7543, loss: 0.9170782566070557\n",
      "step 7544, loss: 0.9404338002204895\n",
      "step 7545, loss: 1.0463576316833496\n",
      "step 7546, loss: 0.9725030660629272\n",
      "step 7547, loss: 0.8760389685630798\n",
      "step 7548, loss: 0.8965675234794617\n",
      "step 7549, loss: 1.0836519002914429\n",
      "step 7550, loss: 0.9063721895217896\n",
      "step 7551, loss: 0.826832115650177\n",
      "step 7552, loss: 0.8605272769927979\n",
      "step 7553, loss: 0.9383957386016846\n",
      "step 7554, loss: 0.9204604029655457\n",
      "step 7555, loss: 1.3208686113357544\n",
      "step 7556, loss: 0.9260313510894775\n",
      "step 7557, loss: 0.9651575088500977\n",
      "step 7558, loss: 1.0980654954910278\n",
      "step 7559, loss: 0.9375426173210144\n",
      "step 7560, loss: 0.981864869594574\n",
      "step 7561, loss: 1.0120635032653809\n",
      "step 7562, loss: 0.8970814943313599\n",
      "step 7563, loss: 0.875647246837616\n",
      "step 7564, loss: 0.9273953437805176\n",
      "step 7565, loss: 1.131448745727539\n",
      "step 7566, loss: 1.0218509435653687\n",
      "step 7567, loss: 0.9928386807441711\n",
      "step 7568, loss: 1.0144133567810059\n",
      "step 7569, loss: 0.9568119645118713\n",
      "step 7570, loss: 0.9128368496894836\n",
      "step 7571, loss: 0.9548013806343079\n",
      "step 7572, loss: 1.0483367443084717\n",
      "step 7573, loss: 1.2475064992904663\n",
      "step 7574, loss: 0.9353640079498291\n",
      "step 7575, loss: 0.9126201272010803\n",
      "step 7576, loss: 0.8872037529945374\n",
      "step 7577, loss: 1.144498348236084\n",
      "step 7578, loss: 0.930792510509491\n",
      "step 7579, loss: 0.8959410786628723\n",
      "step 7580, loss: 0.973289430141449\n",
      "step 7581, loss: 1.0179187059402466\n",
      "step 7582, loss: 1.0935813188552856\n",
      "step 7583, loss: 0.903466522693634\n",
      "step 7584, loss: 0.8570706844329834\n",
      "step 7585, loss: 0.8969559073448181\n",
      "step 7586, loss: 1.0870729684829712\n",
      "step 7587, loss: 0.9878324866294861\n",
      "step 7588, loss: 0.9654267430305481\n",
      "step 7589, loss: 1.2765285968780518\n",
      "step 7590, loss: 1.0149699449539185\n",
      "step 7591, loss: 1.1499276161193848\n",
      "step 7592, loss: 1.0797796249389648\n",
      "step 7593, loss: 1.1202460527420044\n",
      "step 7594, loss: 1.0318970680236816\n",
      "step 7595, loss: 1.0108556747436523\n",
      "step 7596, loss: 0.8998558521270752\n",
      "step 7597, loss: 0.9825557470321655\n",
      "step 7598, loss: 0.9471152424812317\n",
      "step 7599, loss: 0.9026764631271362\n",
      "step 7600, loss: 0.9835846424102783\n",
      "step 7601, loss: 0.8565964102745056\n",
      "step 7602, loss: 1.099056363105774\n",
      "step 7603, loss: 0.8582336902618408\n",
      "step 7604, loss: 1.0493760108947754\n",
      "step 7605, loss: 0.9530792236328125\n",
      "step 7606, loss: 0.9713823199272156\n",
      "step 7607, loss: 0.9803885817527771\n",
      "step 7608, loss: 0.9964024424552917\n",
      "step 7609, loss: 1.0235408544540405\n",
      "step 7610, loss: 1.1290593147277832\n",
      "step 7611, loss: 1.1670117378234863\n",
      "step 7612, loss: 1.1084468364715576\n",
      "step 7613, loss: 0.9101352095603943\n",
      "step 7614, loss: 1.0225815773010254\n",
      "step 7615, loss: 0.9144733548164368\n",
      "step 7616, loss: 0.7660303711891174\n",
      "step 7617, loss: 1.0414063930511475\n",
      "step 7618, loss: 1.0263798236846924\n",
      "step 7619, loss: 0.9975674152374268\n",
      "step 7620, loss: 0.9878149032592773\n",
      "step 7621, loss: 0.9545724391937256\n",
      "step 7622, loss: 0.9517929553985596\n",
      "step 7623, loss: 0.9952573776245117\n",
      "step 7624, loss: 0.910607635974884\n",
      "step 7625, loss: 0.8602370619773865\n",
      "step 7626, loss: 0.8539775013923645\n",
      "step 7627, loss: 0.7588359713554382\n",
      "step 7628, loss: 0.8715271949768066\n",
      "step 7629, loss: 0.8550419211387634\n",
      "step 7630, loss: 1.0452040433883667\n",
      "step 7631, loss: 0.797531247138977\n",
      "step 7632, loss: 1.1168407201766968\n",
      "step 7633, loss: 0.8116308450698853\n",
      "step 7634, loss: 0.7981746792793274\n",
      "step 7635, loss: 0.8428293466567993\n",
      "step 7636, loss: 0.8821759223937988\n",
      "step 7637, loss: 0.8449870347976685\n",
      "step 7638, loss: 0.8173775672912598\n",
      "step 7639, loss: 0.7332733869552612\n",
      "step 7640, loss: 0.889652669429779\n",
      "step 7641, loss: 0.9058901071548462\n",
      "step 7642, loss: 1.1241583824157715\n",
      "step 7643, loss: 0.6704487204551697\n",
      "step 7644, loss: 0.9396852850914001\n",
      "step 7645, loss: 0.8165066242218018\n",
      "step 7646, loss: 0.6566700339317322\n",
      "step 7647, loss: 0.7666783928871155\n",
      "step 7648, loss: 0.8540821075439453\n",
      "step 7649, loss: 0.732398509979248\n",
      "step 7650, loss: 0.8559818863868713\n",
      "step 7651, loss: 0.6737450361251831\n",
      "step 7652, loss: 0.4967285692691803\n",
      "step 7653, loss: 0.4464586079120636\n",
      "step 7654, loss: 0.49552783370018005\n",
      "step 7655, loss: 0.6333591938018799\n",
      "step 7656, loss: 0.38894662261009216\n",
      "step 7657, loss: 0.5157119631767273\n",
      "step 7658, loss: 0.4979821443557739\n",
      "step 7659, loss: 0.43433892726898193\n",
      "step 7660, loss: 0.4408418834209442\n",
      "step 7661, loss: 0.4579757750034332\n",
      "step 7662, loss: 0.4195834994316101\n",
      "step 7663, loss: 0.45221227407455444\n",
      "step 7664, loss: 0.42484673857688904\n",
      "step 7665, loss: 0.3959757685661316\n",
      "step 7666, loss: 0.32802048325538635\n",
      "step 7667, loss: 0.45230042934417725\n",
      "step 7668, loss: 0.44019946455955505\n",
      "step 7669, loss: 0.3066709339618683\n",
      "step 7670, loss: 0.596349835395813\n",
      "step 7671, loss: 0.40222644805908203\n",
      "step 7672, loss: 0.37653085589408875\n",
      "step 7673, loss: 0.3252154588699341\n",
      "step 7674, loss: 0.48389771580696106\n",
      "step 7675, loss: 0.36670368909835815\n",
      "step 7676, loss: 0.43369340896606445\n",
      "step 7677, loss: 0.4071817100048065\n",
      "step 7678, loss: 0.351273775100708\n",
      "step 7679, loss: 0.4344049096107483\n",
      "step 7680, loss: 0.41723716259002686\n",
      "step 7681, loss: 0.32163330912590027\n",
      "step 7682, loss: 0.6428487300872803\n",
      "step 7683, loss: 0.8797218799591064\n",
      "step 7684, loss: 1.019081711769104\n",
      "step 7685, loss: 0.9608029127120972\n",
      "step 7686, loss: 0.6769393682479858\n",
      "step 7687, loss: 0.9291576147079468\n",
      "step 7688, loss: 0.9976873397827148\n",
      "step 7689, loss: 0.8720992803573608\n",
      "step 7690, loss: 0.8861258625984192\n",
      "step 7691, loss: 0.7180736660957336\n",
      "step 7692, loss: 0.8405075073242188\n",
      "step 7693, loss: 0.8746514320373535\n",
      "step 7694, loss: 0.7767336368560791\n",
      "step 7695, loss: 0.703199565410614\n",
      "step 7696, loss: 0.9495213031768799\n",
      "step 7697, loss: 0.8976110219955444\n",
      "step 7698, loss: 0.75164794921875\n",
      "step 7699, loss: 0.9557021260261536\n",
      "step 7700, loss: 0.8131868839263916\n",
      "step 7701, loss: 0.794745147228241\n",
      "step 7702, loss: 0.9739938974380493\n",
      "step 7703, loss: 0.7257458567619324\n",
      "step 7704, loss: 0.8433682918548584\n",
      "step 7705, loss: 0.7559956908226013\n",
      "step 7706, loss: 0.7216655611991882\n",
      "step 7707, loss: 0.7740512490272522\n",
      "step 7708, loss: 0.6725823283195496\n",
      "step 7709, loss: 0.6778830289840698\n",
      "step 7710, loss: 0.6763197183609009\n",
      "step 7711, loss: 0.6555163264274597\n",
      "step 7712, loss: 0.7394421100616455\n",
      "step 7713, loss: 0.9161787033081055\n",
      "step 7714, loss: 0.911026656627655\n",
      "step 7715, loss: 0.6456530690193176\n",
      "step 7716, loss: 0.7830665111541748\n",
      "step 7717, loss: 0.7442951798439026\n",
      "step 7718, loss: 0.7685531377792358\n",
      "step 7719, loss: 0.661780834197998\n",
      "step 7720, loss: 0.7320714592933655\n",
      "step 7721, loss: 0.6117761731147766\n",
      "step 7722, loss: 0.624911367893219\n",
      "step 7723, loss: 0.5103701949119568\n",
      "step 7724, loss: 0.45231249928474426\n",
      "step 7725, loss: 0.6218088269233704\n",
      "step 7726, loss: 0.3735843002796173\n",
      "step 7727, loss: 0.4579530358314514\n",
      "step 7728, loss: 0.4877373278141022\n",
      "step 7729, loss: 0.4855571985244751\n",
      "step 7730, loss: 0.6436035633087158\n",
      "step 7731, loss: 0.5392966866493225\n",
      "step 7732, loss: 0.44470736384391785\n",
      "step 7733, loss: 0.43112891912460327\n",
      "step 7734, loss: 0.3979322910308838\n",
      "step 7735, loss: 0.35117048025131226\n",
      "step 7736, loss: 1.0567651987075806\n",
      "step 7737, loss: 0.9923751950263977\n",
      "step 7738, loss: 0.9805281758308411\n",
      "step 7739, loss: 0.9511405825614929\n",
      "step 7740, loss: 1.0002042055130005\n",
      "step 7741, loss: 0.9266261458396912\n",
      "step 7742, loss: 0.8196361064910889\n",
      "step 7743, loss: 0.9558129906654358\n",
      "step 7744, loss: 0.7399744391441345\n",
      "step 7745, loss: 1.053300142288208\n",
      "step 7746, loss: 0.7532511949539185\n",
      "step 7747, loss: 0.9141644835472107\n",
      "step 7748, loss: 0.9517580270767212\n",
      "step 7749, loss: 1.0259442329406738\n",
      "step 7750, loss: 1.1497361660003662\n",
      "step 7751, loss: 1.0574429035186768\n",
      "step 7752, loss: 1.0425015687942505\n",
      "step 7753, loss: 0.8307999968528748\n",
      "step 7754, loss: 1.2289655208587646\n",
      "step 7755, loss: 1.044162392616272\n",
      "step 7756, loss: 1.086708664894104\n",
      "step 7757, loss: 0.9837010502815247\n",
      "step 7758, loss: 0.9001721739768982\n",
      "step 7759, loss: 1.0236923694610596\n",
      "step 7760, loss: 1.0968763828277588\n",
      "step 7761, loss: 1.1395336389541626\n",
      "step 7762, loss: 0.8536989092826843\n",
      "step 7763, loss: 1.0647557973861694\n",
      "step 7764, loss: 1.0160576105117798\n",
      "step 7765, loss: 0.8794195652008057\n",
      "step 7766, loss: 0.9468201994895935\n",
      "step 7767, loss: 1.1758304834365845\n",
      "step 7768, loss: 0.8114639520645142\n",
      "step 7769, loss: 1.043458342552185\n",
      "step 7770, loss: 0.9817614555358887\n",
      "step 7771, loss: 0.8686450719833374\n",
      "step 7772, loss: 0.9686666131019592\n",
      "step 7773, loss: 0.9513974785804749\n",
      "step 7774, loss: 0.9022997617721558\n",
      "step 7775, loss: 0.8185638189315796\n",
      "step 7776, loss: 0.9426607489585876\n",
      "step 7777, loss: 0.8562835454940796\n",
      "step 7778, loss: 1.075681209564209\n",
      "step 7779, loss: 1.1709383726119995\n",
      "step 7780, loss: 0.8131118416786194\n",
      "step 7781, loss: 0.9979079961776733\n",
      "step 7782, loss: 0.9122481346130371\n",
      "step 7783, loss: 0.9718606472015381\n",
      "step 7784, loss: 0.8248947262763977\n",
      "step 7785, loss: 0.8539942502975464\n",
      "step 7786, loss: 0.9685498476028442\n",
      "step 7787, loss: 1.0010054111480713\n",
      "step 7788, loss: 0.815403163433075\n",
      "step 7789, loss: 0.8494282960891724\n",
      "step 7790, loss: 0.8981429934501648\n",
      "step 7791, loss: 0.7814997434616089\n",
      "step 7792, loss: 0.8342674374580383\n",
      "step 7793, loss: 0.8899946808815002\n",
      "step 7794, loss: 1.1508926153182983\n",
      "step 7795, loss: 0.9509451389312744\n",
      "step 7796, loss: 0.9089531898498535\n",
      "step 7797, loss: 0.859403133392334\n",
      "step 7798, loss: 0.990699291229248\n",
      "step 7799, loss: 0.9358940720558167\n",
      "step 7800, loss: 1.0698448419570923\n",
      "step 7801, loss: 1.056461215019226\n",
      "step 7802, loss: 0.9213016033172607\n",
      "step 7803, loss: 1.0323561429977417\n",
      "step 7804, loss: 1.0837550163269043\n",
      "step 7805, loss: 1.113586664199829\n",
      "step 7806, loss: 0.9754520058631897\n",
      "step 7807, loss: 0.8954900503158569\n",
      "step 7808, loss: 0.8856008052825928\n",
      "step 7809, loss: 1.0483418703079224\n",
      "step 7810, loss: 1.0949269533157349\n",
      "step 7811, loss: 1.1530264616012573\n",
      "step 7812, loss: 1.0534076690673828\n",
      "step 7813, loss: 1.0032495260238647\n",
      "step 7814, loss: 0.984393835067749\n",
      "step 7815, loss: 0.8719804286956787\n",
      "step 7816, loss: 0.7684856653213501\n",
      "step 7817, loss: 0.9226661324501038\n",
      "step 7818, loss: 0.9179627895355225\n",
      "step 7819, loss: 0.9011383652687073\n",
      "step 7820, loss: 0.957057535648346\n",
      "step 7821, loss: 0.8689188361167908\n",
      "step 7822, loss: 0.7644699215888977\n",
      "step 7823, loss: 0.976470947265625\n",
      "step 7824, loss: 0.9177905917167664\n",
      "step 7825, loss: 0.8751493692398071\n",
      "step 7826, loss: 0.5694222450256348\n",
      "step 7827, loss: 0.5971574187278748\n",
      "step 7828, loss: 0.6007750630378723\n",
      "step 7829, loss: 0.5887239575386047\n",
      "step 7830, loss: 0.5486842393875122\n",
      "step 7831, loss: 0.5344634652137756\n",
      "step 7832, loss: 0.5146535038948059\n",
      "step 7833, loss: 0.6235044002532959\n",
      "step 7834, loss: 0.4984601140022278\n",
      "step 7835, loss: 0.6345657706260681\n",
      "step 7836, loss: 0.46742650866508484\n",
      "step 7837, loss: 0.5125719904899597\n",
      "step 7838, loss: 0.46755966544151306\n",
      "step 7839, loss: 0.49612778425216675\n",
      "step 7840, loss: 0.6039502024650574\n",
      "step 7841, loss: 0.8906184434890747\n",
      "step 7842, loss: 0.9167330861091614\n",
      "step 7843, loss: 0.8540672063827515\n",
      "step 7844, loss: 0.8654636144638062\n",
      "step 7845, loss: 0.7205421924591064\n",
      "step 7846, loss: 0.9347832202911377\n",
      "step 7847, loss: 0.8120126128196716\n",
      "step 7848, loss: 0.8463215827941895\n",
      "step 7849, loss: 0.8213820457458496\n",
      "step 7850, loss: 0.9133872389793396\n",
      "step 7851, loss: 0.8616234660148621\n",
      "step 7852, loss: 0.8570489883422852\n",
      "step 7853, loss: 0.9126237034797668\n",
      "step 7854, loss: 1.0051301717758179\n",
      "step 7855, loss: 0.7307751774787903\n",
      "step 7856, loss: 0.7234018445014954\n",
      "step 7857, loss: 0.9483908414840698\n",
      "step 7858, loss: 0.8389517068862915\n",
      "step 7859, loss: 0.675948441028595\n",
      "step 7860, loss: 0.7932119965553284\n",
      "step 7861, loss: 0.8425499200820923\n",
      "step 7862, loss: 0.9095692038536072\n",
      "step 7863, loss: 0.8555230498313904\n",
      "step 7864, loss: 0.9292578101158142\n",
      "step 7865, loss: 0.8701035976409912\n",
      "step 7866, loss: 0.9023224711418152\n",
      "step 7867, loss: 0.892369270324707\n",
      "step 7868, loss: 0.841538667678833\n",
      "step 7869, loss: 0.8207013010978699\n",
      "step 7870, loss: 0.8173070549964905\n",
      "step 7871, loss: 0.9285638332366943\n",
      "step 7872, loss: 1.0409884452819824\n",
      "step 7873, loss: 0.9787625670433044\n",
      "step 7874, loss: 0.8987470865249634\n",
      "step 7875, loss: 0.7348740100860596\n",
      "step 7876, loss: 0.941944420337677\n",
      "step 7877, loss: 0.8512395024299622\n",
      "step 7878, loss: 0.9474505186080933\n",
      "step 7879, loss: 0.8841010928153992\n",
      "step 7880, loss: 0.8032931685447693\n",
      "step 7881, loss: 0.8335168361663818\n",
      "step 7882, loss: 0.9157344698905945\n",
      "step 7883, loss: 0.9496150612831116\n",
      "step 7884, loss: 0.7418266534805298\n",
      "step 7885, loss: 0.946661651134491\n",
      "step 7886, loss: 0.6615456938743591\n",
      "step 7887, loss: 0.8134125471115112\n",
      "step 7888, loss: 0.8905813097953796\n",
      "step 7889, loss: 0.7141169905662537\n",
      "step 7890, loss: 0.666410505771637\n",
      "step 7891, loss: 0.8020794987678528\n",
      "step 7892, loss: 0.9033265113830566\n",
      "step 7893, loss: 0.8603164553642273\n",
      "step 7894, loss: 0.8013187050819397\n",
      "step 7895, loss: 0.7237725853919983\n",
      "step 7896, loss: 0.7403818368911743\n",
      "step 7897, loss: 0.8451588153839111\n",
      "step 7898, loss: 0.6025429964065552\n",
      "step 7899, loss: 0.5805602669715881\n",
      "step 7900, loss: 0.522782027721405\n",
      "step 7901, loss: 0.5226362347602844\n",
      "step 7902, loss: 0.3990364074707031\n",
      "step 7903, loss: 0.46815991401672363\n",
      "step 7904, loss: 0.4347958564758301\n",
      "step 7905, loss: 0.4805392026901245\n",
      "step 7906, loss: 0.4744681715965271\n",
      "step 7907, loss: 0.6165805459022522\n",
      "step 7908, loss: 0.4990096986293793\n",
      "step 7909, loss: 0.4061860144138336\n",
      "step 7910, loss: 0.37857508659362793\n",
      "step 7911, loss: 0.6401553153991699\n",
      "step 7912, loss: 0.7618881464004517\n",
      "step 7913, loss: 0.9427167773246765\n",
      "step 7914, loss: 0.8006280064582825\n",
      "step 7915, loss: 0.720602810382843\n",
      "step 7916, loss: 0.7073763608932495\n",
      "step 7917, loss: 0.815679132938385\n",
      "step 7918, loss: 0.7901648283004761\n",
      "step 7919, loss: 0.7628296613693237\n",
      "step 7920, loss: 0.6313085556030273\n",
      "step 7921, loss: 0.8088168501853943\n",
      "step 7922, loss: 0.7169889807701111\n",
      "step 7923, loss: 0.8346446752548218\n",
      "step 7924, loss: 0.6773971319198608\n",
      "step 7925, loss: 0.9139107465744019\n",
      "step 7926, loss: 0.8666272759437561\n",
      "step 7927, loss: 0.9232351183891296\n",
      "step 7928, loss: 0.831607460975647\n",
      "step 7929, loss: 0.8704777956008911\n",
      "step 7930, loss: 0.8711888194084167\n",
      "step 7931, loss: 0.7986102104187012\n",
      "step 7932, loss: 0.8811248540878296\n",
      "step 7933, loss: 0.8899746537208557\n",
      "step 7934, loss: 0.9571993947029114\n",
      "step 7935, loss: 0.8162606954574585\n",
      "step 7936, loss: 0.8929341435432434\n",
      "step 7937, loss: 0.8137277960777283\n",
      "step 7938, loss: 0.9118592739105225\n",
      "step 7939, loss: 0.9810142517089844\n",
      "step 7940, loss: 0.876634955406189\n",
      "step 7941, loss: 0.8776326775550842\n",
      "step 7942, loss: 0.8175405859947205\n",
      "step 7943, loss: 0.8548263907432556\n",
      "step 7944, loss: 0.8970818519592285\n",
      "step 7945, loss: 0.8742726445198059\n",
      "step 7946, loss: 1.0034085512161255\n",
      "step 7947, loss: 0.9208921194076538\n",
      "step 7948, loss: 0.8059731721878052\n",
      "step 7949, loss: 1.164501667022705\n",
      "step 7950, loss: 0.9523940086364746\n",
      "step 7951, loss: 0.9566008448600769\n",
      "step 7952, loss: 0.8727257251739502\n",
      "step 7953, loss: 0.8322718739509583\n",
      "step 7954, loss: 0.7988376617431641\n",
      "step 7955, loss: 1.1397439241409302\n",
      "step 7956, loss: 0.9216819405555725\n",
      "step 7957, loss: 0.7988867163658142\n",
      "step 7958, loss: 1.014765977859497\n",
      "step 7959, loss: 0.9471696019172668\n",
      "step 7960, loss: 0.9927890300750732\n",
      "step 7961, loss: 1.1325931549072266\n",
      "step 7962, loss: 0.9336560964584351\n",
      "step 7963, loss: 0.9200498461723328\n",
      "step 7964, loss: 1.141676902770996\n",
      "step 7965, loss: 0.9332268834114075\n",
      "step 7966, loss: 0.978135347366333\n",
      "step 7967, loss: 0.9323233366012573\n",
      "step 7968, loss: 0.8310655355453491\n",
      "step 7969, loss: 0.939060389995575\n",
      "step 7970, loss: 1.0005921125411987\n",
      "step 7971, loss: 0.7462819218635559\n",
      "step 7972, loss: 0.851905107498169\n",
      "step 7973, loss: 0.8452889919281006\n",
      "step 7974, loss: 1.0760756731033325\n",
      "step 7975, loss: 0.833296537399292\n",
      "step 7976, loss: 0.8455607295036316\n",
      "step 7977, loss: 0.9126038551330566\n",
      "step 7978, loss: 0.7826478481292725\n",
      "step 7979, loss: 1.032543420791626\n",
      "step 7980, loss: 0.7257839441299438\n",
      "step 7981, loss: 0.725369393825531\n",
      "step 7982, loss: 0.7904103994369507\n",
      "step 7983, loss: 0.781619668006897\n",
      "step 7984, loss: 0.9815220832824707\n",
      "step 7985, loss: 0.7710148692131042\n",
      "step 7986, loss: 0.8194490075111389\n",
      "step 7987, loss: 0.7584229111671448\n",
      "step 7988, loss: 0.723903238773346\n",
      "step 7989, loss: 0.8380390405654907\n",
      "step 7990, loss: 0.9506975412368774\n",
      "step 7991, loss: 0.7759002447128296\n",
      "step 7992, loss: 0.9962795972824097\n",
      "step 7993, loss: 0.8945484757423401\n",
      "step 7994, loss: 0.893145740032196\n",
      "step 7995, loss: 0.876727283000946\n",
      "step 7996, loss: 0.723850429058075\n",
      "step 7997, loss: 0.8076589107513428\n",
      "step 7998, loss: 0.81668621301651\n",
      "step 7999, loss: 0.7819998860359192\n",
      "step 8000, loss: 0.806370198726654\n",
      "step 8001, loss: 0.8726668953895569\n",
      "step 8002, loss: 1.1506282091140747\n",
      "step 8003, loss: 0.7770105600357056\n",
      "step 8004, loss: 0.9467026591300964\n",
      "step 8005, loss: 0.7929912209510803\n",
      "step 8006, loss: 0.8912492990493774\n",
      "step 8007, loss: 0.9367204904556274\n",
      "step 8008, loss: 0.9184880256652832\n",
      "step 8009, loss: 0.8472472429275513\n",
      "step 8010, loss: 0.8902470469474792\n",
      "step 8011, loss: 0.6864991784095764\n",
      "step 8012, loss: 0.9677364826202393\n",
      "step 8013, loss: 0.702422559261322\n",
      "step 8014, loss: 0.7107639908790588\n",
      "step 8015, loss: 0.7457741498947144\n",
      "step 8016, loss: 0.7392163276672363\n",
      "step 8017, loss: 0.8658348321914673\n",
      "step 8018, loss: 0.7629455924034119\n",
      "step 8019, loss: 0.7932053804397583\n",
      "step 8020, loss: 0.8134734034538269\n",
      "step 8021, loss: 0.6919904947280884\n",
      "step 8022, loss: 0.7568678259849548\n",
      "step 8023, loss: 0.83983314037323\n",
      "step 8024, loss: 0.6526944637298584\n",
      "step 8025, loss: 0.5855479836463928\n",
      "step 8026, loss: 0.4402359127998352\n",
      "step 8027, loss: 0.36539530754089355\n",
      "step 8028, loss: 0.42352819442749023\n",
      "step 8029, loss: 0.40213119983673096\n",
      "step 8030, loss: 0.4653606116771698\n",
      "step 8031, loss: 0.3176648020744324\n",
      "step 8032, loss: 0.3676908314228058\n",
      "step 8033, loss: 0.35124364495277405\n",
      "step 8034, loss: 0.4093113839626312\n",
      "step 8035, loss: 0.48141324520111084\n",
      "step 8036, loss: 0.47757232189178467\n",
      "step 8037, loss: 0.4125075340270996\n",
      "step 8038, loss: 0.28493982553482056\n",
      "step 8039, loss: 0.4441215395927429\n",
      "step 8040, loss: 0.49267804622650146\n",
      "step 8041, loss: 0.5796807408332825\n",
      "step 8042, loss: 0.38863644003868103\n",
      "step 8043, loss: 0.38874879479408264\n",
      "step 8044, loss: 0.4985188841819763\n",
      "step 8045, loss: 0.454568475484848\n",
      "step 8046, loss: 0.373038113117218\n",
      "step 8047, loss: 0.486788809299469\n",
      "step 8048, loss: 0.4554767608642578\n",
      "step 8049, loss: 0.3670685291290283\n",
      "step 8050, loss: 0.4717943072319031\n",
      "step 8051, loss: 0.3237435519695282\n",
      "step 8052, loss: 0.8279626369476318\n",
      "step 8053, loss: 1.0226110219955444\n",
      "step 8054, loss: 0.868132472038269\n",
      "step 8055, loss: 0.8113043308258057\n",
      "step 8056, loss: 0.8855842351913452\n",
      "step 8057, loss: 0.7882805466651917\n",
      "step 8058, loss: 0.7923906445503235\n",
      "step 8059, loss: 0.8232420682907104\n",
      "step 8060, loss: 0.8719216585159302\n",
      "step 8061, loss: 0.7894660830497742\n",
      "step 8062, loss: 0.7216200232505798\n",
      "step 8063, loss: 0.6992626190185547\n",
      "step 8064, loss: 0.7649234533309937\n",
      "step 8065, loss: 0.8786770105361938\n",
      "step 8066, loss: 0.8015018701553345\n",
      "step 8067, loss: 0.8716092705726624\n",
      "step 8068, loss: 0.7625821828842163\n",
      "step 8069, loss: 0.8101860284805298\n",
      "step 8070, loss: 0.7998769879341125\n",
      "step 8071, loss: 0.9335520267486572\n",
      "step 8072, loss: 0.8177424073219299\n",
      "step 8073, loss: 0.8086676001548767\n",
      "step 8074, loss: 0.9079954624176025\n",
      "step 8075, loss: 0.8728588223457336\n",
      "step 8076, loss: 0.921934187412262\n",
      "step 8077, loss: 0.7873873710632324\n",
      "step 8078, loss: 0.781505286693573\n",
      "step 8079, loss: 0.8927470445632935\n",
      "step 8080, loss: 0.7958008050918579\n",
      "step 8081, loss: 0.8198972344398499\n",
      "step 8082, loss: 0.9509888291358948\n",
      "step 8083, loss: 0.8540032505989075\n",
      "step 8084, loss: 0.890572726726532\n",
      "step 8085, loss: 0.6354846358299255\n",
      "step 8086, loss: 0.7289310097694397\n",
      "step 8087, loss: 0.8472293019294739\n",
      "step 8088, loss: 0.8735567331314087\n",
      "step 8089, loss: 0.8142445087432861\n",
      "step 8090, loss: 0.8435680270195007\n",
      "step 8091, loss: 0.7150708436965942\n",
      "step 8092, loss: 0.7478930950164795\n",
      "step 8093, loss: 0.8145333528518677\n",
      "step 8094, loss: 0.8663356304168701\n",
      "step 8095, loss: 0.7074145078659058\n",
      "step 8096, loss: 0.7915388345718384\n",
      "step 8097, loss: 0.8808815479278564\n",
      "step 8098, loss: 1.040022373199463\n",
      "step 8099, loss: 0.7896773219108582\n",
      "step 8100, loss: 0.8778074383735657\n",
      "step 8101, loss: 0.7575757503509521\n",
      "step 8102, loss: 0.940901517868042\n",
      "step 8103, loss: 0.707077145576477\n",
      "step 8104, loss: 0.8435954451560974\n",
      "step 8105, loss: 1.0070995092391968\n",
      "step 8106, loss: 0.9605271816253662\n",
      "step 8107, loss: 0.8191794753074646\n",
      "step 8108, loss: 0.753804624080658\n",
      "step 8109, loss: 0.7833946347236633\n",
      "step 8110, loss: 0.9924430251121521\n",
      "step 8111, loss: 0.8218456506729126\n",
      "step 8112, loss: 0.7962504029273987\n",
      "step 8113, loss: 0.8265541195869446\n",
      "step 8114, loss: 0.7580641508102417\n",
      "step 8115, loss: 0.8400900959968567\n",
      "step 8116, loss: 0.7606976628303528\n",
      "step 8117, loss: 0.8567646741867065\n",
      "step 8118, loss: 0.9428151249885559\n",
      "step 8119, loss: 1.0539989471435547\n",
      "step 8120, loss: 1.1125324964523315\n",
      "step 8121, loss: 0.9473616480827332\n",
      "step 8122, loss: 0.8547052145004272\n",
      "step 8123, loss: 0.9631927013397217\n",
      "step 8124, loss: 0.905484676361084\n",
      "step 8125, loss: 0.9409163594245911\n",
      "step 8126, loss: 0.9086188673973083\n",
      "step 8127, loss: 0.7797693014144897\n",
      "step 8128, loss: 0.8946374654769897\n",
      "step 8129, loss: 0.9756056666374207\n",
      "step 8130, loss: 0.6805592775344849\n",
      "step 8131, loss: 0.8258371949195862\n",
      "step 8132, loss: 0.791405200958252\n",
      "step 8133, loss: 0.9057393670082092\n",
      "step 8134, loss: 0.811444103717804\n",
      "step 8135, loss: 0.6863566637039185\n",
      "step 8136, loss: 0.800666868686676\n",
      "step 8137, loss: 0.8845775723457336\n",
      "step 8138, loss: 0.8525002002716064\n",
      "step 8139, loss: 0.7929250597953796\n",
      "step 8140, loss: 0.693130612373352\n",
      "step 8141, loss: 0.8385087847709656\n",
      "step 8142, loss: 0.7681786417961121\n",
      "step 8143, loss: 0.8281633853912354\n",
      "step 8144, loss: 0.7741334438323975\n",
      "step 8145, loss: 0.758395254611969\n",
      "step 8146, loss: 0.5953336358070374\n",
      "step 8147, loss: 0.623647928237915\n",
      "step 8148, loss: 0.6865754723548889\n",
      "step 8149, loss: 0.5679039359092712\n",
      "step 8150, loss: 0.44122546911239624\n",
      "step 8151, loss: 0.3885095417499542\n",
      "step 8152, loss: 0.5644755363464355\n",
      "step 8153, loss: 0.5679966807365417\n",
      "step 8154, loss: 0.5387022495269775\n",
      "step 8155, loss: 0.5403720736503601\n",
      "step 8156, loss: 0.5522162914276123\n",
      "step 8157, loss: 0.4555213153362274\n",
      "step 8158, loss: 0.4868997633457184\n",
      "step 8159, loss: 0.6018655300140381\n",
      "step 8160, loss: 0.4678545892238617\n",
      "step 8161, loss: 0.3906393349170685\n",
      "step 8162, loss: 0.4586735963821411\n",
      "step 8163, loss: 0.49919408559799194\n",
      "step 8164, loss: 0.5383935570716858\n",
      "step 8165, loss: 0.5034148097038269\n",
      "step 8166, loss: 0.4416116774082184\n",
      "step 8167, loss: 0.4138409495353699\n",
      "step 8168, loss: 0.4099079370498657\n",
      "step 8169, loss: 0.39691483974456787\n",
      "step 8170, loss: 0.6732684969902039\n",
      "step 8171, loss: 0.6274780035018921\n",
      "step 8172, loss: 0.6621894240379333\n",
      "step 8173, loss: 0.6133783459663391\n",
      "step 8174, loss: 0.9040768146514893\n",
      "step 8175, loss: 0.7559715509414673\n",
      "step 8176, loss: 0.7182580828666687\n",
      "step 8177, loss: 0.6525900363922119\n",
      "step 8178, loss: 0.8425710201263428\n",
      "step 8179, loss: 0.8152961730957031\n",
      "step 8180, loss: 0.7290248274803162\n",
      "step 8181, loss: 0.5722472071647644\n",
      "step 8182, loss: 0.7549248337745667\n",
      "step 8183, loss: 0.747757613658905\n",
      "step 8184, loss: 0.7935410141944885\n",
      "step 8185, loss: 0.7643311023712158\n",
      "step 8186, loss: 0.8212790489196777\n",
      "step 8187, loss: 0.8693209886550903\n",
      "step 8188, loss: 0.7822662591934204\n",
      "step 8189, loss: 1.006238341331482\n",
      "step 8190, loss: 0.7897213697433472\n",
      "step 8191, loss: 0.8679505586624146\n",
      "step 8192, loss: 0.8356016278266907\n",
      "step 8193, loss: 0.8244913220405579\n",
      "step 8194, loss: 0.8010903000831604\n",
      "step 8195, loss: 0.8817294836044312\n",
      "step 8196, loss: 0.8876092433929443\n",
      "step 8197, loss: 0.8436340689659119\n",
      "step 8198, loss: 0.9856842756271362\n",
      "step 8199, loss: 1.0218297243118286\n",
      "step 8200, loss: 0.9476037621498108\n",
      "step 8201, loss: 0.8632345199584961\n",
      "step 8202, loss: 0.8861718773841858\n",
      "step 8203, loss: 0.9283878803253174\n",
      "step 8204, loss: 0.7173961997032166\n",
      "step 8205, loss: 0.9296230673789978\n",
      "step 8206, loss: 0.8908474445343018\n",
      "step 8207, loss: 0.8374117612838745\n",
      "step 8208, loss: 0.9211062788963318\n",
      "step 8209, loss: 0.9353369474411011\n",
      "step 8210, loss: 0.8487005233764648\n",
      "step 8211, loss: 0.8271946310997009\n",
      "step 8212, loss: 0.7871236801147461\n",
      "step 8213, loss: 0.9373541474342346\n",
      "step 8214, loss: 0.9057323336601257\n",
      "step 8215, loss: 1.1539314985275269\n",
      "step 8216, loss: 0.9687143564224243\n",
      "step 8217, loss: 1.0104780197143555\n",
      "step 8218, loss: 1.085602879524231\n",
      "step 8219, loss: 0.9920235276222229\n",
      "step 8220, loss: 0.8216619491577148\n",
      "step 8221, loss: 1.1775915622711182\n",
      "step 8222, loss: 0.9763121008872986\n",
      "step 8223, loss: 0.8091562390327454\n",
      "step 8224, loss: 0.7729594707489014\n",
      "step 8225, loss: 0.757061243057251\n",
      "step 8226, loss: 1.20029616355896\n",
      "step 8227, loss: 0.8007960319519043\n",
      "step 8228, loss: 0.9893351197242737\n",
      "step 8229, loss: 0.8096810579299927\n",
      "step 8230, loss: 0.8718776702880859\n",
      "step 8231, loss: 0.9444877505302429\n",
      "step 8232, loss: 1.0147333145141602\n",
      "step 8233, loss: 1.0282212495803833\n",
      "step 8234, loss: 0.7768234610557556\n",
      "step 8235, loss: 0.7861821055412292\n",
      "step 8236, loss: 0.981947124004364\n",
      "step 8237, loss: 0.99322909116745\n",
      "step 8238, loss: 0.9768235087394714\n",
      "step 8239, loss: 0.9165393710136414\n",
      "step 8240, loss: 1.0092772245407104\n",
      "step 8241, loss: 1.0077099800109863\n",
      "step 8242, loss: 1.0746010541915894\n",
      "step 8243, loss: 1.0598326921463013\n",
      "step 8244, loss: 0.7442563772201538\n",
      "step 8245, loss: 0.860727846622467\n",
      "step 8246, loss: 0.7298299074172974\n",
      "step 8247, loss: 0.93043053150177\n",
      "step 8248, loss: 0.8486486077308655\n",
      "step 8249, loss: 1.0452150106430054\n",
      "step 8250, loss: 0.9734375476837158\n",
      "step 8251, loss: 0.7713096141815186\n",
      "step 8252, loss: 0.859940767288208\n",
      "step 8253, loss: 0.8560632467269897\n",
      "step 8254, loss: 0.9987301826477051\n",
      "step 8255, loss: 1.0208454132080078\n",
      "step 8256, loss: 1.0283740758895874\n",
      "step 8257, loss: 0.8189532160758972\n",
      "step 8258, loss: 0.7072250247001648\n",
      "step 8259, loss: 0.6453738808631897\n",
      "step 8260, loss: 0.40499669313430786\n",
      "step 8261, loss: 0.5185755491256714\n",
      "step 8262, loss: 0.6564216613769531\n",
      "step 8263, loss: 0.5352611541748047\n",
      "step 8264, loss: 0.5895493626594543\n",
      "step 8265, loss: 0.5475184321403503\n",
      "step 8266, loss: 0.5116125345230103\n",
      "step 8267, loss: 0.5125153064727783\n",
      "step 8268, loss: 0.44699627161026\n",
      "step 8269, loss: 0.5589068531990051\n",
      "step 8270, loss: 0.5125506520271301\n",
      "step 8271, loss: 0.45342063903808594\n",
      "step 8272, loss: 0.3507298231124878\n",
      "step 8273, loss: 0.4319669306278229\n",
      "step 8274, loss: 0.3242812156677246\n",
      "step 8275, loss: 0.43949925899505615\n",
      "step 8276, loss: 0.4159855246543884\n",
      "step 8277, loss: 0.5759313106536865\n",
      "step 8278, loss: 0.7266232967376709\n",
      "step 8279, loss: 0.8003244400024414\n",
      "step 8280, loss: 0.8238318562507629\n",
      "step 8281, loss: 0.8738201856613159\n",
      "step 8282, loss: 0.9077591300010681\n",
      "step 8283, loss: 0.8547173142433167\n",
      "step 8284, loss: 0.7518075704574585\n",
      "step 8285, loss: 0.8776722550392151\n",
      "step 8286, loss: 0.589990496635437\n",
      "step 8287, loss: 0.9164640307426453\n",
      "step 8288, loss: 0.7694208025932312\n",
      "step 8289, loss: 0.7867568731307983\n",
      "step 8290, loss: 0.7319090366363525\n",
      "step 8291, loss: 0.6689746975898743\n",
      "step 8292, loss: 0.7127391695976257\n",
      "step 8293, loss: 0.5621390342712402\n",
      "step 8294, loss: 0.6244616508483887\n",
      "step 8295, loss: 0.8679199814796448\n",
      "step 8296, loss: 0.7786034941673279\n",
      "step 8297, loss: 0.837636411190033\n",
      "step 8298, loss: 0.9220854043960571\n",
      "step 8299, loss: 0.8581801056861877\n",
      "step 8300, loss: 0.9199998378753662\n",
      "step 8301, loss: 1.117910623550415\n",
      "step 8302, loss: 0.8708444237709045\n",
      "step 8303, loss: 0.8720356225967407\n",
      "step 8304, loss: 0.9763105511665344\n",
      "step 8305, loss: 0.8458312749862671\n",
      "step 8306, loss: 0.7338252067565918\n",
      "step 8307, loss: 0.8927774429321289\n",
      "step 8308, loss: 0.876007616519928\n",
      "step 8309, loss: 1.0601232051849365\n",
      "step 8310, loss: 0.7791486978530884\n",
      "step 8311, loss: 0.8971489667892456\n",
      "step 8312, loss: 1.0160866975784302\n",
      "step 8313, loss: 0.7772573232650757\n",
      "step 8314, loss: 0.7787622809410095\n",
      "step 8315, loss: 0.7673640847206116\n",
      "step 8316, loss: 0.9623465538024902\n",
      "step 8317, loss: 0.9259616136550903\n",
      "step 8318, loss: 0.7717845439910889\n",
      "step 8319, loss: 0.9101957082748413\n",
      "step 8320, loss: 0.7687810659408569\n",
      "step 8321, loss: 0.720422625541687\n",
      "step 8322, loss: 0.5742453932762146\n",
      "step 8323, loss: 0.5166754722595215\n",
      "step 8324, loss: 0.4316798746585846\n",
      "step 8325, loss: 0.4779409170150757\n",
      "step 8326, loss: 0.4844624400138855\n",
      "step 8327, loss: 0.42865297198295593\n",
      "step 8328, loss: 0.3432461619377136\n",
      "step 8329, loss: 0.3699621856212616\n",
      "step 8330, loss: 0.3974166512489319\n",
      "step 8331, loss: 0.3233446776866913\n",
      "step 8332, loss: 0.3744763731956482\n",
      "step 8333, loss: 0.3360803425312042\n",
      "step 8334, loss: 0.40428560972213745\n",
      "step 8335, loss: 0.4730735123157501\n",
      "step 8336, loss: 0.4246567487716675\n",
      "step 8337, loss: 0.44647514820098877\n",
      "step 8338, loss: 0.7964783310890198\n",
      "step 8339, loss: 0.8436915278434753\n",
      "step 8340, loss: 0.7743291854858398\n",
      "step 8341, loss: 0.7166456580162048\n",
      "step 8342, loss: 0.8664044737815857\n",
      "step 8343, loss: 0.7956742644309998\n",
      "step 8344, loss: 0.6503726243972778\n",
      "step 8345, loss: 0.8894202709197998\n",
      "step 8346, loss: 0.6894129514694214\n",
      "step 8347, loss: 0.6578428149223328\n",
      "step 8348, loss: 0.8320112824440002\n",
      "step 8349, loss: 0.8134352564811707\n",
      "step 8350, loss: 0.76497882604599\n",
      "step 8351, loss: 0.8136895298957825\n",
      "step 8352, loss: 0.8451526761054993\n",
      "step 8353, loss: 0.783546507358551\n",
      "step 8354, loss: 0.7301148772239685\n",
      "step 8355, loss: 0.658281147480011\n",
      "step 8356, loss: 0.8211972713470459\n",
      "step 8357, loss: 0.8903599381446838\n",
      "step 8358, loss: 0.9564071297645569\n",
      "step 8359, loss: 0.9973199963569641\n",
      "step 8360, loss: 0.9345557689666748\n",
      "step 8361, loss: 0.9491005539894104\n",
      "step 8362, loss: 0.7952674627304077\n",
      "step 8363, loss: 0.773877739906311\n",
      "step 8364, loss: 0.9205334782600403\n",
      "step 8365, loss: 1.034153938293457\n",
      "step 8366, loss: 0.8595696687698364\n",
      "step 8367, loss: 1.0337036848068237\n",
      "step 8368, loss: 0.8699773550033569\n",
      "step 8369, loss: 0.7605308890342712\n",
      "step 8370, loss: 0.853054940700531\n",
      "step 8371, loss: 0.7870937585830688\n",
      "step 8372, loss: 0.9190085530281067\n",
      "step 8373, loss: 0.7450343370437622\n",
      "step 8374, loss: 0.9151912927627563\n",
      "step 8375, loss: 0.9572104215621948\n",
      "step 8376, loss: 1.172723412513733\n",
      "step 8377, loss: 0.9393019080162048\n",
      "step 8378, loss: 0.9765426516532898\n",
      "step 8379, loss: 0.8392571210861206\n",
      "step 8380, loss: 0.9555344581604004\n",
      "step 8381, loss: 0.8680658340454102\n",
      "step 8382, loss: 0.8953216671943665\n",
      "step 8383, loss: 0.7547823190689087\n",
      "step 8384, loss: 0.7230746150016785\n",
      "step 8385, loss: 0.9317665696144104\n",
      "step 8386, loss: 0.8220617175102234\n",
      "step 8387, loss: 0.9022294878959656\n",
      "step 8388, loss: 0.8511898517608643\n",
      "step 8389, loss: 1.088253378868103\n",
      "step 8390, loss: 0.8789098262786865\n",
      "step 8391, loss: 0.7536802291870117\n",
      "step 8392, loss: 0.8652141690254211\n",
      "step 8393, loss: 0.7297050356864929\n",
      "step 8394, loss: 0.7243635058403015\n",
      "step 8395, loss: 0.7564660310745239\n",
      "step 8396, loss: 0.6813828349113464\n",
      "step 8397, loss: 0.7819010019302368\n",
      "step 8398, loss: 0.9229035973548889\n",
      "step 8399, loss: 0.79641193151474\n",
      "step 8400, loss: 0.9311456680297852\n",
      "step 8401, loss: 0.8070982694625854\n",
      "step 8402, loss: 0.8756878972053528\n",
      "step 8403, loss: 0.8276689648628235\n",
      "step 8404, loss: 0.951721727848053\n",
      "step 8405, loss: 0.8033036589622498\n",
      "step 8406, loss: 0.9494079351425171\n",
      "step 8407, loss: 0.9406615495681763\n",
      "step 8408, loss: 0.785058319568634\n",
      "step 8409, loss: 0.866827130317688\n",
      "step 8410, loss: 0.9080532789230347\n",
      "step 8411, loss: 0.89793461561203\n",
      "step 8412, loss: 0.8083005547523499\n",
      "step 8413, loss: 0.8786871433258057\n",
      "step 8414, loss: 0.7632843852043152\n",
      "step 8415, loss: 0.6005936861038208\n",
      "step 8416, loss: 0.9454838037490845\n",
      "step 8417, loss: 0.858177125453949\n",
      "step 8418, loss: 0.7681741714477539\n",
      "step 8419, loss: 0.7054371237754822\n",
      "step 8420, loss: 0.929598331451416\n",
      "step 8421, loss: 0.844966471195221\n",
      "step 8422, loss: 0.7849205136299133\n",
      "step 8423, loss: 0.8540598154067993\n",
      "step 8424, loss: 0.8271721601486206\n",
      "step 8425, loss: 0.6562846899032593\n",
      "step 8426, loss: 0.7308579683303833\n",
      "step 8427, loss: 0.4498017132282257\n",
      "step 8428, loss: 0.45717766880989075\n",
      "step 8429, loss: 0.5442485213279724\n",
      "step 8430, loss: 0.4375620484352112\n",
      "step 8431, loss: 0.37490931153297424\n",
      "step 8432, loss: 0.36022087931632996\n",
      "step 8433, loss: 0.4108564853668213\n",
      "step 8434, loss: 0.3620583713054657\n",
      "step 8435, loss: 0.4663301706314087\n",
      "step 8436, loss: 0.28645938634872437\n",
      "step 8437, loss: 0.3730914294719696\n",
      "step 8438, loss: 0.2881745398044586\n",
      "step 8439, loss: 0.4649415612220764\n",
      "step 8440, loss: 0.48579999804496765\n",
      "step 8441, loss: 0.46630874276161194\n",
      "step 8442, loss: 0.5231336951255798\n",
      "step 8443, loss: 0.33465656638145447\n",
      "step 8444, loss: 0.4180581867694855\n",
      "step 8445, loss: 0.45330458879470825\n",
      "step 8446, loss: 0.44320058822631836\n",
      "step 8447, loss: 0.26619330048561096\n",
      "step 8448, loss: 0.3985976278781891\n",
      "step 8449, loss: 0.8943701982498169\n",
      "step 8450, loss: 0.8157517313957214\n",
      "step 8451, loss: 0.8120254278182983\n",
      "step 8452, loss: 0.6261733770370483\n",
      "step 8453, loss: 0.6885513663291931\n",
      "step 8454, loss: 0.9458264708518982\n",
      "step 8455, loss: 0.712253987789154\n",
      "step 8456, loss: 0.6400625109672546\n",
      "step 8457, loss: 0.6386867165565491\n",
      "step 8458, loss: 0.74930340051651\n",
      "step 8459, loss: 0.7174398303031921\n",
      "step 8460, loss: 0.7681354880332947\n",
      "step 8461, loss: 0.6567270159721375\n",
      "step 8462, loss: 0.7394219636917114\n",
      "step 8463, loss: 0.6052608489990234\n",
      "step 8464, loss: 0.756554126739502\n",
      "step 8465, loss: 0.7670952677726746\n",
      "step 8466, loss: 0.7909342646598816\n",
      "step 8467, loss: 0.721412181854248\n",
      "step 8468, loss: 0.8259520530700684\n",
      "step 8469, loss: 0.7297113537788391\n",
      "step 8470, loss: 0.8593770265579224\n",
      "step 8471, loss: 0.7045719623565674\n",
      "step 8472, loss: 0.9059686064720154\n",
      "step 8473, loss: 0.7648177742958069\n",
      "step 8474, loss: 0.8136630654335022\n",
      "step 8475, loss: 0.7131245732307434\n",
      "step 8476, loss: 0.5861412882804871\n",
      "step 8477, loss: 0.7528523206710815\n",
      "step 8478, loss: 0.736953616142273\n",
      "step 8479, loss: 0.7510848641395569\n",
      "step 8480, loss: 0.7520965337753296\n",
      "step 8481, loss: 0.9399842619895935\n",
      "step 8482, loss: 0.7345486283302307\n",
      "step 8483, loss: 0.6804872155189514\n",
      "step 8484, loss: 0.785731315612793\n",
      "step 8485, loss: 0.6941820383071899\n",
      "step 8486, loss: 0.7641986012458801\n",
      "step 8487, loss: 0.8224608302116394\n",
      "step 8488, loss: 0.6806021332740784\n",
      "step 8489, loss: 0.8539823293685913\n",
      "step 8490, loss: 0.7187370657920837\n",
      "step 8491, loss: 0.6630424857139587\n",
      "step 8492, loss: 0.8510115146636963\n",
      "step 8493, loss: 0.8533689975738525\n",
      "step 8494, loss: 0.7823355793952942\n",
      "step 8495, loss: 0.83517986536026\n",
      "step 8496, loss: 0.7078610062599182\n",
      "step 8497, loss: 0.8399771451950073\n",
      "step 8498, loss: 0.7838044166564941\n",
      "step 8499, loss: 0.7968724370002747\n",
      "step 8500, loss: 0.7517284750938416\n",
      "step 8501, loss: 0.8170560598373413\n",
      "step 8502, loss: 0.7114849090576172\n",
      "step 8503, loss: 0.7912517786026001\n",
      "step 8504, loss: 0.7103828191757202\n",
      "step 8505, loss: 0.8406507968902588\n",
      "step 8506, loss: 0.7814280986785889\n",
      "step 8507, loss: 0.6899542212486267\n",
      "step 8508, loss: 0.8898898959159851\n",
      "step 8509, loss: 0.8184221386909485\n",
      "step 8510, loss: 0.7372537851333618\n",
      "step 8511, loss: 0.7160767316818237\n",
      "step 8512, loss: 0.6739925742149353\n",
      "step 8513, loss: 0.7813772559165955\n",
      "step 8514, loss: 0.7058773636817932\n",
      "step 8515, loss: 0.6575547456741333\n",
      "step 8516, loss: 0.7169713377952576\n",
      "step 8517, loss: 0.8649758100509644\n",
      "step 8518, loss: 0.4838450849056244\n",
      "step 8519, loss: 0.7098126411437988\n",
      "step 8520, loss: 0.6984692811965942\n",
      "step 8521, loss: 0.7651602625846863\n",
      "step 8522, loss: 0.6974785923957825\n",
      "step 8523, loss: 0.5618020296096802\n",
      "step 8524, loss: 0.7138965725898743\n",
      "step 8525, loss: 0.6514623761177063\n",
      "step 8526, loss: 0.7510012984275818\n",
      "step 8527, loss: 0.8287663459777832\n",
      "step 8528, loss: 0.7898566126823425\n",
      "step 8529, loss: 0.9053015112876892\n",
      "step 8530, loss: 0.8009708523750305\n",
      "step 8531, loss: 0.8168087005615234\n",
      "step 8532, loss: 0.6247771978378296\n",
      "step 8533, loss: 0.7856176495552063\n",
      "step 8534, loss: 0.7952703237533569\n",
      "step 8535, loss: 0.6976661086082458\n",
      "step 8536, loss: 0.7808680534362793\n",
      "step 8537, loss: 0.8119120001792908\n",
      "step 8538, loss: 0.7329311370849609\n",
      "step 8539, loss: 0.6706563234329224\n",
      "step 8540, loss: 0.6269758939743042\n",
      "step 8541, loss: 0.8205986618995667\n",
      "step 8542, loss: 0.8509282469749451\n",
      "step 8543, loss: 0.6812002658843994\n",
      "step 8544, loss: 0.7007097005844116\n",
      "step 8545, loss: 0.6954295635223389\n",
      "step 8546, loss: 0.7221309542655945\n",
      "step 8547, loss: 0.9392217993736267\n",
      "step 8548, loss: 0.7569615244865417\n",
      "step 8549, loss: 0.7656411528587341\n",
      "step 8550, loss: 0.6440979838371277\n",
      "step 8551, loss: 0.7010347843170166\n",
      "step 8552, loss: 0.5393078327178955\n",
      "step 8553, loss: 0.5549784898757935\n",
      "step 8554, loss: 0.7239536046981812\n",
      "step 8555, loss: 0.8155699372291565\n",
      "step 8556, loss: 0.7527912259101868\n",
      "step 8557, loss: 0.8437415361404419\n",
      "step 8558, loss: 0.6138383150100708\n",
      "step 8559, loss: 0.3818707764148712\n",
      "step 8560, loss: 0.37758275866508484\n",
      "step 8561, loss: 0.4986840784549713\n",
      "step 8562, loss: 0.36392125487327576\n",
      "step 8563, loss: 0.5437363982200623\n",
      "step 8564, loss: 0.40900132060050964\n",
      "step 8565, loss: 0.4912925362586975\n",
      "step 8566, loss: 0.46519577503204346\n",
      "step 8567, loss: 0.3305210471153259\n",
      "step 8568, loss: 0.41039398312568665\n",
      "step 8569, loss: 0.3295727074146271\n",
      "step 8570, loss: 0.3570845425128937\n",
      "step 8571, loss: 0.3429860770702362\n",
      "step 8572, loss: 0.37621763348579407\n",
      "step 8573, loss: 0.29868900775909424\n",
      "step 8574, loss: 0.43667861819267273\n",
      "step 8575, loss: 0.36442455649375916\n",
      "step 8576, loss: 0.36066824197769165\n",
      "step 8577, loss: 0.5551653504371643\n",
      "step 8578, loss: 0.36209556460380554\n",
      "step 8579, loss: 0.5025429129600525\n",
      "step 8580, loss: 0.4042266011238098\n",
      "step 8581, loss: 0.4254213571548462\n",
      "step 8582, loss: 0.29735803604125977\n",
      "step 8583, loss: 0.37385591864585876\n",
      "step 8584, loss: 0.43094387650489807\n",
      "step 8585, loss: 0.3829804062843323\n",
      "step 8586, loss: 0.3900556266307831\n",
      "step 8587, loss: 0.35211145877838135\n",
      "step 8588, loss: 0.7048506140708923\n",
      "step 8589, loss: 0.7433686852455139\n",
      "step 8590, loss: 0.6667265892028809\n",
      "step 8591, loss: 0.6869540214538574\n",
      "step 8592, loss: 0.621752917766571\n",
      "step 8593, loss: 0.763150691986084\n",
      "step 8594, loss: 0.7350397109985352\n",
      "step 8595, loss: 0.6459603905677795\n",
      "step 8596, loss: 0.7572141289710999\n",
      "step 8597, loss: 0.7445001602172852\n",
      "step 8598, loss: 0.7972109317779541\n",
      "step 8599, loss: 0.6814687848091125\n",
      "step 8600, loss: 0.7490110993385315\n",
      "step 8601, loss: 0.7566704750061035\n",
      "step 8602, loss: 0.8024472594261169\n",
      "step 8603, loss: 0.670785129070282\n",
      "step 8604, loss: 0.720435380935669\n",
      "step 8605, loss: 0.6795820593833923\n",
      "step 8606, loss: 0.8574764728546143\n",
      "step 8607, loss: 0.6182618737220764\n",
      "step 8608, loss: 0.7484996318817139\n",
      "step 8609, loss: 0.6201484799385071\n",
      "step 8610, loss: 0.5939923524856567\n",
      "step 8611, loss: 0.7376389503479004\n",
      "step 8612, loss: 0.6792063117027283\n",
      "step 8613, loss: 0.7045795917510986\n",
      "step 8614, loss: 0.7804264426231384\n",
      "step 8615, loss: 0.7905627489089966\n",
      "step 8616, loss: 0.651282787322998\n",
      "step 8617, loss: 0.8783544301986694\n",
      "step 8618, loss: 0.7872684597969055\n",
      "step 8619, loss: 0.7680834531784058\n",
      "step 8620, loss: 0.7419689893722534\n",
      "step 8621, loss: 0.7649014592170715\n",
      "step 8622, loss: 0.8274665474891663\n",
      "step 8623, loss: 0.7593276500701904\n",
      "step 8624, loss: 0.7923799157142639\n",
      "step 8625, loss: 0.7456780672073364\n",
      "step 8626, loss: 0.7354261875152588\n",
      "step 8627, loss: 0.8277912139892578\n",
      "step 8628, loss: 0.9584088325500488\n",
      "step 8629, loss: 0.7380256056785583\n",
      "step 8630, loss: 0.779417872428894\n",
      "step 8631, loss: 0.8540711402893066\n",
      "step 8632, loss: 0.6984929442405701\n",
      "step 8633, loss: 0.7203702926635742\n",
      "step 8634, loss: 0.8854126930236816\n",
      "step 8635, loss: 0.778630793094635\n",
      "step 8636, loss: 0.8666008114814758\n",
      "step 8637, loss: 0.8104040622711182\n",
      "step 8638, loss: 0.7574161887168884\n",
      "step 8639, loss: 0.9165657162666321\n",
      "step 8640, loss: 0.8551632165908813\n",
      "step 8641, loss: 0.7064169645309448\n",
      "step 8642, loss: 0.9267178773880005\n",
      "step 8643, loss: 0.7597671151161194\n",
      "step 8644, loss: 0.6631035208702087\n",
      "step 8645, loss: 0.7187935709953308\n",
      "step 8646, loss: 0.6376920938491821\n",
      "step 8647, loss: 0.9154579639434814\n",
      "step 8648, loss: 0.9969099164009094\n",
      "step 8649, loss: 0.8976767063140869\n",
      "step 8650, loss: 0.6814794540405273\n",
      "step 8651, loss: 0.6971082091331482\n",
      "step 8652, loss: 0.8397630453109741\n",
      "step 8653, loss: 0.7312026023864746\n",
      "step 8654, loss: 0.8143802285194397\n",
      "step 8655, loss: 0.7701285481452942\n",
      "step 8656, loss: 0.8283125162124634\n",
      "step 8657, loss: 0.8333119750022888\n",
      "step 8658, loss: 0.6182960271835327\n",
      "step 8659, loss: 0.7759152054786682\n",
      "step 8660, loss: 0.9226537942886353\n",
      "step 8661, loss: 0.8353788256645203\n",
      "step 8662, loss: 0.8235399723052979\n",
      "step 8663, loss: 0.8387624621391296\n",
      "step 8664, loss: 0.6641002297401428\n",
      "step 8665, loss: 0.8251693844795227\n",
      "step 8666, loss: 0.8633453845977783\n",
      "step 8667, loss: 0.8792179226875305\n",
      "step 8668, loss: 0.7803426384925842\n",
      "step 8669, loss: 0.6180168986320496\n",
      "step 8670, loss: 0.747369647026062\n",
      "step 8671, loss: 0.8586839437484741\n",
      "step 8672, loss: 0.8049682974815369\n",
      "step 8673, loss: 0.7283391356468201\n",
      "step 8674, loss: 0.9212650656700134\n",
      "step 8675, loss: 0.8805086612701416\n",
      "step 8676, loss: 0.7757568955421448\n",
      "step 8677, loss: 0.814505398273468\n",
      "step 8678, loss: 0.7770364880561829\n",
      "step 8679, loss: 0.6980339884757996\n",
      "step 8680, loss: 0.6555981636047363\n",
      "step 8681, loss: 0.8706064820289612\n",
      "step 8682, loss: 0.7543386220932007\n",
      "step 8683, loss: 0.7144708037376404\n",
      "step 8684, loss: 0.6509841084480286\n",
      "step 8685, loss: 0.8873032331466675\n",
      "step 8686, loss: 0.9334455728530884\n",
      "step 8687, loss: 0.807616651058197\n",
      "step 8688, loss: 0.80571448802948\n",
      "step 8689, loss: 0.7940518260002136\n",
      "step 8690, loss: 0.6853781938552856\n",
      "step 8691, loss: 0.6451073288917542\n",
      "step 8692, loss: 0.9069573879241943\n",
      "step 8693, loss: 0.9844252467155457\n",
      "step 8694, loss: 0.6599582433700562\n",
      "step 8695, loss: 0.9455969333648682\n",
      "step 8696, loss: 0.788938581943512\n",
      "step 8697, loss: 0.830750584602356\n",
      "step 8698, loss: 0.8772350549697876\n",
      "step 8699, loss: 0.8427791595458984\n",
      "step 8700, loss: 0.852935791015625\n",
      "step 8701, loss: 0.8768243789672852\n",
      "step 8702, loss: 0.8644420504570007\n",
      "step 8703, loss: 0.8637814521789551\n",
      "step 8704, loss: 0.6167743802070618\n",
      "step 8705, loss: 0.8817303776741028\n",
      "step 8706, loss: 0.7926862239837646\n",
      "step 8707, loss: 0.7733666896820068\n",
      "step 8708, loss: 0.7669666409492493\n",
      "step 8709, loss: 0.9973533153533936\n",
      "step 8710, loss: 1.01673424243927\n",
      "step 8711, loss: 0.9327682852745056\n",
      "step 8712, loss: 0.8507347106933594\n",
      "step 8713, loss: 0.831445038318634\n",
      "step 8714, loss: 0.8555862307548523\n",
      "step 8715, loss: 0.9058300852775574\n",
      "step 8716, loss: 0.6445521712303162\n",
      "step 8717, loss: 0.9743536710739136\n",
      "step 8718, loss: 0.720024824142456\n",
      "step 8719, loss: 0.6359670758247375\n",
      "step 8720, loss: 1.0155324935913086\n",
      "step 8721, loss: 0.6816360950469971\n",
      "step 8722, loss: 1.01083505153656\n",
      "step 8723, loss: 0.797809898853302\n",
      "step 8724, loss: 0.9718859195709229\n",
      "step 8725, loss: 0.7783941030502319\n",
      "step 8726, loss: 0.8351157307624817\n",
      "step 8727, loss: 0.7452659606933594\n",
      "step 8728, loss: 0.8028731346130371\n",
      "step 8729, loss: 0.815114438533783\n",
      "step 8730, loss: 0.7420626282691956\n",
      "step 8731, loss: 0.8682447075843811\n",
      "step 8732, loss: 0.8754992485046387\n",
      "step 8733, loss: 0.712868869304657\n",
      "step 8734, loss: 0.7138878703117371\n",
      "step 8735, loss: 0.7461585402488708\n",
      "step 8736, loss: 0.8134788870811462\n",
      "step 8737, loss: 0.8741137385368347\n",
      "step 8738, loss: 0.7214369177818298\n",
      "step 8739, loss: 0.8671322464942932\n",
      "step 8740, loss: 1.1413719654083252\n",
      "step 8741, loss: 0.8678672909736633\n",
      "step 8742, loss: 0.7836552858352661\n",
      "step 8743, loss: 0.6479598879814148\n",
      "step 8744, loss: 0.8992712497711182\n",
      "step 8745, loss: 0.8267699480056763\n",
      "step 8746, loss: 0.7070769667625427\n",
      "step 8747, loss: 0.6397708654403687\n",
      "step 8748, loss: 0.7799594402313232\n",
      "step 8749, loss: 0.7592818737030029\n",
      "step 8750, loss: 0.8140900731086731\n",
      "step 8751, loss: 0.7192467451095581\n",
      "step 8752, loss: 0.7676460146903992\n",
      "step 8753, loss: 0.6729888319969177\n",
      "step 8754, loss: 0.700589120388031\n",
      "step 8755, loss: 0.8913536071777344\n",
      "step 8756, loss: 0.7987423539161682\n",
      "step 8757, loss: 0.6747876405715942\n",
      "step 8758, loss: 0.7710406184196472\n",
      "step 8759, loss: 0.6637886166572571\n",
      "step 8760, loss: 0.6900699138641357\n",
      "step 8761, loss: 0.6319423913955688\n",
      "step 8762, loss: 0.762298047542572\n",
      "step 8763, loss: 0.5623111724853516\n",
      "step 8764, loss: 0.7417804002761841\n",
      "step 8765, loss: 0.6884705424308777\n",
      "step 8766, loss: 0.7358152866363525\n",
      "step 8767, loss: 0.63329017162323\n",
      "step 8768, loss: 0.7006152868270874\n",
      "step 8769, loss: 0.6658899188041687\n",
      "step 8770, loss: 0.6876242756843567\n",
      "step 8771, loss: 0.6231030225753784\n",
      "step 8772, loss: 0.32395023107528687\n",
      "step 8773, loss: 0.442592054605484\n",
      "step 8774, loss: 0.4174029231071472\n",
      "step 8775, loss: 0.529565155506134\n",
      "step 8776, loss: 0.32310110330581665\n",
      "step 8777, loss: 0.3967674970626831\n",
      "step 8778, loss: 0.36530742049217224\n",
      "step 8779, loss: 0.3668898642063141\n",
      "step 8780, loss: 0.45581161975860596\n",
      "step 8781, loss: 0.4979439675807953\n",
      "step 8782, loss: 0.2770159840583801\n",
      "step 8783, loss: 0.26223406195640564\n",
      "step 8784, loss: 0.35709208250045776\n",
      "step 8785, loss: 0.27795302867889404\n",
      "step 8786, loss: 0.47459813952445984\n",
      "step 8787, loss: 0.4349297285079956\n",
      "step 8788, loss: 0.3224888741970062\n",
      "step 8789, loss: 0.3051137924194336\n",
      "step 8790, loss: 0.4133562743663788\n",
      "step 8791, loss: 0.4022120237350464\n",
      "step 8792, loss: 0.4174143671989441\n",
      "step 8793, loss: 0.34949102997779846\n",
      "step 8794, loss: 0.5502897500991821\n",
      "step 8795, loss: 0.44694429636001587\n",
      "step 8796, loss: 0.36494261026382446\n",
      "step 8797, loss: 0.340000182390213\n",
      "step 8798, loss: 0.400928258895874\n",
      "step 8799, loss: 0.6334993243217468\n",
      "step 8800, loss: 0.5021077394485474\n",
      "step 8801, loss: 0.39899304509162903\n",
      "step 8802, loss: 0.4784923493862152\n",
      "step 8803, loss: 0.8049176931381226\n",
      "step 8804, loss: 0.7451069951057434\n",
      "step 8805, loss: 0.5978451371192932\n",
      "step 8806, loss: 0.782829225063324\n",
      "step 8807, loss: 0.6672530174255371\n",
      "step 8808, loss: 0.6222012639045715\n",
      "step 8809, loss: 0.6898164749145508\n",
      "step 8810, loss: 0.7096089720726013\n",
      "step 8811, loss: 0.6307459473609924\n",
      "step 8812, loss: 0.7900857925415039\n",
      "step 8813, loss: 0.6478679776191711\n",
      "step 8814, loss: 0.5542738437652588\n",
      "step 8815, loss: 0.6185047030448914\n",
      "step 8816, loss: 0.6657987833023071\n",
      "step 8817, loss: 0.6179507374763489\n",
      "step 8818, loss: 0.7758784294128418\n",
      "step 8819, loss: 0.6526090502738953\n",
      "step 8820, loss: 0.5736250281333923\n",
      "step 8821, loss: 0.7518623471260071\n",
      "step 8822, loss: 0.6534996032714844\n",
      "step 8823, loss: 0.7136341333389282\n",
      "step 8824, loss: 0.5755444765090942\n",
      "step 8825, loss: 0.6474623680114746\n",
      "step 8826, loss: 0.6555319428443909\n",
      "step 8827, loss: 0.8337321281433105\n",
      "step 8828, loss: 0.6147117018699646\n",
      "step 8829, loss: 0.5550718307495117\n",
      "step 8830, loss: 0.657508134841919\n",
      "step 8831, loss: 0.6316378116607666\n",
      "step 8832, loss: 0.5921249985694885\n",
      "step 8833, loss: 0.7947646379470825\n",
      "step 8834, loss: 0.556502103805542\n",
      "step 8835, loss: 0.6896678805351257\n",
      "step 8836, loss: 0.6183888912200928\n",
      "step 8837, loss: 0.5977687835693359\n",
      "step 8838, loss: 0.6103385090827942\n",
      "step 8839, loss: 0.5395016670227051\n",
      "step 8840, loss: 0.6244538426399231\n",
      "step 8841, loss: 0.5784736275672913\n",
      "step 8842, loss: 0.5397699475288391\n",
      "step 8843, loss: 0.4445341229438782\n",
      "step 8844, loss: 0.3824384808540344\n",
      "step 8845, loss: 0.38918885588645935\n",
      "step 8846, loss: 0.5030760765075684\n",
      "step 8847, loss: 0.35855671763420105\n",
      "step 8848, loss: 0.4447699785232544\n",
      "step 8849, loss: 0.31725963950157166\n",
      "step 8850, loss: 0.46751219034194946\n",
      "step 8851, loss: 0.47537684440612793\n",
      "step 8852, loss: 0.5728678107261658\n",
      "step 8853, loss: 0.5126659274101257\n",
      "step 8854, loss: 0.41028672456741333\n",
      "step 8855, loss: 0.32865414023399353\n",
      "step 8856, loss: 0.7033641934394836\n",
      "step 8857, loss: 0.8943721652030945\n",
      "step 8858, loss: 0.8369150161743164\n",
      "step 8859, loss: 0.7565041184425354\n",
      "step 8860, loss: 0.8659830093383789\n",
      "step 8861, loss: 0.7964801788330078\n",
      "step 8862, loss: 0.6743375658988953\n",
      "step 8863, loss: 0.8483756184577942\n",
      "step 8864, loss: 0.6364293694496155\n",
      "step 8865, loss: 1.0106065273284912\n",
      "step 8866, loss: 0.7087584137916565\n",
      "step 8867, loss: 0.8297843933105469\n",
      "step 8868, loss: 0.8200782537460327\n",
      "step 8869, loss: 0.8612744212150574\n",
      "step 8870, loss: 0.6894387602806091\n",
      "step 8871, loss: 0.6446915864944458\n",
      "step 8872, loss: 0.8195136785507202\n",
      "step 8873, loss: 0.8736070990562439\n",
      "step 8874, loss: 0.972624659538269\n",
      "step 8875, loss: 0.8551341891288757\n",
      "step 8876, loss: 0.8187532424926758\n",
      "step 8877, loss: 0.7144734859466553\n",
      "step 8878, loss: 0.8309684991836548\n",
      "step 8879, loss: 0.9099457859992981\n",
      "step 8880, loss: 1.0237563848495483\n",
      "step 8881, loss: 0.8658456802368164\n",
      "step 8882, loss: 0.7684236168861389\n",
      "step 8883, loss: 0.7627438902854919\n",
      "step 8884, loss: 0.8158134818077087\n",
      "step 8885, loss: 0.6817810535430908\n",
      "step 8886, loss: 0.8279303908348083\n",
      "step 8887, loss: 0.8882701396942139\n",
      "step 8888, loss: 0.7449263334274292\n",
      "step 8889, loss: 0.759160578250885\n",
      "step 8890, loss: 0.7386263608932495\n",
      "step 8891, loss: 0.8901906609535217\n",
      "step 8892, loss: 0.9072626829147339\n",
      "step 8893, loss: 0.800618588924408\n",
      "step 8894, loss: 0.7994253635406494\n",
      "step 8895, loss: 0.7557522058486938\n",
      "step 8896, loss: 0.6987861394882202\n",
      "step 8897, loss: 0.9352819919586182\n",
      "step 8898, loss: 0.9100817441940308\n",
      "step 8899, loss: 0.7865725159645081\n",
      "step 8900, loss: 0.8721843957901001\n",
      "step 8901, loss: 0.8060792684555054\n",
      "step 8902, loss: 0.7935506701469421\n",
      "step 8903, loss: 0.753387451171875\n",
      "step 8904, loss: 0.8113195300102234\n",
      "step 8905, loss: 0.6728177070617676\n",
      "step 8906, loss: 0.8032633066177368\n",
      "step 8907, loss: 0.7937667369842529\n",
      "step 8908, loss: 0.7107418775558472\n",
      "step 8909, loss: 0.6574296355247498\n",
      "step 8910, loss: 0.7865749001502991\n",
      "step 8911, loss: 0.8452078104019165\n",
      "step 8912, loss: 0.6770938038825989\n",
      "step 8913, loss: 0.6538044214248657\n",
      "step 8914, loss: 1.0323970317840576\n",
      "step 8915, loss: 0.7433230876922607\n",
      "step 8916, loss: 0.7004807591438293\n",
      "step 8917, loss: 0.6805064082145691\n",
      "step 8918, loss: 0.8731778860092163\n",
      "step 8919, loss: 0.8612909913063049\n",
      "step 8920, loss: 0.8846960663795471\n",
      "step 8921, loss: 0.8016879558563232\n",
      "step 8922, loss: 0.7635212540626526\n",
      "step 8923, loss: 0.7267407774925232\n",
      "step 8924, loss: 0.891033411026001\n",
      "step 8925, loss: 1.1710543632507324\n",
      "step 8926, loss: 0.7957559823989868\n",
      "step 8927, loss: 0.8719585537910461\n",
      "step 8928, loss: 0.9298800230026245\n",
      "step 8929, loss: 0.7627712488174438\n",
      "step 8930, loss: 0.9575580954551697\n",
      "step 8931, loss: 0.8610165119171143\n",
      "step 8932, loss: 0.667980432510376\n",
      "step 8933, loss: 0.7367572784423828\n",
      "step 8934, loss: 0.7934494614601135\n",
      "step 8935, loss: 0.79432612657547\n",
      "step 8936, loss: 0.7023869752883911\n",
      "step 8937, loss: 0.7559545040130615\n",
      "step 8938, loss: 0.6749634742736816\n",
      "step 8939, loss: 0.7394840121269226\n",
      "step 8940, loss: 0.7220759987831116\n",
      "step 8941, loss: 0.6842668652534485\n",
      "step 8942, loss: 0.6042712926864624\n",
      "step 8943, loss: 0.8501776456832886\n",
      "step 8944, loss: 0.751765251159668\n",
      "step 8945, loss: 0.6796419620513916\n",
      "step 8946, loss: 0.516838014125824\n",
      "step 8947, loss: 0.5169017314910889\n",
      "step 8948, loss: 0.5467677116394043\n",
      "step 8949, loss: 0.5414023399353027\n",
      "step 8950, loss: 0.557211697101593\n",
      "step 8951, loss: 0.4951140880584717\n",
      "step 8952, loss: 0.3972901701927185\n",
      "step 8953, loss: 0.5009469389915466\n",
      "step 8954, loss: 0.4254758358001709\n",
      "step 8955, loss: 0.5776819586753845\n",
      "step 8956, loss: 0.474839448928833\n",
      "step 8957, loss: 0.5413662791252136\n",
      "step 8958, loss: 0.37558427453041077\n",
      "step 8959, loss: 0.44161948561668396\n",
      "step 8960, loss: 0.6356399059295654\n",
      "step 8961, loss: 0.7559177875518799\n",
      "step 8962, loss: 0.8424522280693054\n",
      "step 8963, loss: 0.5597954988479614\n",
      "step 8964, loss: 0.5473031401634216\n",
      "step 8965, loss: 0.7172597050666809\n",
      "step 8966, loss: 0.7812914848327637\n",
      "step 8967, loss: 0.6884763836860657\n",
      "step 8968, loss: 0.6653785109519958\n",
      "step 8969, loss: 0.572635293006897\n",
      "step 8970, loss: 0.6664735674858093\n",
      "step 8971, loss: 0.9191221594810486\n",
      "step 8972, loss: 0.8259099721908569\n",
      "step 8973, loss: 0.7432406544685364\n",
      "step 8974, loss: 0.8330477476119995\n",
      "step 8975, loss: 0.7133829593658447\n",
      "step 8976, loss: 0.6590952277183533\n",
      "step 8977, loss: 0.6250629425048828\n",
      "step 8978, loss: 0.6816202998161316\n",
      "step 8979, loss: 0.7170756459236145\n",
      "step 8980, loss: 0.6390624642372131\n",
      "step 8981, loss: 0.6993531584739685\n",
      "step 8982, loss: 0.7496545910835266\n",
      "step 8983, loss: 0.6542649865150452\n",
      "step 8984, loss: 0.6772637367248535\n",
      "step 8985, loss: 0.7348032593727112\n",
      "step 8986, loss: 0.7436676621437073\n",
      "step 8987, loss: 0.7391143441200256\n",
      "step 8988, loss: 0.7003886699676514\n",
      "step 8989, loss: 0.7012618780136108\n",
      "step 8990, loss: 0.6452067494392395\n",
      "step 8991, loss: 0.6505950689315796\n",
      "step 8992, loss: 0.8995409607887268\n",
      "step 8993, loss: 0.7276945114135742\n",
      "step 8994, loss: 0.720285177230835\n",
      "step 8995, loss: 0.6587063670158386\n",
      "step 8996, loss: 0.7637636065483093\n",
      "step 8997, loss: 0.6090618371963501\n",
      "step 8998, loss: 0.7663441300392151\n",
      "step 8999, loss: 0.9418925046920776\n",
      "step 9000, loss: 0.6748266220092773\n",
      "step 9001, loss: 0.7983314990997314\n",
      "step 9002, loss: 0.6856353282928467\n",
      "step 9003, loss: 0.7313423156738281\n",
      "step 9004, loss: 0.5896279811859131\n",
      "step 9005, loss: 0.8336338400840759\n",
      "step 9006, loss: 0.7866601347923279\n",
      "step 9007, loss: 0.5431978702545166\n",
      "step 9008, loss: 0.7015419602394104\n",
      "step 9009, loss: 0.5886567234992981\n",
      "step 9010, loss: 0.6698184609413147\n",
      "step 9011, loss: 0.797944188117981\n",
      "step 9012, loss: 0.7997344136238098\n",
      "step 9013, loss: 0.6018146872520447\n",
      "step 9014, loss: 0.6183831095695496\n",
      "step 9015, loss: 0.6147192716598511\n",
      "step 9016, loss: 0.6760216951370239\n",
      "step 9017, loss: 0.5516974925994873\n",
      "step 9018, loss: 0.5536637306213379\n",
      "step 9019, loss: 0.3542082905769348\n",
      "step 9020, loss: 0.4558144807815552\n",
      "step 9021, loss: 0.39754924178123474\n",
      "step 9022, loss: 0.46433067321777344\n",
      "step 9023, loss: 0.3088882863521576\n",
      "step 9024, loss: 0.3516332507133484\n",
      "step 9025, loss: 0.45620450377464294\n",
      "step 9026, loss: 0.518304169178009\n",
      "step 9027, loss: 0.4035722017288208\n",
      "step 9028, loss: 0.3617856502532959\n",
      "step 9029, loss: 0.25931811332702637\n",
      "step 9030, loss: 0.3267177939414978\n",
      "step 9031, loss: 0.5464890599250793\n",
      "step 9032, loss: 0.5127679109573364\n",
      "step 9033, loss: 0.7759188413619995\n",
      "step 9034, loss: 0.6307065486907959\n",
      "step 9035, loss: 0.5405209064483643\n",
      "step 9036, loss: 0.6646520495414734\n",
      "step 9037, loss: 0.5804887413978577\n",
      "step 9038, loss: 0.542398989200592\n",
      "step 9039, loss: 0.6411792635917664\n",
      "step 9040, loss: 0.7226989269256592\n",
      "step 9041, loss: 0.6525305509567261\n",
      "step 9042, loss: 0.663952112197876\n",
      "step 9043, loss: 0.731300950050354\n",
      "step 9044, loss: 0.7243565320968628\n",
      "step 9045, loss: 0.8169978857040405\n",
      "step 9046, loss: 0.7415013313293457\n",
      "step 9047, loss: 0.6277815699577332\n",
      "step 9048, loss: 0.6458819508552551\n",
      "step 9049, loss: 0.7452969551086426\n",
      "step 9050, loss: 0.677678644657135\n",
      "step 9051, loss: 0.6650937795639038\n",
      "step 9052, loss: 0.7116784453392029\n",
      "step 9053, loss: 0.6818552017211914\n",
      "step 9054, loss: 0.9719603061676025\n",
      "step 9055, loss: 0.8467191457748413\n",
      "step 9056, loss: 0.8155995607376099\n",
      "step 9057, loss: 0.7820126414299011\n",
      "step 9058, loss: 0.8265912532806396\n",
      "step 9059, loss: 0.7616685628890991\n",
      "step 9060, loss: 0.8266139626502991\n",
      "step 9061, loss: 0.8220679759979248\n",
      "step 9062, loss: 0.7656068801879883\n",
      "step 9063, loss: 0.8663843870162964\n",
      "step 9064, loss: 0.8607839345932007\n",
      "step 9065, loss: 0.688684344291687\n",
      "step 9066, loss: 0.7782670259475708\n",
      "step 9067, loss: 0.8768461346626282\n",
      "step 9068, loss: 0.8884750008583069\n",
      "step 9069, loss: 0.9088209867477417\n",
      "step 9070, loss: 0.8343238830566406\n",
      "step 9071, loss: 0.7854872345924377\n",
      "step 9072, loss: 0.8399397730827332\n",
      "step 9073, loss: 0.5848920941352844\n",
      "step 9074, loss: 0.6843790411949158\n",
      "step 9075, loss: 0.7684711217880249\n",
      "step 9076, loss: 0.7426449656486511\n",
      "step 9077, loss: 0.6831004619598389\n",
      "step 9078, loss: 0.8274722099304199\n",
      "step 9079, loss: 0.8675329685211182\n",
      "step 9080, loss: 0.7920388579368591\n",
      "step 9081, loss: 1.0590673685073853\n",
      "step 9082, loss: 0.8078394532203674\n",
      "step 9083, loss: 0.8108370304107666\n",
      "step 9084, loss: 0.7956331968307495\n",
      "step 9085, loss: 0.7701529264450073\n",
      "step 9086, loss: 0.7390882968902588\n",
      "step 9087, loss: 0.8191832900047302\n",
      "step 9088, loss: 0.7948815822601318\n",
      "step 9089, loss: 0.7107169032096863\n",
      "step 9090, loss: 0.7315921783447266\n",
      "step 9091, loss: 0.7422558665275574\n",
      "step 9092, loss: 0.5708920955657959\n",
      "step 9093, loss: 0.7029065489768982\n",
      "step 9094, loss: 0.8601635694503784\n",
      "step 9095, loss: 0.6967987418174744\n",
      "step 9096, loss: 0.7552416920661926\n",
      "step 9097, loss: 0.8343600034713745\n",
      "step 9098, loss: 0.6163763999938965\n",
      "step 9099, loss: 0.8279076814651489\n",
      "step 9100, loss: 0.6735936403274536\n",
      "step 9101, loss: 0.7226220369338989\n",
      "step 9102, loss: 0.6078473925590515\n",
      "step 9103, loss: 0.8043375611305237\n",
      "step 9104, loss: 0.8777585029602051\n",
      "step 9105, loss: 0.9348875284194946\n",
      "step 9106, loss: 0.9332391023635864\n",
      "step 9107, loss: 0.7149146795272827\n",
      "step 9108, loss: 0.5850876569747925\n",
      "step 9109, loss: 0.7335102558135986\n",
      "step 9110, loss: 0.6783609390258789\n",
      "step 9111, loss: 0.5396857261657715\n",
      "step 9112, loss: 0.6644306778907776\n",
      "step 9113, loss: 0.707639217376709\n",
      "step 9114, loss: 0.6764171719551086\n",
      "step 9115, loss: 0.6261969804763794\n",
      "step 9116, loss: 0.7423482537269592\n",
      "step 9117, loss: 0.7467977404594421\n",
      "step 9118, loss: 0.8253433704376221\n",
      "step 9119, loss: 0.5438065528869629\n",
      "step 9120, loss: 0.7906031012535095\n",
      "step 9121, loss: 0.816546618938446\n",
      "step 9122, loss: 0.6957904100418091\n",
      "step 9123, loss: 0.8153711557388306\n",
      "step 9124, loss: 0.8144870400428772\n",
      "step 9125, loss: 0.6517627239227295\n",
      "step 9126, loss: 0.671952486038208\n",
      "step 9127, loss: 0.6277933716773987\n",
      "step 9128, loss: 0.6855291128158569\n",
      "step 9129, loss: 0.6252062320709229\n",
      "step 9130, loss: 0.7361498475074768\n",
      "step 9131, loss: 0.645147979259491\n",
      "step 9132, loss: 0.7543613314628601\n",
      "step 9133, loss: 0.6275833249092102\n",
      "step 9134, loss: 0.4920099675655365\n",
      "step 9135, loss: 0.48873504996299744\n",
      "step 9136, loss: 0.7225992679595947\n",
      "step 9137, loss: 0.5898566842079163\n",
      "step 9138, loss: 0.6661005020141602\n",
      "step 9139, loss: 0.6790739893913269\n",
      "step 9140, loss: 0.6215986609458923\n",
      "step 9141, loss: 0.54658043384552\n",
      "step 9142, loss: 0.5392422676086426\n",
      "step 9143, loss: 0.6144638657569885\n",
      "step 9144, loss: 0.6941627264022827\n",
      "step 9145, loss: 0.6051357388496399\n",
      "step 9146, loss: 0.4386904537677765\n",
      "step 9147, loss: 0.37818628549575806\n",
      "step 9148, loss: 0.39188653230667114\n",
      "step 9149, loss: 0.40800705552101135\n",
      "step 9150, loss: 0.36715167760849\n",
      "step 9151, loss: 0.3502921164035797\n",
      "step 9152, loss: 0.4243277907371521\n",
      "step 9153, loss: 0.3776230812072754\n",
      "step 9154, loss: 0.3257687985897064\n",
      "step 9155, loss: 0.3436291813850403\n",
      "step 9156, loss: 0.358913779258728\n",
      "step 9157, loss: 0.32567450404167175\n",
      "step 9158, loss: 0.3133469820022583\n",
      "step 9159, loss: 0.39445045590400696\n",
      "step 9160, loss: 0.4314253628253937\n",
      "step 9161, loss: 0.2710784375667572\n",
      "step 9162, loss: 0.27918994426727295\n",
      "step 9163, loss: 0.34462422132492065\n",
      "step 9164, loss: 0.4964793026447296\n",
      "step 9165, loss: 0.5574612021446228\n",
      "step 9166, loss: 0.43950608372688293\n",
      "step 9167, loss: 0.43548405170440674\n",
      "step 9168, loss: 0.3546127676963806\n",
      "step 9169, loss: 0.4165516495704651\n",
      "step 9170, loss: 0.38440001010894775\n",
      "step 9171, loss: 0.3536088168621063\n",
      "step 9172, loss: 0.6962056159973145\n",
      "step 9173, loss: 0.7046476602554321\n",
      "step 9174, loss: 0.6861094236373901\n",
      "step 9175, loss: 0.7359954714775085\n",
      "step 9176, loss: 0.7252222299575806\n",
      "step 9177, loss: 0.7431559562683105\n",
      "step 9178, loss: 0.6566516757011414\n",
      "step 9179, loss: 0.7348547577857971\n",
      "step 9180, loss: 0.7408120036125183\n",
      "step 9181, loss: 0.6188918948173523\n",
      "step 9182, loss: 0.5587241053581238\n",
      "step 9183, loss: 0.6700150370597839\n",
      "step 9184, loss: 0.5435727834701538\n",
      "step 9185, loss: 0.6185227632522583\n",
      "step 9186, loss: 0.6094860434532166\n",
      "step 9187, loss: 0.5796144008636475\n",
      "step 9188, loss: 0.6712455749511719\n",
      "step 9189, loss: 0.6366681456565857\n",
      "step 9190, loss: 0.5784168839454651\n",
      "step 9191, loss: 0.5202033519744873\n",
      "step 9192, loss: 0.7077243328094482\n",
      "step 9193, loss: 0.7174029350280762\n",
      "step 9194, loss: 0.6405740976333618\n",
      "step 9195, loss: 0.7530872225761414\n",
      "step 9196, loss: 0.804162859916687\n",
      "step 9197, loss: 0.5469080209732056\n",
      "step 9198, loss: 0.8180201053619385\n",
      "step 9199, loss: 0.7540956139564514\n",
      "step 9200, loss: 0.654583215713501\n",
      "step 9201, loss: 0.626868724822998\n",
      "step 9202, loss: 0.6522722244262695\n",
      "step 9203, loss: 0.6973189115524292\n",
      "step 9204, loss: 0.9197933077812195\n",
      "step 9205, loss: 0.6919048428535461\n",
      "step 9206, loss: 0.6027028560638428\n",
      "step 9207, loss: 0.6773776412010193\n",
      "step 9208, loss: 0.6495413184165955\n",
      "step 9209, loss: 0.8093680143356323\n",
      "step 9210, loss: 0.6663932204246521\n",
      "step 9211, loss: 0.7234126329421997\n",
      "step 9212, loss: 0.6885565519332886\n",
      "step 9213, loss: 0.648557722568512\n",
      "step 9214, loss: 0.723798394203186\n",
      "step 9215, loss: 0.6176594495773315\n",
      "step 9216, loss: 0.6002539396286011\n",
      "step 9217, loss: 0.9050884246826172\n",
      "step 9218, loss: 0.910966157913208\n",
      "step 9219, loss: 0.6576584577560425\n",
      "step 9220, loss: 0.5642896294593811\n",
      "step 9221, loss: 0.6887184381484985\n",
      "step 9222, loss: 0.594385027885437\n",
      "step 9223, loss: 0.6298196911811829\n",
      "step 9224, loss: 0.6890097260475159\n",
      "step 9225, loss: 0.7748384475708008\n",
      "step 9226, loss: 0.7457712888717651\n",
      "step 9227, loss: 0.8307471871376038\n",
      "step 9228, loss: 0.6399707794189453\n",
      "step 9229, loss: 0.6581697463989258\n",
      "step 9230, loss: 0.7368084788322449\n",
      "step 9231, loss: 0.6701563596725464\n",
      "step 9232, loss: 0.538640558719635\n",
      "step 9233, loss: 0.5978800058364868\n",
      "step 9234, loss: 0.6340883374214172\n",
      "step 9235, loss: 0.6548696756362915\n",
      "step 9236, loss: 0.5735810399055481\n",
      "step 9237, loss: 0.7411317229270935\n",
      "step 9238, loss: 0.7892035841941833\n",
      "step 9239, loss: 0.7355888485908508\n",
      "step 9240, loss: 0.861851692199707\n",
      "step 9241, loss: 0.6942026615142822\n",
      "step 9242, loss: 0.730502188205719\n",
      "step 9243, loss: 0.6279430985450745\n",
      "step 9244, loss: 0.6531234383583069\n",
      "step 9245, loss: 0.7705667018890381\n",
      "step 9246, loss: 0.7251417636871338\n",
      "step 9247, loss: 0.5835109949111938\n",
      "step 9248, loss: 0.695517897605896\n",
      "step 9249, loss: 0.7776535749435425\n",
      "step 9250, loss: 0.6597009897232056\n",
      "step 9251, loss: 0.6563763618469238\n",
      "step 9252, loss: 0.6151029467582703\n",
      "step 9253, loss: 0.7244224548339844\n",
      "step 9254, loss: 0.7134255766868591\n",
      "step 9255, loss: 0.6627330183982849\n",
      "step 9256, loss: 0.7293914556503296\n",
      "step 9257, loss: 0.733073890209198\n",
      "step 9258, loss: 0.7858675122261047\n",
      "step 9259, loss: 0.6828886866569519\n",
      "step 9260, loss: 0.7161741852760315\n",
      "step 9261, loss: 0.62357497215271\n",
      "step 9262, loss: 0.6995279788970947\n",
      "step 9263, loss: 0.7130667567253113\n",
      "step 9264, loss: 0.5953853726387024\n",
      "step 9265, loss: 0.5880992412567139\n",
      "step 9266, loss: 0.6234634518623352\n",
      "step 9267, loss: 0.6531941890716553\n",
      "step 9268, loss: 0.5612964034080505\n",
      "step 9269, loss: 0.5876641869544983\n",
      "step 9270, loss: 0.39684683084487915\n",
      "step 9271, loss: 0.37348318099975586\n",
      "step 9272, loss: 0.3857235908508301\n",
      "step 9273, loss: 0.42352503538131714\n",
      "step 9274, loss: 0.4679177403450012\n",
      "step 9275, loss: 0.5098880529403687\n",
      "step 9276, loss: 0.582855761051178\n",
      "step 9277, loss: 0.42376577854156494\n",
      "step 9278, loss: 0.4885353446006775\n",
      "step 9279, loss: 0.49552470445632935\n",
      "step 9280, loss: 0.46518009901046753\n",
      "step 9281, loss: 0.36614981293678284\n",
      "step 9282, loss: 0.42305636405944824\n",
      "step 9283, loss: 0.46746116876602173\n",
      "step 9284, loss: 0.41146743297576904\n",
      "step 9285, loss: 0.4123799204826355\n",
      "step 9286, loss: 0.3738490343093872\n",
      "step 9287, loss: 0.2555232048034668\n",
      "step 9288, loss: 0.3523971438407898\n",
      "step 9289, loss: 0.3246366083621979\n",
      "step 9290, loss: 0.7958255410194397\n",
      "step 9291, loss: 0.5764980912208557\n",
      "step 9292, loss: 0.5522710680961609\n",
      "step 9293, loss: 0.6100753545761108\n",
      "step 9294, loss: 0.6375147700309753\n",
      "step 9295, loss: 0.5771421194076538\n",
      "step 9296, loss: 0.5792223215103149\n",
      "step 9297, loss: 0.6977692246437073\n",
      "step 9298, loss: 0.7681335210800171\n",
      "step 9299, loss: 0.7416452169418335\n",
      "step 9300, loss: 0.6641246676445007\n",
      "step 9301, loss: 0.5258097052574158\n",
      "step 9302, loss: 0.6103905439376831\n",
      "step 9303, loss: 0.7019153237342834\n",
      "step 9304, loss: 0.6703394055366516\n",
      "step 9305, loss: 0.7128511071205139\n",
      "step 9306, loss: 0.7372779846191406\n",
      "step 9307, loss: 0.7708677053451538\n",
      "step 9308, loss: 0.6717451214790344\n",
      "step 9309, loss: 0.7675853371620178\n",
      "step 9310, loss: 0.6428731679916382\n",
      "step 9311, loss: 0.6990340352058411\n",
      "step 9312, loss: 0.7551654577255249\n",
      "step 9313, loss: 0.7372013330459595\n",
      "step 9314, loss: 0.6055192947387695\n",
      "step 9315, loss: 0.7405719757080078\n",
      "step 9316, loss: 0.7454882860183716\n",
      "step 9317, loss: 0.5945906639099121\n",
      "step 9318, loss: 0.8535870313644409\n",
      "step 9319, loss: 0.9209198951721191\n",
      "step 9320, loss: 0.5670565366744995\n",
      "step 9321, loss: 0.8646863102912903\n",
      "step 9322, loss: 0.7498123645782471\n",
      "step 9323, loss: 0.7137669324874878\n",
      "step 9324, loss: 0.7652754783630371\n",
      "step 9325, loss: 0.7292140126228333\n",
      "step 9326, loss: 0.8838144540786743\n",
      "step 9327, loss: 0.7043416500091553\n",
      "step 9328, loss: 0.7966861128807068\n",
      "step 9329, loss: 0.8818901777267456\n",
      "step 9330, loss: 0.7952354550361633\n",
      "step 9331, loss: 0.6403958201408386\n",
      "step 9332, loss: 0.7699252367019653\n",
      "step 9333, loss: 0.7768245935440063\n",
      "step 9334, loss: 0.6722147464752197\n",
      "step 9335, loss: 0.7548948526382446\n",
      "step 9336, loss: 0.8841743469238281\n",
      "step 9337, loss: 0.8848348259925842\n",
      "step 9338, loss: 0.8377821445465088\n",
      "step 9339, loss: 0.8883221745491028\n",
      "step 9340, loss: 0.7528882622718811\n",
      "step 9341, loss: 0.6969305872917175\n",
      "step 9342, loss: 0.6962390542030334\n",
      "step 9343, loss: 0.6177955865859985\n",
      "step 9344, loss: 0.6560525894165039\n",
      "step 9345, loss: 0.7805043458938599\n",
      "step 9346, loss: 0.8980848789215088\n",
      "step 9347, loss: 0.8530923128128052\n",
      "step 9348, loss: 0.7571542859077454\n",
      "step 9349, loss: 0.7902581691741943\n",
      "step 9350, loss: 0.7042953372001648\n",
      "step 9351, loss: 0.7725543975830078\n",
      "step 9352, loss: 0.8059092164039612\n",
      "step 9353, loss: 0.8767917156219482\n",
      "step 9354, loss: 0.8509648442268372\n",
      "step 9355, loss: 0.7547689080238342\n",
      "step 9356, loss: 1.0280158519744873\n",
      "step 9357, loss: 0.7491337060928345\n",
      "step 9358, loss: 0.87691330909729\n",
      "step 9359, loss: 0.8649373650550842\n",
      "step 9360, loss: 0.83504319190979\n",
      "step 9361, loss: 0.830350935459137\n",
      "step 9362, loss: 0.7728341817855835\n",
      "step 9363, loss: 0.8442007899284363\n",
      "step 9364, loss: 0.5478239059448242\n",
      "step 9365, loss: 0.7526295781135559\n",
      "step 9366, loss: 0.8212235569953918\n",
      "step 9367, loss: 0.8204451203346252\n",
      "step 9368, loss: 0.46711423993110657\n",
      "step 9369, loss: 0.8036962747573853\n",
      "step 9370, loss: 0.6395811438560486\n",
      "step 9371, loss: 0.6999102234840393\n",
      "step 9372, loss: 0.7090891599655151\n",
      "step 9373, loss: 0.7441782355308533\n",
      "step 9374, loss: 0.6576231122016907\n",
      "step 9375, loss: 0.8217148184776306\n",
      "step 9376, loss: 0.6884500980377197\n",
      "step 9377, loss: 0.7431085705757141\n",
      "step 9378, loss: 0.6921352744102478\n",
      "step 9379, loss: 0.6994561553001404\n",
      "step 9380, loss: 0.48949134349823\n",
      "step 9381, loss: 0.43161606788635254\n",
      "step 9382, loss: 0.45266592502593994\n",
      "step 9383, loss: 0.4257514178752899\n",
      "step 9384, loss: 0.4671022593975067\n",
      "step 9385, loss: 0.5264870524406433\n",
      "step 9386, loss: 0.33795493841171265\n",
      "step 9387, loss: 0.33428633213043213\n",
      "step 9388, loss: 0.48243290185928345\n",
      "step 9389, loss: 0.4719482958316803\n",
      "step 9390, loss: 0.5525513887405396\n",
      "step 9391, loss: 0.4065988063812256\n",
      "step 9392, loss: 0.37024933099746704\n",
      "step 9393, loss: 0.4482562839984894\n",
      "step 9394, loss: 0.2663743793964386\n",
      "step 9395, loss: 0.4099712073802948\n",
      "step 9396, loss: 0.40120717883110046\n",
      "step 9397, loss: 0.5276843309402466\n",
      "step 9398, loss: 0.5862807035446167\n",
      "step 9399, loss: 0.6745424270629883\n",
      "step 9400, loss: 0.6696770191192627\n",
      "step 9401, loss: 0.5967502593994141\n",
      "step 9402, loss: 0.552894115447998\n",
      "step 9403, loss: 0.8398520946502686\n",
      "step 9404, loss: 0.6763371229171753\n",
      "step 9405, loss: 0.7721177935600281\n",
      "step 9406, loss: 0.6624410152435303\n",
      "step 9407, loss: 0.6199008822441101\n",
      "step 9408, loss: 0.512568473815918\n",
      "step 9409, loss: 0.8129532337188721\n",
      "step 9410, loss: 0.7547551393508911\n",
      "step 9411, loss: 0.5704244375228882\n",
      "step 9412, loss: 0.6991011500358582\n",
      "step 9413, loss: 0.5939873456954956\n",
      "step 9414, loss: 0.429568886756897\n",
      "step 9415, loss: 0.4821021556854248\n",
      "step 9416, loss: 0.6821685433387756\n",
      "step 9417, loss: 0.6853180527687073\n",
      "step 9418, loss: 0.5938146114349365\n",
      "step 9419, loss: 0.6580321788787842\n",
      "step 9420, loss: 0.6310229897499084\n",
      "step 9421, loss: 0.7263616323471069\n",
      "step 9422, loss: 0.5422282218933105\n",
      "step 9423, loss: 0.7158867716789246\n",
      "step 9424, loss: 0.7505151033401489\n",
      "step 9425, loss: 0.6293533444404602\n",
      "step 9426, loss: 0.7984327673912048\n",
      "step 9427, loss: 0.8242955803871155\n",
      "step 9428, loss: 0.7012682557106018\n",
      "step 9429, loss: 0.7389528155326843\n",
      "step 9430, loss: 0.7574900984764099\n",
      "step 9431, loss: 0.6807671785354614\n",
      "step 9432, loss: 0.6544760465621948\n",
      "step 9433, loss: 0.7096916437149048\n",
      "step 9434, loss: 0.6396480798721313\n",
      "step 9435, loss: 0.6610682010650635\n",
      "step 9436, loss: 0.7031375765800476\n",
      "step 9437, loss: 0.7998250722885132\n",
      "step 9438, loss: 0.6623924970626831\n",
      "step 9439, loss: 0.7269702553749084\n",
      "step 9440, loss: 0.5674408674240112\n",
      "step 9441, loss: 0.6227142214775085\n",
      "step 9442, loss: 0.40787163376808167\n",
      "step 9443, loss: 0.31087392568588257\n",
      "step 9444, loss: 0.5141446590423584\n",
      "step 9445, loss: 0.3743230998516083\n",
      "step 9446, loss: 0.3761287033557892\n",
      "step 9447, loss: 0.41590818762779236\n",
      "step 9448, loss: 0.48946723341941833\n",
      "step 9449, loss: 0.38023078441619873\n",
      "step 9450, loss: 0.36988121271133423\n",
      "step 9451, loss: 0.3373014032840729\n",
      "step 9452, loss: 0.4474101662635803\n",
      "step 9453, loss: 0.248233363032341\n",
      "step 9454, loss: 0.3455590009689331\n",
      "step 9455, loss: 0.5104837417602539\n",
      "step 9456, loss: 0.3723658621311188\n",
      "step 9457, loss: 0.3671632707118988\n",
      "step 9458, loss: 0.68791663646698\n",
      "step 9459, loss: 0.6436622738838196\n",
      "step 9460, loss: 0.703983724117279\n",
      "step 9461, loss: 0.6567233204841614\n",
      "step 9462, loss: 0.712132453918457\n",
      "step 9463, loss: 0.6332011222839355\n",
      "step 9464, loss: 0.6255355477333069\n",
      "step 9465, loss: 0.7518280744552612\n",
      "step 9466, loss: 0.757770836353302\n",
      "step 9467, loss: 0.6403188109397888\n",
      "step 9468, loss: 0.6664972305297852\n",
      "step 9469, loss: 0.6006858348846436\n",
      "step 9470, loss: 0.6579688787460327\n",
      "step 9471, loss: 0.6480457186698914\n",
      "step 9472, loss: 0.4863112270832062\n",
      "step 9473, loss: 0.4779145121574402\n",
      "step 9474, loss: 0.7547391653060913\n",
      "step 9475, loss: 0.5927138328552246\n",
      "step 9476, loss: 0.7447777390480042\n",
      "step 9477, loss: 0.6863296627998352\n",
      "step 9478, loss: 0.7498642206192017\n",
      "step 9479, loss: 0.8107622265815735\n",
      "step 9480, loss: 1.0032325983047485\n",
      "step 9481, loss: 0.7411019206047058\n",
      "step 9482, loss: 0.7746893167495728\n",
      "step 9483, loss: 0.6532528400421143\n",
      "step 9484, loss: 0.8103843927383423\n",
      "step 9485, loss: 0.6794553399085999\n",
      "step 9486, loss: 0.6308928728103638\n",
      "step 9487, loss: 0.9432071447372437\n",
      "step 9488, loss: 0.739416241645813\n",
      "step 9489, loss: 0.6921249032020569\n",
      "step 9490, loss: 0.6720773577690125\n",
      "step 9491, loss: 0.6547821164131165\n",
      "step 9492, loss: 0.8183760046958923\n",
      "step 9493, loss: 0.6373041272163391\n",
      "step 9494, loss: 0.7537416815757751\n",
      "step 9495, loss: 0.8564267754554749\n",
      "step 9496, loss: 0.8926724195480347\n",
      "step 9497, loss: 0.774151086807251\n",
      "step 9498, loss: 0.8953804969787598\n",
      "step 9499, loss: 0.7408528923988342\n",
      "step 9500, loss: 0.7463932037353516\n",
      "step 9501, loss: 0.8740895390510559\n",
      "step 9502, loss: 0.7813879251480103\n",
      "step 9503, loss: 0.6344089508056641\n",
      "step 9504, loss: 0.5192759037017822\n",
      "step 9505, loss: 0.898352861404419\n",
      "step 9506, loss: 0.6309834122657776\n",
      "step 9507, loss: 0.7204813957214355\n",
      "step 9508, loss: 0.5874391794204712\n",
      "step 9509, loss: 0.810808539390564\n",
      "step 9510, loss: 0.7251769304275513\n",
      "step 9511, loss: 0.783901572227478\n",
      "step 9512, loss: 0.7030020356178284\n",
      "step 9513, loss: 0.5324554443359375\n",
      "step 9514, loss: 0.5684114694595337\n",
      "step 9515, loss: 0.7765618562698364\n",
      "step 9516, loss: 0.6380889415740967\n",
      "step 9517, loss: 0.5806968212127686\n",
      "step 9518, loss: 0.7389130592346191\n",
      "step 9519, loss: 0.6082265377044678\n",
      "step 9520, loss: 0.7633523941040039\n",
      "step 9521, loss: 0.6736762523651123\n",
      "step 9522, loss: 0.6679469347000122\n",
      "step 9523, loss: 0.6944282650947571\n",
      "step 9524, loss: 0.7098563313484192\n",
      "step 9525, loss: 0.6488530039787292\n",
      "step 9526, loss: 0.7749197483062744\n",
      "step 9527, loss: 0.8346809148788452\n",
      "step 9528, loss: 0.666432797908783\n",
      "step 9529, loss: 0.7183239459991455\n",
      "step 9530, loss: 0.6435406804084778\n",
      "step 9531, loss: 0.6079440116882324\n",
      "step 9532, loss: 0.6861560940742493\n",
      "step 9533, loss: 0.6617991328239441\n",
      "step 9534, loss: 0.6522853970527649\n",
      "step 9535, loss: 0.6068804264068604\n",
      "step 9536, loss: 0.6938313841819763\n",
      "step 9537, loss: 0.7067981958389282\n",
      "step 9538, loss: 0.6596822738647461\n",
      "step 9539, loss: 0.5941832065582275\n",
      "step 9540, loss: 0.7744465470314026\n",
      "step 9541, loss: 0.6836615800857544\n",
      "step 9542, loss: 0.5888103246688843\n",
      "step 9543, loss: 0.6766403317451477\n",
      "step 9544, loss: 0.5955791473388672\n",
      "step 9545, loss: 0.6740706562995911\n",
      "step 9546, loss: 0.5327773094177246\n",
      "step 9547, loss: 0.435100793838501\n",
      "step 9548, loss: 0.38839203119277954\n",
      "step 9549, loss: 0.3030354678630829\n",
      "step 9550, loss: 0.457182914018631\n",
      "step 9551, loss: 0.3311983048915863\n",
      "step 9552, loss: 0.3121439218521118\n",
      "step 9553, loss: 0.31217119097709656\n",
      "step 9554, loss: 0.36055853962898254\n",
      "step 9555, loss: 0.3746398985385895\n",
      "step 9556, loss: 0.3229418992996216\n",
      "step 9557, loss: 0.353940486907959\n",
      "step 9558, loss: 0.3882898688316345\n",
      "step 9559, loss: 0.662169337272644\n",
      "step 9560, loss: 0.4761016070842743\n",
      "step 9561, loss: 0.5503743290901184\n",
      "step 9562, loss: 0.41577214002609253\n",
      "step 9563, loss: 0.377488374710083\n",
      "step 9564, loss: 0.32082799077033997\n",
      "step 9565, loss: 0.42088428139686584\n",
      "step 9566, loss: 0.3911682069301605\n",
      "step 9567, loss: 0.32218897342681885\n",
      "step 9568, loss: 0.4914661943912506\n",
      "step 9569, loss: 0.850475013256073\n",
      "step 9570, loss: 0.6920149922370911\n",
      "step 9571, loss: 0.6150801777839661\n",
      "step 9572, loss: 0.4704112112522125\n",
      "step 9573, loss: 0.6339921951293945\n",
      "step 9574, loss: 0.6915544271469116\n",
      "step 9575, loss: 0.6393805146217346\n",
      "step 9576, loss: 0.5680519938468933\n",
      "step 9577, loss: 0.7070761919021606\n",
      "step 9578, loss: 0.6515194177627563\n",
      "step 9579, loss: 0.5889081954956055\n",
      "step 9580, loss: 0.6126132011413574\n",
      "step 9581, loss: 0.6138590574264526\n",
      "step 9582, loss: 0.6749194264411926\n",
      "step 9583, loss: 0.6338661313056946\n",
      "step 9584, loss: 0.5275284647941589\n",
      "step 9585, loss: 0.7328981161117554\n",
      "step 9586, loss: 0.5261772274971008\n",
      "step 9587, loss: 0.6624656319618225\n",
      "step 9588, loss: 0.5937596559524536\n",
      "step 9589, loss: 0.569657027721405\n",
      "step 9590, loss: 0.6276474595069885\n",
      "step 9591, loss: 0.5955503582954407\n",
      "step 9592, loss: 0.602820098400116\n",
      "step 9593, loss: 0.5753264427185059\n",
      "step 9594, loss: 0.5691747069358826\n",
      "step 9595, loss: 0.6257966756820679\n",
      "step 9596, loss: 0.49009934067726135\n",
      "step 9597, loss: 0.5159868597984314\n",
      "step 9598, loss: 0.6227540969848633\n",
      "step 9599, loss: 0.6240339875221252\n",
      "step 9600, loss: 0.6714110374450684\n",
      "step 9601, loss: 0.6471356749534607\n",
      "step 9602, loss: 0.4675025939941406\n",
      "step 9603, loss: 0.637639045715332\n",
      "step 9604, loss: 0.5617935061454773\n",
      "step 9605, loss: 0.6251514554023743\n",
      "step 9606, loss: 0.5935326814651489\n",
      "step 9607, loss: 0.676048994064331\n",
      "step 9608, loss: 0.5657474398612976\n",
      "step 9609, loss: 0.6461840271949768\n",
      "step 9610, loss: 0.5454490184783936\n",
      "step 9611, loss: 0.6619691848754883\n",
      "step 9612, loss: 0.6672062277793884\n",
      "step 9613, loss: 0.7045682072639465\n",
      "step 9614, loss: 0.7436899542808533\n",
      "step 9615, loss: 0.6329062581062317\n",
      "step 9616, loss: 0.7881297469139099\n",
      "step 9617, loss: 0.5785627365112305\n",
      "step 9618, loss: 0.5526172518730164\n",
      "step 9619, loss: 0.5229119062423706\n",
      "step 9620, loss: 0.7811895608901978\n",
      "step 9621, loss: 0.5939596891403198\n",
      "step 9622, loss: 0.6255583167076111\n",
      "step 9623, loss: 0.6827678680419922\n",
      "step 9624, loss: 0.6688675880432129\n",
      "step 9625, loss: 0.7575951814651489\n",
      "step 9626, loss: 0.5792961120605469\n",
      "step 9627, loss: 0.6293391585350037\n",
      "step 9628, loss: 0.6736093163490295\n",
      "step 9629, loss: 0.5825290083885193\n",
      "step 9630, loss: 0.5097657442092896\n",
      "step 9631, loss: 0.5667452216148376\n",
      "step 9632, loss: 0.6022809743881226\n",
      "step 9633, loss: 0.644020676612854\n",
      "step 9634, loss: 0.45846569538116455\n",
      "step 9635, loss: 0.6257015466690063\n",
      "step 9636, loss: 0.6709883809089661\n",
      "step 9637, loss: 0.7857444882392883\n",
      "step 9638, loss: 0.47648513317108154\n",
      "step 9639, loss: 0.5682134032249451\n",
      "step 9640, loss: 0.6044620275497437\n",
      "step 9641, loss: 0.5540385246276855\n",
      "step 9642, loss: 0.6252469420433044\n",
      "step 9643, loss: 0.5812691450119019\n",
      "step 9644, loss: 0.616593062877655\n",
      "step 9645, loss: 0.5080230236053467\n",
      "step 9646, loss: 0.6377152800559998\n",
      "step 9647, loss: 0.6320575475692749\n",
      "step 9648, loss: 0.6471753120422363\n",
      "step 9649, loss: 0.7959822416305542\n",
      "step 9650, loss: 0.7117764949798584\n",
      "step 9651, loss: 0.6842182874679565\n",
      "step 9652, loss: 0.5319070816040039\n",
      "step 9653, loss: 0.6192454695701599\n",
      "step 9654, loss: 0.6590406894683838\n",
      "step 9655, loss: 0.5488300919532776\n",
      "step 9656, loss: 0.5239893794059753\n",
      "step 9657, loss: 0.7554892301559448\n",
      "step 9658, loss: 0.7095422744750977\n",
      "step 9659, loss: 0.5816160440444946\n",
      "step 9660, loss: 0.6803560853004456\n",
      "step 9661, loss: 0.6900115013122559\n",
      "step 9662, loss: 0.5975551009178162\n",
      "step 9663, loss: 0.5770382881164551\n",
      "step 9664, loss: 0.6495478749275208\n",
      "step 9665, loss: 0.6403361558914185\n",
      "step 9666, loss: 0.652628481388092\n",
      "step 9667, loss: 0.8458194136619568\n",
      "step 9668, loss: 0.711459219455719\n",
      "step 9669, loss: 0.6430832147598267\n",
      "step 9670, loss: 0.5516451597213745\n",
      "step 9671, loss: 0.5512633323669434\n",
      "step 9672, loss: 0.4883122742176056\n",
      "step 9673, loss: 0.475119024515152\n",
      "step 9674, loss: 0.6436302065849304\n",
      "step 9675, loss: 0.5578288435935974\n",
      "step 9676, loss: 0.6654025912284851\n",
      "step 9677, loss: 0.6328323483467102\n",
      "step 9678, loss: 0.47236549854278564\n",
      "step 9679, loss: 0.5319276452064514\n",
      "step 9680, loss: 0.49906888604164124\n",
      "step 9681, loss: 0.4321620762348175\n",
      "step 9682, loss: 0.45996856689453125\n",
      "step 9683, loss: 0.4925897419452667\n",
      "step 9684, loss: 0.4645512104034424\n",
      "step 9685, loss: 0.3259200155735016\n",
      "step 9686, loss: 0.39735421538352966\n",
      "step 9687, loss: 0.3086889386177063\n",
      "step 9688, loss: 0.34645408391952515\n",
      "step 9689, loss: 0.36893436312675476\n",
      "step 9690, loss: 0.36115625500679016\n",
      "step 9691, loss: 0.4972046911716461\n",
      "step 9692, loss: 0.3106798827648163\n",
      "step 9693, loss: 0.3690902888774872\n",
      "step 9694, loss: 0.30254751443862915\n",
      "step 9695, loss: 0.34081611037254333\n",
      "step 9696, loss: 0.505326509475708\n",
      "step 9697, loss: 0.5617349743843079\n",
      "step 9698, loss: 0.25309064984321594\n",
      "step 9699, loss: 0.33610543608665466\n",
      "step 9700, loss: 0.31320130825042725\n",
      "step 9701, loss: 0.3994709849357605\n",
      "step 9702, loss: 0.3539893329143524\n",
      "step 9703, loss: 0.312096506357193\n",
      "step 9704, loss: 0.30651092529296875\n",
      "step 9705, loss: 0.38726145029067993\n",
      "step 9706, loss: 0.4284481108188629\n",
      "step 9707, loss: 0.35852038860321045\n",
      "step 9708, loss: 0.5302437543869019\n",
      "step 9709, loss: 0.7098987102508545\n",
      "step 9710, loss: 0.5153241753578186\n",
      "step 9711, loss: 0.6399655342102051\n",
      "step 9712, loss: 0.5269569158554077\n",
      "step 9713, loss: 0.6850191950798035\n",
      "step 9714, loss: 0.6724494099617004\n",
      "step 9715, loss: 0.4896009862422943\n",
      "step 9716, loss: 0.6031368970870972\n",
      "step 9717, loss: 0.5922338962554932\n",
      "step 9718, loss: 0.6144620180130005\n",
      "step 9719, loss: 0.4307873845100403\n",
      "step 9720, loss: 0.5372197031974792\n",
      "step 9721, loss: 0.5485653877258301\n",
      "step 9722, loss: 0.6277146339416504\n",
      "step 9723, loss: 0.6174622178077698\n",
      "step 9724, loss: 0.6970912218093872\n",
      "step 9725, loss: 0.5156642198562622\n",
      "step 9726, loss: 0.6300696134567261\n",
      "step 9727, loss: 0.5016638040542603\n",
      "step 9728, loss: 0.5626577734947205\n",
      "step 9729, loss: 0.5524047017097473\n",
      "step 9730, loss: 0.5704001188278198\n",
      "step 9731, loss: 0.7455645799636841\n",
      "step 9732, loss: 0.662705659866333\n",
      "step 9733, loss: 0.7390826344490051\n",
      "step 9734, loss: 0.6529435515403748\n",
      "step 9735, loss: 0.5828570127487183\n",
      "step 9736, loss: 0.4927248954772949\n",
      "step 9737, loss: 0.5995357036590576\n",
      "step 9738, loss: 0.7650270462036133\n",
      "step 9739, loss: 0.6441852450370789\n",
      "step 9740, loss: 0.51670902967453\n",
      "step 9741, loss: 0.6625651121139526\n",
      "step 9742, loss: 0.6176548600196838\n",
      "step 9743, loss: 0.659103512763977\n",
      "step 9744, loss: 0.5828462839126587\n",
      "step 9745, loss: 0.5105606913566589\n",
      "step 9746, loss: 0.6145370602607727\n",
      "step 9747, loss: 0.7473994493484497\n",
      "step 9748, loss: 0.5700291991233826\n",
      "step 9749, loss: 0.6932766437530518\n",
      "step 9750, loss: 0.7579632997512817\n",
      "step 9751, loss: 0.5186309218406677\n",
      "step 9752, loss: 0.5994570255279541\n",
      "step 9753, loss: 0.6071674227714539\n",
      "step 9754, loss: 0.6392155289649963\n",
      "step 9755, loss: 0.6433192491531372\n",
      "step 9756, loss: 0.660843014717102\n",
      "step 9757, loss: 0.7402181029319763\n",
      "step 9758, loss: 0.6470955610275269\n",
      "step 9759, loss: 0.8034290075302124\n",
      "step 9760, loss: 0.702436625957489\n",
      "step 9761, loss: 0.7179358601570129\n",
      "step 9762, loss: 0.6272777318954468\n",
      "step 9763, loss: 0.896992564201355\n",
      "step 9764, loss: 0.6565889120101929\n",
      "step 9765, loss: 0.5504040718078613\n",
      "step 9766, loss: 0.5425055623054504\n",
      "step 9767, loss: 0.6459222435951233\n",
      "step 9768, loss: 0.7036088109016418\n",
      "step 9769, loss: 0.7220867276191711\n",
      "step 9770, loss: 0.5586022734642029\n",
      "step 9771, loss: 0.6700901985168457\n",
      "step 9772, loss: 0.6507564783096313\n",
      "step 9773, loss: 0.6432608962059021\n",
      "step 9774, loss: 0.8215768337249756\n",
      "step 9775, loss: 0.5603001117706299\n",
      "step 9776, loss: 0.5568565726280212\n",
      "step 9777, loss: 0.8477311730384827\n",
      "step 9778, loss: 0.665418803691864\n",
      "step 9779, loss: 0.5993852615356445\n",
      "step 9780, loss: 0.783696174621582\n",
      "step 9781, loss: 0.7084587812423706\n",
      "step 9782, loss: 0.7600441575050354\n",
      "step 9783, loss: 0.6095689535140991\n",
      "step 9784, loss: 0.6915499567985535\n",
      "step 9785, loss: 0.5420071482658386\n",
      "step 9786, loss: 0.7834913730621338\n",
      "step 9787, loss: 0.843974769115448\n",
      "step 9788, loss: 0.6933534145355225\n",
      "step 9789, loss: 0.6989176869392395\n",
      "step 9790, loss: 0.74465012550354\n",
      "step 9791, loss: 0.5587676167488098\n",
      "step 9792, loss: 0.790014922618866\n",
      "step 9793, loss: 0.6819123029708862\n",
      "step 9794, loss: 0.840545117855072\n",
      "step 9795, loss: 0.8041964769363403\n",
      "step 9796, loss: 0.6365630626678467\n",
      "step 9797, loss: 0.6921661496162415\n",
      "step 9798, loss: 0.763152539730072\n",
      "step 9799, loss: 0.678524374961853\n",
      "step 9800, loss: 0.5325354933738708\n",
      "step 9801, loss: 0.844621479511261\n",
      "step 9802, loss: 0.7167268395423889\n",
      "step 9803, loss: 0.6161057353019714\n",
      "step 9804, loss: 0.6301314234733582\n",
      "step 9805, loss: 0.6842321753501892\n",
      "step 9806, loss: 0.8049754500389099\n",
      "step 9807, loss: 0.5858592391014099\n",
      "step 9808, loss: 0.6853369474411011\n",
      "step 9809, loss: 0.6751300096511841\n",
      "step 9810, loss: 0.6260554790496826\n",
      "step 9811, loss: 0.8199021220207214\n",
      "step 9812, loss: 0.6906604766845703\n",
      "step 9813, loss: 0.8228464722633362\n",
      "step 9814, loss: 0.7673854827880859\n",
      "step 9815, loss: 0.7826221585273743\n",
      "step 9816, loss: 0.8304548263549805\n",
      "step 9817, loss: 0.49809128046035767\n",
      "step 9818, loss: 0.624780535697937\n",
      "step 9819, loss: 0.6330476403236389\n",
      "step 9820, loss: 0.8114889860153198\n",
      "step 9821, loss: 0.5561119914054871\n",
      "step 9822, loss: 0.5933417677879333\n",
      "step 9823, loss: 0.5803661346435547\n",
      "step 9824, loss: 0.5193302631378174\n",
      "step 9825, loss: 0.5688937902450562\n",
      "step 9826, loss: 0.7091656923294067\n",
      "step 9827, loss: 0.5741603374481201\n",
      "step 9828, loss: 0.7018271088600159\n",
      "step 9829, loss: 0.7276504039764404\n",
      "step 9830, loss: 0.6146437525749207\n",
      "step 9831, loss: 0.7250356078147888\n",
      "step 9832, loss: 0.716285765171051\n",
      "step 9833, loss: 0.6961411237716675\n",
      "step 9834, loss: 0.6363990306854248\n",
      "step 9835, loss: 0.8426647782325745\n",
      "step 9836, loss: 0.5760373473167419\n",
      "step 9837, loss: 0.6095489263534546\n",
      "step 9838, loss: 0.6945635676383972\n",
      "step 9839, loss: 0.6634013652801514\n",
      "step 9840, loss: 0.7180372476577759\n",
      "step 9841, loss: 0.5370663404464722\n",
      "step 9842, loss: 0.8272408246994019\n",
      "step 9843, loss: 0.7206908464431763\n",
      "step 9844, loss: 0.7183414101600647\n",
      "step 9845, loss: 0.6373054385185242\n",
      "step 9846, loss: 0.7621043920516968\n",
      "step 9847, loss: 0.6016002893447876\n",
      "step 9848, loss: 0.7518429756164551\n",
      "step 9849, loss: 0.8627528548240662\n",
      "step 9850, loss: 0.7195887565612793\n",
      "step 9851, loss: 0.7487831711769104\n",
      "step 9852, loss: 0.8866182565689087\n",
      "step 9853, loss: 0.6966028809547424\n",
      "step 9854, loss: 0.6392948031425476\n",
      "step 9855, loss: 0.7107235789299011\n",
      "step 9856, loss: 0.5649381875991821\n",
      "step 9857, loss: 0.5964810252189636\n",
      "step 9858, loss: 0.5823189616203308\n",
      "step 9859, loss: 0.8230062127113342\n",
      "step 9860, loss: 0.6414781212806702\n",
      "step 9861, loss: 0.5639557838439941\n",
      "step 9862, loss: 0.722051739692688\n",
      "step 9863, loss: 0.7226424813270569\n",
      "step 9864, loss: 0.7001937627792358\n",
      "step 9865, loss: 0.544650137424469\n",
      "step 9866, loss: 0.6132463216781616\n",
      "step 9867, loss: 0.5572956800460815\n",
      "step 9868, loss: 0.6057236194610596\n",
      "step 9869, loss: 0.593230128288269\n",
      "step 9870, loss: 0.7350829839706421\n",
      "step 9871, loss: 0.47378918528556824\n",
      "step 9872, loss: 0.7181611657142639\n",
      "step 9873, loss: 0.6637992262840271\n",
      "step 9874, loss: 0.576630175113678\n",
      "step 9875, loss: 0.6464973092079163\n",
      "step 9876, loss: 0.587300181388855\n",
      "step 9877, loss: 0.6435427069664001\n",
      "step 9878, loss: 0.5978981852531433\n",
      "step 9879, loss: 0.5355312824249268\n",
      "step 9880, loss: 0.6298447251319885\n",
      "step 9881, loss: 0.601566731929779\n",
      "step 9882, loss: 0.6795255541801453\n",
      "step 9883, loss: 0.6720123887062073\n",
      "step 9884, loss: 0.7945120930671692\n",
      "step 9885, loss: 0.5448878407478333\n",
      "step 9886, loss: 0.5541373491287231\n",
      "step 9887, loss: 0.5272781252861023\n",
      "step 9888, loss: 0.5725901126861572\n",
      "step 9889, loss: 0.48554664850234985\n",
      "step 9890, loss: 0.562254011631012\n",
      "step 9891, loss: 0.5616480112075806\n",
      "step 9892, loss: 0.35154080390930176\n",
      "step 9893, loss: 0.315911203622818\n",
      "step 9894, loss: 0.3040922284126282\n",
      "step 9895, loss: 0.4254646897315979\n",
      "step 9896, loss: 0.3412359356880188\n",
      "step 9897, loss: 0.3262910544872284\n",
      "step 9898, loss: 0.3853905498981476\n",
      "step 9899, loss: 0.35522183775901794\n",
      "step 9900, loss: 0.30160731077194214\n",
      "step 9901, loss: 0.37206488847732544\n",
      "step 9902, loss: 0.34967637062072754\n",
      "step 9903, loss: 0.2455538958311081\n",
      "step 9904, loss: 0.3918072283267975\n",
      "step 9905, loss: 0.2998729348182678\n",
      "step 9906, loss: 0.38397765159606934\n",
      "step 9907, loss: 0.27866894006729126\n",
      "step 9908, loss: 0.3792729377746582\n",
      "step 9909, loss: 0.30745458602905273\n",
      "step 9910, loss: 0.31632691621780396\n",
      "step 9911, loss: 0.4297006130218506\n",
      "step 9912, loss: 0.31136083602905273\n",
      "step 9913, loss: 0.4387604892253876\n",
      "step 9914, loss: 0.3807506263256073\n",
      "step 9915, loss: 0.314405620098114\n",
      "step 9916, loss: 0.351832777261734\n",
      "step 9917, loss: 0.26787087321281433\n",
      "step 9918, loss: 0.3349989354610443\n",
      "step 9919, loss: 0.4664999544620514\n",
      "step 9920, loss: 0.38211938738822937\n",
      "step 9921, loss: 0.420743852853775\n",
      "step 9922, loss: 0.29786956310272217\n",
      "step 9923, loss: 0.6042740345001221\n",
      "step 9924, loss: 0.61195969581604\n",
      "step 9925, loss: 0.6960886716842651\n",
      "step 9926, loss: 0.5870863795280457\n",
      "step 9927, loss: 0.6184648275375366\n",
      "step 9928, loss: 0.6384409070014954\n",
      "step 9929, loss: 0.5889593362808228\n",
      "step 9930, loss: 0.5017972588539124\n",
      "step 9931, loss: 0.5893587470054626\n",
      "step 9932, loss: 0.5699600577354431\n",
      "step 9933, loss: 0.5689992308616638\n",
      "step 9934, loss: 0.4909968376159668\n",
      "step 9935, loss: 0.5517385005950928\n",
      "step 9936, loss: 0.5508058667182922\n",
      "step 9937, loss: 0.5709617733955383\n",
      "step 9938, loss: 0.5492187738418579\n",
      "step 9939, loss: 0.6706200838088989\n",
      "step 9940, loss: 0.4554910361766815\n",
      "step 9941, loss: 0.5977060198783875\n",
      "step 9942, loss: 0.5213820338249207\n",
      "step 9943, loss: 0.5430410504341125\n",
      "step 9944, loss: 0.5423281192779541\n",
      "step 9945, loss: 0.45433488488197327\n",
      "step 9946, loss: 0.5335932970046997\n",
      "step 9947, loss: 0.6257414817810059\n",
      "step 9948, loss: 0.5411848425865173\n",
      "step 9949, loss: 0.6308766007423401\n",
      "step 9950, loss: 0.6462439894676208\n",
      "step 9951, loss: 0.5207681655883789\n",
      "step 9952, loss: 0.5223850607872009\n",
      "step 9953, loss: 0.6532031893730164\n",
      "step 9954, loss: 0.7090229988098145\n",
      "step 9955, loss: 0.5495350360870361\n",
      "step 9956, loss: 0.4580105245113373\n",
      "step 9957, loss: 0.5500609874725342\n",
      "step 9958, loss: 0.6261422038078308\n",
      "step 9959, loss: 0.46505439281463623\n",
      "step 9960, loss: 0.5292685627937317\n",
      "step 9961, loss: 0.462485134601593\n",
      "step 9962, loss: 0.4514170289039612\n",
      "step 9963, loss: 0.36252230405807495\n",
      "step 9964, loss: 0.32347163558006287\n",
      "step 9965, loss: 0.299736887216568\n",
      "step 9966, loss: 0.4313885271549225\n",
      "step 9967, loss: 0.3974751830101013\n",
      "step 9968, loss: 0.3218204975128174\n",
      "step 9969, loss: 0.4475516676902771\n",
      "step 9970, loss: 0.6175887584686279\n",
      "step 9971, loss: 0.4387771189212799\n",
      "step 9972, loss: 0.4403740167617798\n",
      "step 9973, loss: 0.4024622142314911\n",
      "step 9974, loss: 0.3451322913169861\n",
      "step 9975, loss: 0.2277098149061203\n",
      "step 9976, loss: 0.7340347170829773\n",
      "step 9977, loss: 0.7213704586029053\n",
      "step 9978, loss: 0.7426344156265259\n",
      "step 9979, loss: 0.6366093754768372\n",
      "step 9980, loss: 0.7057836055755615\n",
      "step 9981, loss: 0.6152535676956177\n",
      "step 9982, loss: 0.6777859330177307\n",
      "step 9983, loss: 0.5860508680343628\n",
      "step 9984, loss: 0.578833818435669\n",
      "step 9985, loss: 0.7362063527107239\n",
      "step 9986, loss: 0.6260460019111633\n",
      "step 9987, loss: 0.7636579275131226\n",
      "step 9988, loss: 0.7190138101577759\n",
      "step 9989, loss: 0.6709210872650146\n",
      "step 9990, loss: 0.6025300621986389\n",
      "step 9991, loss: 0.6931714415550232\n",
      "step 9992, loss: 0.6085246801376343\n",
      "step 9993, loss: 0.519493579864502\n",
      "step 9994, loss: 0.6528187990188599\n",
      "step 9995, loss: 0.7142676115036011\n",
      "step 9996, loss: 0.6703659892082214\n",
      "step 9997, loss: 0.6239723563194275\n",
      "step 9998, loss: 0.7495691776275635\n",
      "step 9999, loss: 0.7375993132591248\n",
      "step 10000, loss: 0.9640672206878662\n",
      "step 10001, loss: 0.7647898197174072\n",
      "step 10002, loss: 0.6435261368751526\n",
      "step 10003, loss: 0.7117915749549866\n",
      "step 10004, loss: 0.6973865628242493\n",
      "step 10005, loss: 0.5674707889556885\n",
      "step 10006, loss: 0.6621277332305908\n",
      "step 10007, loss: 0.9612565040588379\n",
      "step 10008, loss: 0.5662393569946289\n",
      "step 10009, loss: 0.7342622876167297\n",
      "step 10010, loss: 0.7268759608268738\n",
      "step 10011, loss: 0.7529362440109253\n",
      "step 10012, loss: 0.6932571530342102\n",
      "step 10013, loss: 0.7727158665657043\n",
      "step 10014, loss: 0.5560009479522705\n",
      "step 10015, loss: 0.6454342007637024\n",
      "step 10016, loss: 0.5892367362976074\n",
      "step 10017, loss: 0.618694543838501\n",
      "step 10018, loss: 0.6313795447349548\n",
      "step 10019, loss: 0.7211148738861084\n",
      "step 10020, loss: 0.6343773007392883\n",
      "step 10021, loss: 0.5794265866279602\n",
      "step 10022, loss: 0.45945262908935547\n",
      "step 10023, loss: 0.6779505610466003\n",
      "step 10024, loss: 0.5728063583374023\n",
      "step 10025, loss: 0.6240622997283936\n",
      "step 10026, loss: 0.6255140900611877\n",
      "step 10027, loss: 0.8361436128616333\n",
      "step 10028, loss: 0.6287209987640381\n",
      "step 10029, loss: 0.6162626147270203\n",
      "step 10030, loss: 0.6080936789512634\n",
      "step 10031, loss: 0.6201807856559753\n",
      "step 10032, loss: 0.6494158506393433\n",
      "step 10033, loss: 0.6783589720726013\n",
      "step 10034, loss: 0.7414688467979431\n",
      "step 10035, loss: 0.7679977416992188\n",
      "step 10036, loss: 0.6846847534179688\n",
      "step 10037, loss: 0.672935426235199\n",
      "step 10038, loss: 0.6256394982337952\n",
      "step 10039, loss: 0.620922863483429\n",
      "step 10040, loss: 0.8120934367179871\n",
      "step 10041, loss: 0.7778425812721252\n",
      "step 10042, loss: 0.6237984895706177\n",
      "step 10043, loss: 0.5589255094528198\n",
      "step 10044, loss: 0.8114335536956787\n",
      "step 10045, loss: 0.8167660236358643\n",
      "step 10046, loss: 0.767939567565918\n",
      "step 10047, loss: 0.7050399780273438\n",
      "step 10048, loss: 0.6296045184135437\n",
      "step 10049, loss: 0.7094407081604004\n",
      "step 10050, loss: 0.5835620164871216\n",
      "step 10051, loss: 0.7135242819786072\n",
      "step 10052, loss: 0.6492496132850647\n",
      "step 10053, loss: 0.581654965877533\n",
      "step 10054, loss: 0.6969273686408997\n",
      "step 10055, loss: 0.715490996837616\n",
      "step 10056, loss: 0.6782752871513367\n",
      "step 10057, loss: 0.5955336093902588\n",
      "step 10058, loss: 0.610064685344696\n",
      "step 10059, loss: 0.6084355115890503\n",
      "step 10060, loss: 0.6758345365524292\n",
      "step 10061, loss: 0.6743399500846863\n",
      "step 10062, loss: 0.7142670154571533\n",
      "step 10063, loss: 0.7331218719482422\n",
      "step 10064, loss: 0.5232256054878235\n",
      "step 10065, loss: 0.6823994517326355\n",
      "step 10066, loss: 0.4446796774864197\n",
      "step 10067, loss: 0.3416133522987366\n",
      "step 10068, loss: 0.42929014563560486\n",
      "step 10069, loss: 0.4559309184551239\n",
      "step 10070, loss: 0.3314380645751953\n",
      "step 10071, loss: 0.4375499188899994\n",
      "step 10072, loss: 0.31788429617881775\n",
      "step 10073, loss: 0.3656807839870453\n",
      "step 10074, loss: 0.34199434518814087\n",
      "step 10075, loss: 0.4813510775566101\n",
      "step 10076, loss: 0.45856773853302\n",
      "step 10077, loss: 0.4501904547214508\n",
      "step 10078, loss: 0.424903929233551\n",
      "step 10079, loss: 0.45445501804351807\n",
      "step 10080, loss: 0.5455428957939148\n",
      "step 10081, loss: 0.7451152801513672\n",
      "step 10082, loss: 0.614449143409729\n",
      "step 10083, loss: 0.6674651503562927\n",
      "step 10084, loss: 0.6189771890640259\n",
      "step 10085, loss: 0.6030889749526978\n",
      "step 10086, loss: 0.5156680941581726\n",
      "step 10087, loss: 0.5348946452140808\n",
      "step 10088, loss: 0.5189107060432434\n",
      "step 10089, loss: 0.5585746765136719\n",
      "step 10090, loss: 0.630501389503479\n",
      "step 10091, loss: 0.7084870934486389\n",
      "step 10092, loss: 0.5425267219543457\n",
      "step 10093, loss: 0.6476620435714722\n",
      "step 10094, loss: 0.6458410620689392\n",
      "step 10095, loss: 0.6035665273666382\n",
      "step 10096, loss: 0.520796537399292\n",
      "step 10097, loss: 0.7176628708839417\n",
      "step 10098, loss: 0.552133321762085\n",
      "step 10099, loss: 0.5927884578704834\n",
      "step 10100, loss: 0.5799814462661743\n",
      "step 10101, loss: 0.6216902732849121\n",
      "step 10102, loss: 0.6920461058616638\n",
      "step 10103, loss: 0.5098020434379578\n",
      "step 10104, loss: 0.8106862902641296\n",
      "step 10105, loss: 0.7490237355232239\n",
      "step 10106, loss: 0.5720279216766357\n",
      "step 10107, loss: 0.6855558156967163\n",
      "step 10108, loss: 0.6789426803588867\n",
      "step 10109, loss: 0.626314103603363\n",
      "step 10110, loss: 0.6557912826538086\n",
      "step 10111, loss: 0.5624459981918335\n",
      "step 10112, loss: 0.8754035830497742\n",
      "step 10113, loss: 0.5918290615081787\n",
      "step 10114, loss: 0.619076132774353\n",
      "step 10115, loss: 0.6327471733093262\n",
      "step 10116, loss: 0.6910446286201477\n",
      "step 10117, loss: 0.6380279064178467\n",
      "step 10118, loss: 0.5795204043388367\n",
      "step 10119, loss: 0.5350283980369568\n",
      "step 10120, loss: 0.5772293210029602\n",
      "step 10121, loss: 0.6024392247200012\n",
      "step 10122, loss: 0.6261916160583496\n",
      "step 10123, loss: 0.5958312749862671\n",
      "step 10124, loss: 0.6833444237709045\n",
      "step 10125, loss: 0.6975016593933105\n",
      "step 10126, loss: 0.5889601707458496\n",
      "step 10127, loss: 0.5743352174758911\n",
      "step 10128, loss: 0.6103222370147705\n",
      "step 10129, loss: 0.5092055797576904\n",
      "step 10130, loss: 0.687629222869873\n",
      "step 10131, loss: 0.6844978332519531\n",
      "step 10132, loss: 0.5480479598045349\n",
      "step 10133, loss: 0.6139609217643738\n",
      "step 10134, loss: 0.4473554491996765\n",
      "step 10135, loss: 0.5780690908432007\n",
      "step 10136, loss: 0.5258482694625854\n",
      "step 10137, loss: 0.4782145619392395\n",
      "step 10138, loss: 0.4993673264980316\n",
      "step 10139, loss: 0.5125993490219116\n",
      "step 10140, loss: 0.5311437845230103\n",
      "step 10141, loss: 0.3986232876777649\n",
      "step 10142, loss: 0.4027259945869446\n",
      "step 10143, loss: 0.3452889621257782\n",
      "step 10144, loss: 0.4405345320701599\n",
      "step 10145, loss: 0.3548004925251007\n",
      "step 10146, loss: 0.4172939956188202\n",
      "step 10147, loss: 0.3818112015724182\n",
      "step 10148, loss: 0.3969140946865082\n",
      "step 10149, loss: 0.328387588262558\n",
      "step 10150, loss: 0.4346356987953186\n",
      "step 10151, loss: 0.5637925863265991\n",
      "step 10152, loss: 0.5521345734596252\n",
      "step 10153, loss: 0.6682786345481873\n",
      "step 10154, loss: 0.43238845467567444\n",
      "step 10155, loss: 0.5166600346565247\n",
      "step 10156, loss: 0.4460642337799072\n",
      "step 10157, loss: 0.6229944229125977\n",
      "step 10158, loss: 0.6238831281661987\n",
      "step 10159, loss: 0.538584291934967\n",
      "step 10160, loss: 0.6559780240058899\n",
      "step 10161, loss: 0.6827558279037476\n",
      "step 10162, loss: 0.5961320400238037\n",
      "step 10163, loss: 0.7651253938674927\n",
      "step 10164, loss: 0.552107036113739\n",
      "step 10165, loss: 0.6896162033081055\n",
      "step 10166, loss: 0.6239305138587952\n",
      "step 10167, loss: 0.758965790271759\n",
      "step 10168, loss: 0.7600674033164978\n",
      "step 10169, loss: 0.6957663297653198\n",
      "step 10170, loss: 0.596797525882721\n",
      "step 10171, loss: 0.5208698511123657\n",
      "step 10172, loss: 0.770472526550293\n",
      "step 10173, loss: 0.6246851682662964\n",
      "step 10174, loss: 0.6950717568397522\n",
      "step 10175, loss: 0.613909900188446\n",
      "step 10176, loss: 0.6852251291275024\n",
      "step 10177, loss: 0.6933729648590088\n",
      "step 10178, loss: 0.8057976961135864\n",
      "step 10179, loss: 0.7060586214065552\n",
      "step 10180, loss: 0.7966930866241455\n",
      "step 10181, loss: 0.6599620580673218\n",
      "step 10182, loss: 0.5412771701812744\n",
      "step 10183, loss: 0.8085972666740417\n",
      "step 10184, loss: 0.8079231977462769\n",
      "step 10185, loss: 0.7439236640930176\n",
      "step 10186, loss: 0.7031932473182678\n",
      "step 10187, loss: 0.7262449860572815\n",
      "step 10188, loss: 0.6959918737411499\n",
      "step 10189, loss: 0.7037373781204224\n",
      "step 10190, loss: 0.6703236699104309\n",
      "step 10191, loss: 0.7407968640327454\n",
      "step 10192, loss: 0.7747704386711121\n",
      "step 10193, loss: 0.6151339411735535\n",
      "step 10194, loss: 0.6349489092826843\n",
      "step 10195, loss: 0.6134624481201172\n",
      "step 10196, loss: 0.6682825684547424\n",
      "step 10197, loss: 0.739177405834198\n",
      "step 10198, loss: 0.584976077079773\n",
      "step 10199, loss: 0.6644448041915894\n",
      "step 10200, loss: 0.6617626547813416\n",
      "step 10201, loss: 0.7413724064826965\n",
      "step 10202, loss: 0.7259096503257751\n",
      "step 10203, loss: 0.8224415183067322\n",
      "step 10204, loss: 0.6508313417434692\n",
      "step 10205, loss: 0.5572190880775452\n",
      "step 10206, loss: 0.6270191669464111\n",
      "step 10207, loss: 0.6941150426864624\n",
      "step 10208, loss: 0.6411744952201843\n",
      "step 10209, loss: 0.9257918000221252\n",
      "step 10210, loss: 0.7030617594718933\n",
      "step 10211, loss: 0.5513737201690674\n",
      "step 10212, loss: 0.5206536650657654\n",
      "step 10213, loss: 0.6389965415000916\n",
      "step 10214, loss: 0.8227440118789673\n",
      "step 10215, loss: 0.6876460909843445\n",
      "step 10216, loss: 0.4891524016857147\n",
      "step 10217, loss: 0.70400470495224\n",
      "step 10218, loss: 0.70904541015625\n",
      "step 10219, loss: 0.6565566062927246\n",
      "step 10220, loss: 0.5748388171195984\n",
      "step 10221, loss: 0.6663411855697632\n",
      "step 10222, loss: 0.5522328615188599\n",
      "step 10223, loss: 0.6841504573822021\n",
      "step 10224, loss: 0.5471933484077454\n",
      "step 10225, loss: 0.5133519172668457\n",
      "step 10226, loss: 0.6837118864059448\n",
      "step 10227, loss: 0.6302713751792908\n",
      "step 10228, loss: 0.5147629976272583\n",
      "step 10229, loss: 0.6226826906204224\n",
      "step 10230, loss: 0.6042044758796692\n",
      "step 10231, loss: 0.5654812455177307\n",
      "step 10232, loss: 0.6133963465690613\n",
      "step 10233, loss: 0.5723831653594971\n",
      "step 10234, loss: 0.6182945370674133\n",
      "step 10235, loss: 0.6025073528289795\n",
      "step 10236, loss: 0.6379643082618713\n",
      "step 10237, loss: 0.7952053546905518\n",
      "step 10238, loss: 0.6403110027313232\n",
      "step 10239, loss: 0.5246511101722717\n",
      "step 10240, loss: 0.7527847290039062\n",
      "step 10241, loss: 0.5306982398033142\n",
      "step 10242, loss: 0.603691577911377\n",
      "step 10243, loss: 0.6011674404144287\n",
      "step 10244, loss: 0.5363895297050476\n",
      "step 10245, loss: 0.5616939067840576\n",
      "step 10246, loss: 0.7122743129730225\n",
      "step 10247, loss: 0.739287257194519\n",
      "step 10248, loss: 0.8609287738800049\n",
      "step 10249, loss: 0.8214675784111023\n",
      "step 10250, loss: 0.6057590842247009\n",
      "step 10251, loss: 0.5669854879379272\n",
      "step 10252, loss: 0.6483169198036194\n",
      "step 10253, loss: 0.6106340885162354\n",
      "step 10254, loss: 0.7137839198112488\n",
      "step 10255, loss: 0.5922165513038635\n",
      "step 10256, loss: 0.7525310516357422\n",
      "step 10257, loss: 0.5822306871414185\n",
      "step 10258, loss: 0.6117190718650818\n",
      "step 10259, loss: 0.7703894376754761\n",
      "step 10260, loss: 0.6557133197784424\n",
      "step 10261, loss: 0.5005472302436829\n",
      "step 10262, loss: 0.48504000902175903\n",
      "step 10263, loss: 0.48815351724624634\n",
      "step 10264, loss: 0.4916903078556061\n",
      "step 10265, loss: 0.539801299571991\n",
      "step 10266, loss: 0.4157141447067261\n",
      "step 10267, loss: 0.34023311734199524\n",
      "step 10268, loss: 0.2799142301082611\n",
      "step 10269, loss: 0.420658677816391\n",
      "step 10270, loss: 0.4340830445289612\n",
      "step 10271, loss: 0.22600750625133514\n",
      "step 10272, loss: 0.32116973400115967\n",
      "step 10273, loss: 0.28868183493614197\n",
      "step 10274, loss: 0.3045574128627777\n",
      "step 10275, loss: 0.29090893268585205\n",
      "step 10276, loss: 0.3696078062057495\n",
      "step 10277, loss: 0.3444156050682068\n",
      "step 10278, loss: 0.34835582971572876\n",
      "step 10279, loss: 0.33229097723960876\n",
      "step 10280, loss: 0.3443322777748108\n",
      "step 10281, loss: 0.3414614498615265\n",
      "step 10282, loss: 0.4154217839241028\n",
      "step 10283, loss: 0.33419692516326904\n",
      "step 10284, loss: 0.430593878030777\n",
      "step 10285, loss: 0.44072866439819336\n",
      "step 10286, loss: 0.30336567759513855\n",
      "step 10287, loss: 0.39513537287712097\n",
      "step 10288, loss: 0.33077284693717957\n",
      "step 10289, loss: 0.4485199451446533\n",
      "step 10290, loss: 0.38991260528564453\n",
      "step 10291, loss: 0.4126893877983093\n",
      "step 10292, loss: 0.6183409690856934\n",
      "step 10293, loss: 0.6715513467788696\n",
      "step 10294, loss: 0.5987067222595215\n",
      "step 10295, loss: 0.6484174132347107\n",
      "step 10296, loss: 0.5603119730949402\n",
      "step 10297, loss: 0.5604235529899597\n",
      "step 10298, loss: 0.6225675940513611\n",
      "step 10299, loss: 0.5937022566795349\n",
      "step 10300, loss: 0.6574761867523193\n",
      "step 10301, loss: 0.5409458875656128\n",
      "step 10302, loss: 0.5927064418792725\n",
      "step 10303, loss: 0.5373992323875427\n",
      "step 10304, loss: 0.48599985241889954\n",
      "step 10305, loss: 0.626724898815155\n",
      "step 10306, loss: 0.5988940596580505\n",
      "step 10307, loss: 0.6269503831863403\n",
      "step 10308, loss: 0.5235980749130249\n",
      "step 10309, loss: 0.63394695520401\n",
      "step 10310, loss: 0.5207296013832092\n",
      "step 10311, loss: 0.5408267378807068\n",
      "step 10312, loss: 0.622785210609436\n",
      "step 10313, loss: 0.5352212190628052\n",
      "step 10314, loss: 0.5639695525169373\n",
      "step 10315, loss: 0.5683179497718811\n",
      "step 10316, loss: 0.693486213684082\n",
      "step 10317, loss: 0.5243367552757263\n",
      "step 10318, loss: 0.7976561784744263\n",
      "step 10319, loss: 0.6537065505981445\n",
      "step 10320, loss: 0.44679638743400574\n",
      "step 10321, loss: 0.5092706084251404\n",
      "step 10322, loss: 0.534604549407959\n",
      "step 10323, loss: 0.5847954154014587\n",
      "step 10324, loss: 0.6796258687973022\n",
      "step 10325, loss: 0.6102601885795593\n",
      "step 10326, loss: 0.5191760659217834\n",
      "step 10327, loss: 0.7252212762832642\n",
      "step 10328, loss: 0.650322437286377\n",
      "step 10329, loss: 0.47435712814331055\n",
      "step 10330, loss: 0.5386107563972473\n",
      "step 10331, loss: 0.6444926857948303\n",
      "step 10332, loss: 0.5207431316375732\n",
      "step 10333, loss: 0.720930814743042\n",
      "step 10334, loss: 0.5165256857872009\n",
      "step 10335, loss: 0.5532252192497253\n",
      "step 10336, loss: 0.619348406791687\n",
      "step 10337, loss: 0.650348424911499\n",
      "step 10338, loss: 0.7227067351341248\n",
      "step 10339, loss: 0.7407379746437073\n",
      "step 10340, loss: 0.5448046922683716\n",
      "step 10341, loss: 0.4939020276069641\n",
      "step 10342, loss: 0.4919653534889221\n",
      "step 10343, loss: 0.5764554142951965\n",
      "step 10344, loss: 0.5423325896263123\n",
      "step 10345, loss: 0.5977274179458618\n",
      "step 10346, loss: 0.6895259618759155\n",
      "step 10347, loss: 0.6073606014251709\n",
      "step 10348, loss: 0.6146616339683533\n",
      "step 10349, loss: 0.5749747157096863\n",
      "step 10350, loss: 0.5727626085281372\n",
      "step 10351, loss: 0.7114670276641846\n",
      "step 10352, loss: 0.4866284132003784\n",
      "step 10353, loss: 0.5578539967536926\n",
      "step 10354, loss: 0.4901168644428253\n",
      "step 10355, loss: 0.6157270669937134\n",
      "step 10356, loss: 0.5153464078903198\n",
      "step 10357, loss: 0.5324651002883911\n",
      "step 10358, loss: 0.6867583990097046\n",
      "step 10359, loss: 0.5324288606643677\n",
      "step 10360, loss: 0.6202529072761536\n",
      "step 10361, loss: 0.6965207457542419\n",
      "step 10362, loss: 0.5747101902961731\n",
      "step 10363, loss: 0.6690828800201416\n",
      "step 10364, loss: 0.7133121490478516\n",
      "step 10365, loss: 0.5653795003890991\n",
      "step 10366, loss: 0.6931102871894836\n",
      "step 10367, loss: 0.5974210500717163\n",
      "step 10368, loss: 0.5592756271362305\n",
      "step 10369, loss: 0.495600163936615\n",
      "step 10370, loss: 0.5213125944137573\n",
      "step 10371, loss: 0.49853408336639404\n",
      "step 10372, loss: 0.6271881461143494\n",
      "step 10373, loss: 0.5977049469947815\n",
      "step 10374, loss: 0.6879293322563171\n",
      "step 10375, loss: 0.47178950905799866\n",
      "step 10376, loss: 0.6744681596755981\n",
      "step 10377, loss: 0.6166242957115173\n",
      "step 10378, loss: 0.5190867781639099\n",
      "step 10379, loss: 0.5183271765708923\n",
      "step 10380, loss: 0.5875240564346313\n",
      "step 10381, loss: 0.46854230761528015\n",
      "step 10382, loss: 0.5993852019309998\n",
      "step 10383, loss: 0.6614303588867188\n",
      "step 10384, loss: 0.5406384468078613\n",
      "step 10385, loss: 0.5883045196533203\n",
      "step 10386, loss: 0.6488515138626099\n",
      "step 10387, loss: 0.5257833003997803\n",
      "step 10388, loss: 0.4412582516670227\n",
      "step 10389, loss: 0.3700619041919708\n",
      "step 10390, loss: 0.3831462860107422\n",
      "step 10391, loss: 0.3401932120323181\n",
      "step 10392, loss: 0.3559192419052124\n",
      "step 10393, loss: 0.43002063035964966\n",
      "step 10394, loss: 0.5212270617485046\n",
      "step 10395, loss: 0.4488506615161896\n",
      "step 10396, loss: 0.5079848766326904\n",
      "step 10397, loss: 0.26957258582115173\n",
      "step 10398, loss: 0.5181317329406738\n",
      "step 10399, loss: 0.5594522953033447\n",
      "step 10400, loss: 0.3447573781013489\n",
      "step 10401, loss: 0.31932345032691956\n",
      "step 10402, loss: 0.36507686972618103\n",
      "step 10403, loss: 0.45767003297805786\n",
      "step 10404, loss: 0.42192214727401733\n",
      "step 10405, loss: 0.3965533971786499\n",
      "step 10406, loss: 0.35549551248550415\n",
      "step 10407, loss: 0.3156772255897522\n",
      "step 10408, loss: 0.4278266429901123\n",
      "step 10409, loss: 0.35154107213020325\n",
      "step 10410, loss: 0.4409148097038269\n",
      "step 10411, loss: 0.3822978734970093\n",
      "step 10412, loss: 0.5950942635536194\n",
      "step 10413, loss: 0.4696081280708313\n",
      "step 10414, loss: 0.6179773211479187\n",
      "step 10415, loss: 0.5562276244163513\n",
      "step 10416, loss: 0.6522143483161926\n",
      "step 10417, loss: 0.4341371953487396\n",
      "step 10418, loss: 0.5391293168067932\n",
      "step 10419, loss: 0.48302537202835083\n",
      "step 10420, loss: 0.5013693571090698\n",
      "step 10421, loss: 0.6496179699897766\n",
      "step 10422, loss: 0.5622919201850891\n",
      "step 10423, loss: 0.4908430874347687\n",
      "step 10424, loss: 0.6024937629699707\n",
      "step 10425, loss: 0.5848532319068909\n",
      "step 10426, loss: 0.5483993887901306\n",
      "step 10427, loss: 0.7217327356338501\n",
      "step 10428, loss: 0.41537022590637207\n",
      "step 10429, loss: 0.725812554359436\n",
      "step 10430, loss: 0.6056991815567017\n",
      "step 10431, loss: 0.5066272616386414\n",
      "step 10432, loss: 0.5684411525726318\n",
      "step 10433, loss: 0.5268567800521851\n",
      "step 10434, loss: 0.6252589225769043\n",
      "step 10435, loss: 0.6844526529312134\n",
      "step 10436, loss: 0.6359870433807373\n",
      "step 10437, loss: 0.6454564929008484\n",
      "step 10438, loss: 0.7954898476600647\n",
      "step 10439, loss: 0.6685677170753479\n",
      "step 10440, loss: 0.48120489716529846\n",
      "step 10441, loss: 0.6183906197547913\n",
      "step 10442, loss: 0.7202765941619873\n",
      "step 10443, loss: 0.6151527166366577\n",
      "step 10444, loss: 0.6458702087402344\n",
      "step 10445, loss: 0.6326137781143188\n",
      "step 10446, loss: 0.5343828201293945\n",
      "step 10447, loss: 0.6767547130584717\n",
      "step 10448, loss: 0.715721070766449\n",
      "step 10449, loss: 0.7713702321052551\n",
      "step 10450, loss: 0.7377725839614868\n",
      "step 10451, loss: 0.5909040570259094\n",
      "step 10452, loss: 0.6986026167869568\n",
      "step 10453, loss: 0.5036301612854004\n",
      "step 10454, loss: 0.5377596020698547\n",
      "step 10455, loss: 0.6039368510246277\n",
      "step 10456, loss: 0.5906676054000854\n",
      "step 10457, loss: 0.7988548278808594\n",
      "step 10458, loss: 0.7427553534507751\n",
      "step 10459, loss: 0.824945867061615\n",
      "step 10460, loss: 0.7131086587905884\n",
      "step 10461, loss: 0.7059488296508789\n",
      "step 10462, loss: 0.565294623374939\n",
      "step 10463, loss: 0.5010266304016113\n",
      "step 10464, loss: 0.6417542099952698\n",
      "step 10465, loss: 0.5383415222167969\n",
      "step 10466, loss: 0.7947961688041687\n",
      "step 10467, loss: 0.7988152503967285\n",
      "step 10468, loss: 0.7477931976318359\n",
      "step 10469, loss: 0.6386367678642273\n",
      "step 10470, loss: 0.6722708940505981\n",
      "step 10471, loss: 0.8082224726676941\n",
      "step 10472, loss: 0.6415941119194031\n",
      "step 10473, loss: 0.8457578420639038\n",
      "step 10474, loss: 0.7111262679100037\n",
      "step 10475, loss: 0.6700850129127502\n",
      "step 10476, loss: 0.8240259885787964\n",
      "step 10477, loss: 0.8327235579490662\n",
      "step 10478, loss: 0.6634394526481628\n",
      "step 10479, loss: 0.7172154188156128\n",
      "step 10480, loss: 0.7670586705207825\n",
      "step 10481, loss: 0.8773754239082336\n",
      "step 10482, loss: 0.8763114809989929\n",
      "step 10483, loss: 0.8879368901252747\n",
      "step 10484, loss: 0.5691191554069519\n",
      "step 10485, loss: 0.7276464700698853\n",
      "step 10486, loss: 0.7613510489463806\n",
      "step 10487, loss: 0.7430830001831055\n",
      "step 10488, loss: 0.727452278137207\n",
      "step 10489, loss: 0.6223228573799133\n",
      "step 10490, loss: 0.5825534462928772\n",
      "step 10491, loss: 0.6441702246665955\n",
      "step 10492, loss: 0.7550951838493347\n",
      "step 10493, loss: 0.6279270648956299\n",
      "step 10494, loss: 0.7084612250328064\n",
      "step 10495, loss: 0.7212222218513489\n",
      "step 10496, loss: 0.6740838289260864\n",
      "step 10497, loss: 0.6014769077301025\n",
      "step 10498, loss: 0.5077300071716309\n",
      "step 10499, loss: 0.43435409665107727\n",
      "step 10500, loss: 0.4794211685657501\n",
      "step 10501, loss: 0.5472548007965088\n",
      "step 10502, loss: 0.6493989825248718\n",
      "step 10503, loss: 0.32396772503852844\n",
      "step 10504, loss: 0.5647211074829102\n",
      "step 10505, loss: 0.432948499917984\n",
      "step 10506, loss: 0.35666099190711975\n",
      "step 10507, loss: 0.35790324211120605\n",
      "step 10508, loss: 0.36885079741477966\n",
      "step 10509, loss: 0.4522530734539032\n",
      "step 10510, loss: 0.6582325100898743\n",
      "step 10511, loss: 0.43847858905792236\n",
      "step 10512, loss: 0.419974684715271\n",
      "step 10513, loss: 0.3990526795387268\n",
      "step 10514, loss: 0.3341454267501831\n",
      "step 10515, loss: 0.241628959774971\n",
      "step 10516, loss: 0.2897326946258545\n",
      "step 10517, loss: 0.49357154965400696\n",
      "step 10518, loss: 0.6588919162750244\n",
      "step 10519, loss: 0.6097067594528198\n",
      "step 10520, loss: 0.6019116044044495\n",
      "step 10521, loss: 0.5788324475288391\n",
      "step 10522, loss: 0.6015585064888\n",
      "step 10523, loss: 0.7176836729049683\n",
      "step 10524, loss: 0.6024797558784485\n",
      "step 10525, loss: 0.6154237389564514\n",
      "step 10526, loss: 0.5884146690368652\n",
      "step 10527, loss: 0.7510433793067932\n",
      "step 10528, loss: 0.5804131627082825\n",
      "step 10529, loss: 0.5709542036056519\n",
      "step 10530, loss: 0.6525688767433167\n",
      "step 10531, loss: 0.4956311583518982\n",
      "step 10532, loss: 0.4002598226070404\n",
      "step 10533, loss: 0.4706978499889374\n",
      "step 10534, loss: 0.6036075949668884\n",
      "step 10535, loss: 0.5284625887870789\n",
      "step 10536, loss: 0.6367725133895874\n",
      "step 10537, loss: 0.7211166620254517\n",
      "step 10538, loss: 0.6083815097808838\n",
      "step 10539, loss: 0.6342298984527588\n",
      "step 10540, loss: 0.6238301396369934\n",
      "step 10541, loss: 0.5641300082206726\n",
      "step 10542, loss: 0.5833345055580139\n",
      "step 10543, loss: 0.5384827256202698\n",
      "step 10544, loss: 0.6298154592514038\n",
      "step 10545, loss: 0.7114249467849731\n",
      "step 10546, loss: 0.6301826238632202\n",
      "step 10547, loss: 0.5385655760765076\n",
      "step 10548, loss: 0.6261581182479858\n",
      "step 10549, loss: 0.7513211965560913\n",
      "step 10550, loss: 0.5735952258110046\n",
      "step 10551, loss: 0.7148756384849548\n",
      "step 10552, loss: 0.5351492166519165\n",
      "step 10553, loss: 0.612588107585907\n",
      "step 10554, loss: 0.527275562286377\n",
      "step 10555, loss: 0.6574642658233643\n",
      "step 10556, loss: 0.7076975703239441\n",
      "step 10557, loss: 0.6114252209663391\n",
      "step 10558, loss: 0.5995358228683472\n",
      "step 10559, loss: 0.6800076961517334\n",
      "step 10560, loss: 0.5044638514518738\n",
      "step 10561, loss: 0.5257738828659058\n",
      "step 10562, loss: 0.5487985610961914\n",
      "step 10563, loss: 0.2952580451965332\n",
      "step 10564, loss: 0.4441596269607544\n",
      "step 10565, loss: 0.3582920730113983\n",
      "step 10566, loss: 0.3501039147377014\n",
      "step 10567, loss: 0.33464786410331726\n",
      "step 10568, loss: 0.2424844652414322\n",
      "step 10569, loss: 0.3303054869174957\n",
      "step 10570, loss: 0.28252139687538147\n",
      "step 10571, loss: 0.2964286208152771\n",
      "step 10572, loss: 0.3963889181613922\n",
      "step 10573, loss: 0.44475001096725464\n",
      "step 10574, loss: 0.3674639165401459\n",
      "step 10575, loss: 0.3062886893749237\n",
      "step 10576, loss: 0.3528803288936615\n",
      "step 10577, loss: 0.5084257125854492\n",
      "step 10578, loss: 0.6444479823112488\n",
      "step 10579, loss: 0.5691733956336975\n",
      "step 10580, loss: 0.5550039410591125\n",
      "step 10581, loss: 0.6658915281295776\n",
      "step 10582, loss: 0.5804538130760193\n",
      "step 10583, loss: 0.49092212319374084\n",
      "step 10584, loss: 0.5447118878364563\n",
      "step 10585, loss: 0.5485696792602539\n",
      "step 10586, loss: 0.4952661097049713\n",
      "step 10587, loss: 0.49335265159606934\n",
      "step 10588, loss: 0.4915274381637573\n",
      "step 10589, loss: 0.6657247543334961\n",
      "step 10590, loss: 0.5215163826942444\n",
      "step 10591, loss: 0.6164047718048096\n",
      "step 10592, loss: 0.44102126359939575\n",
      "step 10593, loss: 0.5603044033050537\n",
      "step 10594, loss: 0.5533570647239685\n",
      "step 10595, loss: 0.5364727973937988\n",
      "step 10596, loss: 0.6571528315544128\n",
      "step 10597, loss: 0.6950480937957764\n",
      "step 10598, loss: 0.5772853493690491\n",
      "step 10599, loss: 0.6971254944801331\n",
      "step 10600, loss: 0.6535982489585876\n",
      "step 10601, loss: 0.7391438484191895\n",
      "step 10602, loss: 0.757161021232605\n",
      "step 10603, loss: 0.7621526718139648\n",
      "step 10604, loss: 0.6336211562156677\n",
      "step 10605, loss: 0.732962429523468\n",
      "step 10606, loss: 0.7112005949020386\n",
      "step 10607, loss: 0.7047232389450073\n",
      "step 10608, loss: 0.5686607360839844\n",
      "step 10609, loss: 0.6575943827629089\n",
      "step 10610, loss: 0.8605866432189941\n",
      "step 10611, loss: 0.6326429843902588\n",
      "step 10612, loss: 0.6626541018486023\n",
      "step 10613, loss: 0.6658428311347961\n",
      "step 10614, loss: 0.5117998719215393\n",
      "step 10615, loss: 0.6061972975730896\n",
      "step 10616, loss: 0.9082174897193909\n",
      "step 10617, loss: 0.6853737235069275\n",
      "step 10618, loss: 0.6609747409820557\n",
      "step 10619, loss: 0.6178280711174011\n",
      "step 10620, loss: 0.5736731886863708\n",
      "step 10621, loss: 0.6776437759399414\n",
      "step 10622, loss: 0.6419112682342529\n",
      "step 10623, loss: 0.5490102171897888\n",
      "step 10624, loss: 0.6673357486724854\n",
      "step 10625, loss: 0.7058098316192627\n",
      "step 10626, loss: 0.7156185507774353\n",
      "step 10627, loss: 0.6753196716308594\n",
      "step 10628, loss: 0.635886549949646\n",
      "step 10629, loss: 0.813258171081543\n",
      "step 10630, loss: 0.6659971475601196\n",
      "step 10631, loss: 0.5161049962043762\n",
      "step 10632, loss: 0.7190186977386475\n",
      "step 10633, loss: 0.5646962523460388\n",
      "step 10634, loss: 0.6246915459632874\n",
      "step 10635, loss: 0.6384177803993225\n",
      "step 10636, loss: 0.47538843750953674\n",
      "step 10637, loss: 0.5419386625289917\n",
      "step 10638, loss: 0.6788572072982788\n",
      "step 10639, loss: 0.6359620690345764\n",
      "step 10640, loss: 0.5053882002830505\n",
      "step 10641, loss: 0.5329446792602539\n",
      "step 10642, loss: 0.6594721078872681\n",
      "step 10643, loss: 0.5460784435272217\n",
      "step 10644, loss: 0.6362630724906921\n",
      "step 10645, loss: 0.5966063737869263\n",
      "step 10646, loss: 0.5435360670089722\n",
      "step 10647, loss: 0.6469188332557678\n",
      "step 10648, loss: 0.6358628869056702\n",
      "step 10649, loss: 0.7051657438278198\n",
      "step 10650, loss: 0.5778447985649109\n",
      "step 10651, loss: 0.7522125840187073\n",
      "step 10652, loss: 0.6656163930892944\n",
      "step 10653, loss: 0.7760505080223083\n",
      "step 10654, loss: 0.5380827188491821\n",
      "step 10655, loss: 0.5418010354042053\n",
      "step 10656, loss: 0.6659616827964783\n",
      "step 10657, loss: 0.5510280728340149\n",
      "step 10658, loss: 0.7324063777923584\n",
      "step 10659, loss: 0.6178627610206604\n",
      "step 10660, loss: 0.6496056318283081\n",
      "step 10661, loss: 0.5909506678581238\n",
      "step 10662, loss: 0.5580500364303589\n",
      "step 10663, loss: 0.5251540541648865\n",
      "step 10664, loss: 0.521409809589386\n",
      "step 10665, loss: 0.6455463767051697\n",
      "step 10666, loss: 0.5746636986732483\n",
      "step 10667, loss: 0.4017089903354645\n",
      "step 10668, loss: 0.4321517050266266\n",
      "step 10669, loss: 0.2715693712234497\n",
      "step 10670, loss: 0.4388465881347656\n",
      "step 10671, loss: 0.3261592388153076\n",
      "step 10672, loss: 0.3050195276737213\n",
      "step 10673, loss: 0.2813957929611206\n",
      "step 10674, loss: 0.43590307235717773\n",
      "step 10675, loss: 0.2700943946838379\n",
      "step 10676, loss: 0.22852672636508942\n",
      "step 10677, loss: 0.29808446764945984\n",
      "step 10678, loss: 0.25173845887184143\n",
      "step 10679, loss: 0.6097845435142517\n",
      "step 10680, loss: 0.3716719448566437\n",
      "step 10681, loss: 0.47388121485710144\n",
      "step 10682, loss: 0.3035455048084259\n",
      "step 10683, loss: 0.3061891794204712\n",
      "step 10684, loss: 0.4620319604873657\n",
      "step 10685, loss: 0.3562629818916321\n",
      "step 10686, loss: 0.2839149534702301\n",
      "step 10687, loss: 0.34460213780403137\n",
      "step 10688, loss: 0.45955660939216614\n",
      "step 10689, loss: 0.6040560007095337\n",
      "step 10690, loss: 0.5528249740600586\n",
      "step 10691, loss: 0.6122886538505554\n",
      "step 10692, loss: 0.5619293451309204\n",
      "step 10693, loss: 0.6496816873550415\n",
      "step 10694, loss: 0.5472065210342407\n",
      "step 10695, loss: 0.628016471862793\n",
      "step 10696, loss: 0.6697401404380798\n",
      "step 10697, loss: 0.47714489698410034\n",
      "step 10698, loss: 0.6049501895904541\n",
      "step 10699, loss: 0.5808061957359314\n",
      "step 10700, loss: 0.5066970586776733\n",
      "step 10701, loss: 0.4392496347427368\n",
      "step 10702, loss: 0.6572258472442627\n",
      "step 10703, loss: 0.4589257836341858\n",
      "step 10704, loss: 0.5336458683013916\n",
      "step 10705, loss: 0.6512511968612671\n",
      "step 10706, loss: 0.610257089138031\n",
      "step 10707, loss: 0.505774736404419\n",
      "step 10708, loss: 0.7284646034240723\n",
      "step 10709, loss: 0.6546376347541809\n",
      "step 10710, loss: 0.6657636165618896\n",
      "step 10711, loss: 0.5900495648384094\n",
      "step 10712, loss: 0.5977290868759155\n",
      "step 10713, loss: 0.5856088399887085\n",
      "step 10714, loss: 0.5644825100898743\n",
      "step 10715, loss: 0.5904051065444946\n",
      "step 10716, loss: 0.5668769478797913\n",
      "step 10717, loss: 0.5908107161521912\n",
      "step 10718, loss: 0.6327105164527893\n",
      "step 10719, loss: 0.6700467467308044\n",
      "step 10720, loss: 0.45597362518310547\n",
      "step 10721, loss: 0.5757337808609009\n",
      "step 10722, loss: 0.6014914512634277\n",
      "step 10723, loss: 0.5096592307090759\n",
      "step 10724, loss: 0.5431788563728333\n",
      "step 10725, loss: 0.5555250644683838\n",
      "step 10726, loss: 0.7371533513069153\n",
      "step 10727, loss: 0.5128061175346375\n",
      "step 10728, loss: 0.6022837162017822\n",
      "step 10729, loss: 0.6161171197891235\n",
      "step 10730, loss: 0.6299430131912231\n",
      "step 10731, loss: 0.5229732394218445\n",
      "step 10732, loss: 0.5983846783638\n",
      "step 10733, loss: 0.656603217124939\n",
      "step 10734, loss: 0.5242998003959656\n",
      "step 10735, loss: 0.6019731760025024\n",
      "step 10736, loss: 0.6777903437614441\n",
      "step 10737, loss: 0.598010241985321\n",
      "step 10738, loss: 0.6584668159484863\n",
      "step 10739, loss: 0.606736958026886\n",
      "step 10740, loss: 0.6049823760986328\n",
      "step 10741, loss: 0.5562453269958496\n",
      "step 10742, loss: 0.5938091278076172\n",
      "step 10743, loss: 0.5092063546180725\n",
      "step 10744, loss: 0.5638448596000671\n",
      "step 10745, loss: 0.5779547095298767\n",
      "step 10746, loss: 0.6002827286720276\n",
      "step 10747, loss: 0.6780677437782288\n",
      "step 10748, loss: 0.6885218024253845\n",
      "step 10749, loss: 0.4520283639431\n",
      "step 10750, loss: 0.6032921671867371\n",
      "step 10751, loss: 0.5453620553016663\n",
      "step 10752, loss: 0.627802848815918\n",
      "step 10753, loss: 0.5843091607093811\n",
      "step 10754, loss: 0.6691033840179443\n",
      "step 10755, loss: 0.5276521444320679\n",
      "step 10756, loss: 0.5621469020843506\n",
      "step 10757, loss: 0.571418046951294\n",
      "step 10758, loss: 0.7234930992126465\n",
      "step 10759, loss: 0.6025999784469604\n",
      "step 10760, loss: 0.6695522665977478\n",
      "step 10761, loss: 0.47025248408317566\n",
      "step 10762, loss: 0.5337796211242676\n",
      "step 10763, loss: 0.5577698945999146\n",
      "step 10764, loss: 0.5217783451080322\n",
      "step 10765, loss: 0.6358881592750549\n",
      "step 10766, loss: 0.678105890750885\n",
      "step 10767, loss: 0.5558998584747314\n",
      "step 10768, loss: 0.5149052143096924\n",
      "step 10769, loss: 0.6721071600914001\n",
      "step 10770, loss: 0.6409008502960205\n",
      "step 10771, loss: 0.669580340385437\n",
      "step 10772, loss: 0.5568379759788513\n",
      "step 10773, loss: 0.44184085726737976\n",
      "step 10774, loss: 0.5340674519538879\n",
      "step 10775, loss: 0.5635793209075928\n",
      "step 10776, loss: 0.6069047451019287\n",
      "step 10777, loss: 0.6749533414840698\n",
      "step 10778, loss: 0.5299098491668701\n",
      "step 10779, loss: 0.5911244750022888\n",
      "step 10780, loss: 0.4502643644809723\n",
      "step 10781, loss: 0.5093544721603394\n",
      "step 10782, loss: 0.5250564813613892\n",
      "step 10783, loss: 0.45148324966430664\n",
      "step 10784, loss: 0.5304126739501953\n",
      "step 10785, loss: 0.5670645236968994\n",
      "step 10786, loss: 0.5573717355728149\n",
      "step 10787, loss: 0.6710259914398193\n",
      "step 10788, loss: 0.5472038984298706\n",
      "step 10789, loss: 0.6062495708465576\n",
      "step 10790, loss: 0.5152573585510254\n",
      "step 10791, loss: 0.4879611134529114\n",
      "step 10792, loss: 0.6198700666427612\n",
      "step 10793, loss: 0.4924842417240143\n",
      "step 10794, loss: 0.6205799579620361\n",
      "step 10795, loss: 0.47569739818573\n",
      "step 10796, loss: 0.6550967693328857\n",
      "step 10797, loss: 0.6268348693847656\n",
      "step 10798, loss: 0.6111101508140564\n",
      "step 10799, loss: 0.32476624846458435\n",
      "step 10800, loss: 0.4239136874675751\n",
      "step 10801, loss: 0.376292884349823\n",
      "step 10802, loss: 0.26269376277923584\n",
      "step 10803, loss: 0.3103673458099365\n",
      "step 10804, loss: 0.32662346959114075\n",
      "step 10805, loss: 0.33585861325263977\n",
      "step 10806, loss: 0.4437916874885559\n",
      "step 10807, loss: 0.2576082944869995\n",
      "step 10808, loss: 0.38209256529808044\n",
      "step 10809, loss: 0.3345951735973358\n",
      "step 10810, loss: 0.28580591082572937\n",
      "step 10811, loss: 0.287906289100647\n",
      "step 10812, loss: 0.2837923765182495\n",
      "step 10813, loss: 0.293389230966568\n",
      "step 10814, loss: 0.31941407918930054\n",
      "step 10815, loss: 0.2796917259693146\n",
      "step 10816, loss: 0.32740363478660583\n",
      "step 10817, loss: 0.45141157507896423\n",
      "step 10818, loss: 0.2936314344406128\n",
      "step 10819, loss: 0.3664492070674896\n",
      "step 10820, loss: 0.39435288310050964\n",
      "step 10821, loss: 0.4373698830604553\n",
      "step 10822, loss: 0.3336314857006073\n",
      "step 10823, loss: 0.2850210666656494\n",
      "step 10824, loss: 0.3120870888233185\n",
      "step 10825, loss: 0.3582841753959656\n",
      "step 10826, loss: 0.4136766493320465\n",
      "step 10827, loss: 0.34894368052482605\n",
      "step 10828, loss: 0.5051644444465637\n",
      "step 10829, loss: 0.6530308723449707\n",
      "step 10830, loss: 0.6170918345451355\n",
      "step 10831, loss: 0.4700634479522705\n",
      "step 10832, loss: 0.6244606971740723\n",
      "step 10833, loss: 0.6516515016555786\n",
      "step 10834, loss: 0.4732632339000702\n",
      "step 10835, loss: 0.6478918790817261\n",
      "step 10836, loss: 0.5569760203361511\n",
      "step 10837, loss: 0.6428686380386353\n",
      "step 10838, loss: 0.6452200412750244\n",
      "step 10839, loss: 0.43692994117736816\n",
      "step 10840, loss: 0.5496785044670105\n",
      "step 10841, loss: 0.6353626251220703\n",
      "step 10842, loss: 0.5562452077865601\n",
      "step 10843, loss: 0.6221464276313782\n",
      "step 10844, loss: 0.5396642088890076\n",
      "step 10845, loss: 0.568653404712677\n",
      "step 10846, loss: 0.405805379152298\n",
      "step 10847, loss: 0.656251072883606\n",
      "step 10848, loss: 0.5248543620109558\n",
      "step 10849, loss: 0.5179332494735718\n",
      "step 10850, loss: 0.5615532994270325\n",
      "step 10851, loss: 0.5223105549812317\n",
      "step 10852, loss: 0.5180061459541321\n",
      "step 10853, loss: 0.4256240129470825\n",
      "step 10854, loss: 0.5615649223327637\n",
      "step 10855, loss: 0.5633583068847656\n",
      "step 10856, loss: 0.6080971956253052\n",
      "step 10857, loss: 0.42966389656066895\n",
      "step 10858, loss: 0.6549386978149414\n",
      "step 10859, loss: 0.5011736154556274\n",
      "step 10860, loss: 0.5218813419342041\n",
      "step 10861, loss: 0.5261014699935913\n",
      "step 10862, loss: 0.531762957572937\n",
      "step 10863, loss: 0.5355888605117798\n",
      "step 10864, loss: 0.6071723699569702\n",
      "step 10865, loss: 0.5551545023918152\n",
      "step 10866, loss: 0.6897258162498474\n",
      "step 10867, loss: 0.5398352742195129\n",
      "step 10868, loss: 0.7040197253227234\n",
      "step 10869, loss: 0.6560633778572083\n",
      "step 10870, loss: 0.5234919190406799\n",
      "step 10871, loss: 0.5887171030044556\n",
      "step 10872, loss: 0.5455120205879211\n",
      "step 10873, loss: 0.6995055079460144\n",
      "step 10874, loss: 0.4474477469921112\n",
      "step 10875, loss: 0.6468456983566284\n",
      "step 10876, loss: 0.6777507066726685\n",
      "step 10877, loss: 0.7920250296592712\n",
      "step 10878, loss: 0.6469336748123169\n",
      "step 10879, loss: 0.6604339480400085\n",
      "step 10880, loss: 0.5923098921775818\n",
      "step 10881, loss: 0.42971712350845337\n",
      "step 10882, loss: 0.5929533839225769\n",
      "step 10883, loss: 0.5275560021400452\n",
      "step 10884, loss: 0.5315247774124146\n",
      "step 10885, loss: 0.5337905287742615\n",
      "step 10886, loss: 0.8193125128746033\n",
      "step 10887, loss: 0.5540710687637329\n",
      "step 10888, loss: 0.4868631064891815\n",
      "step 10889, loss: 0.6435847282409668\n",
      "step 10890, loss: 0.5081573128700256\n",
      "step 10891, loss: 0.493198424577713\n",
      "step 10892, loss: 0.7108227014541626\n",
      "step 10893, loss: 0.7773945331573486\n",
      "step 10894, loss: 0.6222160458564758\n",
      "step 10895, loss: 0.6489503383636475\n",
      "step 10896, loss: 0.529697597026825\n",
      "step 10897, loss: 0.5118821859359741\n",
      "step 10898, loss: 0.6214074492454529\n",
      "step 10899, loss: 0.5691007971763611\n",
      "step 10900, loss: 0.5260424613952637\n",
      "step 10901, loss: 0.6425966024398804\n",
      "step 10902, loss: 0.5367249846458435\n",
      "step 10903, loss: 0.6198849678039551\n",
      "step 10904, loss: 0.7089177370071411\n",
      "step 10905, loss: 0.6006295680999756\n",
      "step 10906, loss: 0.5412541627883911\n",
      "step 10907, loss: 0.5674591660499573\n",
      "step 10908, loss: 0.6109785437583923\n",
      "step 10909, loss: 0.6291407346725464\n",
      "step 10910, loss: 0.7177354693412781\n",
      "step 10911, loss: 0.4784712791442871\n",
      "step 10912, loss: 0.5243428349494934\n",
      "step 10913, loss: 0.5808036923408508\n",
      "step 10914, loss: 0.7569000720977783\n",
      "step 10915, loss: 0.6016812920570374\n",
      "step 10916, loss: 0.5733985900878906\n",
      "step 10917, loss: 0.6012770533561707\n",
      "step 10918, loss: 0.5033561587333679\n",
      "step 10919, loss: 0.5756350755691528\n",
      "step 10920, loss: 0.580849826335907\n",
      "step 10921, loss: 0.7517864108085632\n",
      "step 10922, loss: 0.5022137761116028\n",
      "step 10923, loss: 0.40680140256881714\n",
      "step 10924, loss: 0.6940739750862122\n",
      "step 10925, loss: 0.6800979375839233\n",
      "step 10926, loss: 0.5891616940498352\n",
      "step 10927, loss: 0.7897452116012573\n",
      "step 10928, loss: 0.5453826785087585\n",
      "step 10929, loss: 0.6601619720458984\n",
      "step 10930, loss: 0.594404935836792\n",
      "step 10931, loss: 0.6112733483314514\n",
      "step 10932, loss: 0.6206074953079224\n",
      "step 10933, loss: 0.6370418071746826\n",
      "step 10934, loss: 0.6292250752449036\n",
      "step 10935, loss: 0.6354433298110962\n",
      "step 10936, loss: 0.6134949922561646\n",
      "step 10937, loss: 0.625313401222229\n",
      "step 10938, loss: 0.5861064195632935\n",
      "step 10939, loss: 0.6694875359535217\n",
      "step 10940, loss: 0.7118724584579468\n",
      "step 10941, loss: 0.6893059015274048\n",
      "step 10942, loss: 0.5927438139915466\n",
      "step 10943, loss: 0.708500325679779\n",
      "step 10944, loss: 0.4224236011505127\n",
      "step 10945, loss: 0.7562777996063232\n",
      "step 10946, loss: 0.6265677213668823\n",
      "step 10947, loss: 0.6737958192825317\n",
      "step 10948, loss: 0.52047199010849\n",
      "step 10949, loss: 0.6754359602928162\n",
      "step 10950, loss: 0.5066462755203247\n",
      "step 10951, loss: 0.7916389107704163\n",
      "step 10952, loss: 0.5151994228363037\n",
      "step 10953, loss: 0.5114542245864868\n",
      "step 10954, loss: 0.6872801184654236\n",
      "step 10955, loss: 0.7505595088005066\n",
      "step 10956, loss: 0.5677715539932251\n",
      "step 10957, loss: 0.7502802014350891\n",
      "step 10958, loss: 0.4598076045513153\n",
      "step 10959, loss: 0.5059973001480103\n",
      "step 10960, loss: 0.6106921434402466\n",
      "step 10961, loss: 0.5982068777084351\n",
      "step 10962, loss: 0.7253221869468689\n",
      "step 10963, loss: 0.5271251201629639\n",
      "step 10964, loss: 0.5673079490661621\n",
      "step 10965, loss: 0.6306629776954651\n",
      "step 10966, loss: 0.5532719492912292\n",
      "step 10967, loss: 0.5988989472389221\n",
      "step 10968, loss: 0.7659027576446533\n",
      "step 10969, loss: 0.6387559771537781\n",
      "step 10970, loss: 0.6958408951759338\n",
      "step 10971, loss: 0.6715959906578064\n",
      "step 10972, loss: 0.5341418385505676\n",
      "step 10973, loss: 0.44015201926231384\n",
      "step 10974, loss: 0.5561255812644958\n",
      "step 10975, loss: 0.6887028217315674\n",
      "step 10976, loss: 0.6360057592391968\n",
      "step 10977, loss: 0.686099112033844\n",
      "step 10978, loss: 0.6490234136581421\n",
      "step 10979, loss: 0.6989778876304626\n",
      "step 10980, loss: 0.7965488433837891\n",
      "step 10981, loss: 0.4775088429450989\n",
      "step 10982, loss: 0.5473126769065857\n",
      "step 10983, loss: 0.531868040561676\n",
      "step 10984, loss: 0.5914134979248047\n",
      "step 10985, loss: 0.6447747945785522\n",
      "step 10986, loss: 0.695622444152832\n",
      "step 10987, loss: 0.48435690999031067\n",
      "step 10988, loss: 0.47684794664382935\n",
      "step 10989, loss: 0.745373547077179\n",
      "step 10990, loss: 0.6319504380226135\n",
      "step 10991, loss: 0.492256224155426\n",
      "step 10992, loss: 0.612633466720581\n",
      "step 10993, loss: 0.5026825666427612\n",
      "step 10994, loss: 0.6141043901443481\n",
      "step 10995, loss: 0.6377347707748413\n",
      "step 10996, loss: 0.7196429371833801\n",
      "step 10997, loss: 0.53704833984375\n",
      "step 10998, loss: 0.43890702724456787\n",
      "step 10999, loss: 0.6154345273971558\n",
      "step 11000, loss: 0.46633443236351013\n",
      "step 11001, loss: 0.5390418171882629\n",
      "step 11002, loss: 0.6469903588294983\n",
      "step 11003, loss: 0.48179763555526733\n",
      "step 11004, loss: 0.6348245143890381\n",
      "step 11005, loss: 0.6068612933158875\n",
      "step 11006, loss: 0.5384810566902161\n",
      "step 11007, loss: 0.5034677386283875\n",
      "step 11008, loss: 0.5067163109779358\n",
      "step 11009, loss: 0.491531640291214\n",
      "step 11010, loss: 0.4888519048690796\n",
      "step 11011, loss: 0.6048166155815125\n",
      "step 11012, loss: 0.2524470388889313\n",
      "step 11013, loss: 0.24170783162117004\n",
      "step 11014, loss: 0.38417893648147583\n",
      "step 11015, loss: 0.32564935088157654\n",
      "step 11016, loss: 0.2885207235813141\n",
      "step 11017, loss: 0.23696966469287872\n",
      "step 11018, loss: 0.2729392945766449\n",
      "step 11019, loss: 0.2203119844198227\n",
      "step 11020, loss: 0.29083195328712463\n",
      "step 11021, loss: 0.30256712436676025\n",
      "step 11022, loss: 0.18523254990577698\n",
      "step 11023, loss: 0.4287770092487335\n",
      "step 11024, loss: 0.2576175928115845\n",
      "step 11025, loss: 0.3231917917728424\n",
      "step 11026, loss: 0.36489638686180115\n",
      "step 11027, loss: 0.41321420669555664\n",
      "step 11028, loss: 0.3562004566192627\n",
      "step 11029, loss: 0.2719646096229553\n",
      "step 11030, loss: 0.2389526665210724\n",
      "step 11031, loss: 0.5826852917671204\n",
      "step 11032, loss: 0.3226431906223297\n",
      "step 11033, loss: 0.29740288853645325\n",
      "step 11034, loss: 0.35322368144989014\n",
      "step 11035, loss: 0.39280757308006287\n",
      "step 11036, loss: 0.4243808686733246\n",
      "step 11037, loss: 0.25175750255584717\n",
      "step 11038, loss: 0.4089958369731903\n",
      "step 11039, loss: 0.378814697265625\n",
      "step 11040, loss: 0.39921408891677856\n",
      "step 11041, loss: 0.4187358319759369\n",
      "step 11042, loss: 0.3967399001121521\n",
      "step 11043, loss: 0.5420825481414795\n",
      "step 11044, loss: 0.5513826608657837\n",
      "step 11045, loss: 0.5608311295509338\n",
      "step 11046, loss: 0.5199916958808899\n",
      "step 11047, loss: 0.5606129765510559\n",
      "step 11048, loss: 0.5648561716079712\n",
      "step 11049, loss: 0.571045458316803\n",
      "step 11050, loss: 0.5741769075393677\n",
      "step 11051, loss: 0.5411696434020996\n",
      "step 11052, loss: 0.5025392174720764\n",
      "step 11053, loss: 0.47831690311431885\n",
      "step 11054, loss: 0.4590769112110138\n",
      "step 11055, loss: 0.3867482841014862\n",
      "step 11056, loss: 0.47922247648239136\n",
      "step 11057, loss: 0.5281791090965271\n",
      "step 11058, loss: 0.4426703453063965\n",
      "step 11059, loss: 0.4779903292655945\n",
      "step 11060, loss: 0.5539513230323792\n",
      "step 11061, loss: 0.4900594651699066\n",
      "step 11062, loss: 0.5220714807510376\n",
      "step 11063, loss: 0.5068638920783997\n",
      "step 11064, loss: 0.47738203406333923\n",
      "step 11065, loss: 0.4487735331058502\n",
      "step 11066, loss: 0.4346291422843933\n",
      "step 11067, loss: 0.37950456142425537\n",
      "step 11068, loss: 0.4697893559932709\n",
      "step 11069, loss: 0.5328636765480042\n",
      "step 11070, loss: 0.5206317901611328\n",
      "step 11071, loss: 0.48705247044563293\n",
      "step 11072, loss: 0.4994780719280243\n",
      "step 11073, loss: 0.5341188311576843\n",
      "step 11074, loss: 0.5936872363090515\n",
      "step 11075, loss: 0.5338220596313477\n",
      "step 11076, loss: 0.5689131617546082\n",
      "step 11077, loss: 0.49536851048469543\n",
      "step 11078, loss: 0.4973636567592621\n",
      "step 11079, loss: 0.41552790999412537\n",
      "step 11080, loss: 0.40576955676078796\n",
      "step 11081, loss: 0.3735247254371643\n",
      "step 11082, loss: 0.43428683280944824\n",
      "step 11083, loss: 0.43471425771713257\n",
      "step 11084, loss: 0.2778221368789673\n",
      "step 11085, loss: 0.3681713938713074\n",
      "step 11086, loss: 0.34050118923187256\n",
      "step 11087, loss: 0.2871246933937073\n",
      "step 11088, loss: 0.47941502928733826\n",
      "step 11089, loss: 0.5547652244567871\n",
      "step 11090, loss: 0.494488388299942\n",
      "step 11091, loss: 0.45703834295272827\n",
      "step 11092, loss: 0.6256405115127563\n",
      "step 11093, loss: 0.4119753837585449\n",
      "step 11094, loss: 0.2879391610622406\n",
      "step 11095, loss: 0.2486516386270523\n",
      "step 11096, loss: 0.5230153799057007\n",
      "step 11097, loss: 0.6702567338943481\n",
      "step 11098, loss: 0.6002291440963745\n",
      "step 11099, loss: 0.5751895904541016\n",
      "step 11100, loss: 0.5892696380615234\n",
      "step 11101, loss: 0.6335867643356323\n",
      "step 11102, loss: 0.5140061974525452\n",
      "step 11103, loss: 0.5727767944335938\n",
      "step 11104, loss: 0.4092666208744049\n",
      "step 11105, loss: 0.5794209241867065\n",
      "step 11106, loss: 0.513635516166687\n",
      "step 11107, loss: 0.6211416721343994\n",
      "step 11108, loss: 0.6263238787651062\n",
      "step 11109, loss: 0.5320440530776978\n",
      "step 11110, loss: 0.5955228805541992\n",
      "step 11111, loss: 0.6054413318634033\n",
      "step 11112, loss: 0.6089377403259277\n",
      "step 11113, loss: 0.5292133092880249\n",
      "step 11114, loss: 0.7303676605224609\n",
      "step 11115, loss: 0.5376367568969727\n",
      "step 11116, loss: 0.7203992009162903\n",
      "step 11117, loss: 0.6253211498260498\n",
      "step 11118, loss: 0.5065777897834778\n",
      "step 11119, loss: 0.6276007294654846\n",
      "step 11120, loss: 0.795982837677002\n",
      "step 11121, loss: 0.709587574005127\n",
      "step 11122, loss: 0.5251508951187134\n",
      "step 11123, loss: 0.655681312084198\n",
      "step 11124, loss: 0.6147137880325317\n",
      "step 11125, loss: 0.64339280128479\n",
      "step 11126, loss: 0.6619248390197754\n",
      "step 11127, loss: 0.5767924785614014\n",
      "step 11128, loss: 0.5288692116737366\n",
      "step 11129, loss: 0.5795814990997314\n",
      "step 11130, loss: 0.6888278722763062\n",
      "step 11131, loss: 0.735738217830658\n",
      "step 11132, loss: 0.7770053148269653\n",
      "step 11133, loss: 0.5893942713737488\n",
      "step 11134, loss: 0.6827511787414551\n",
      "step 11135, loss: 0.4948388636112213\n",
      "step 11136, loss: 0.6302932500839233\n",
      "step 11137, loss: 0.6342884302139282\n",
      "step 11138, loss: 0.63584303855896\n",
      "step 11139, loss: 0.7042667865753174\n",
      "step 11140, loss: 0.5838721990585327\n",
      "step 11141, loss: 0.4277023673057556\n",
      "step 11142, loss: 0.6352877616882324\n",
      "step 11143, loss: 0.4552873969078064\n",
      "step 11144, loss: 0.5884466171264648\n",
      "step 11145, loss: 0.5200948715209961\n",
      "step 11146, loss: 0.5705724954605103\n",
      "step 11147, loss: 0.5806059241294861\n",
      "step 11148, loss: 0.5568408966064453\n",
      "step 11149, loss: 0.5301641225814819\n",
      "step 11150, loss: 0.6311066150665283\n",
      "step 11151, loss: 0.5184201002120972\n",
      "step 11152, loss: 0.5214529037475586\n",
      "step 11153, loss: 0.5127972960472107\n",
      "step 11154, loss: 0.6138337254524231\n",
      "step 11155, loss: 0.6761792898178101\n",
      "step 11156, loss: 0.4525615870952606\n",
      "step 11157, loss: 0.7162158489227295\n",
      "step 11158, loss: 0.816709578037262\n",
      "step 11159, loss: 0.5749940872192383\n",
      "step 11160, loss: 0.6359771490097046\n",
      "step 11161, loss: 0.5549492835998535\n",
      "step 11162, loss: 0.6990830898284912\n",
      "step 11163, loss: 0.5583720207214355\n",
      "step 11164, loss: 0.6106124520301819\n",
      "step 11165, loss: 0.6940345764160156\n",
      "step 11166, loss: 0.6005753874778748\n",
      "step 11167, loss: 0.5156126618385315\n",
      "step 11168, loss: 0.6368353366851807\n",
      "step 11169, loss: 0.6258140802383423\n",
      "step 11170, loss: 0.6006120443344116\n",
      "step 11171, loss: 0.709428071975708\n",
      "step 11172, loss: 0.47276467084884644\n",
      "step 11173, loss: 0.5358996987342834\n",
      "step 11174, loss: 0.5221773386001587\n",
      "step 11175, loss: 0.6074965000152588\n",
      "step 11176, loss: 0.675688624382019\n",
      "step 11177, loss: 0.5138139128684998\n",
      "step 11178, loss: 0.6586037278175354\n",
      "step 11179, loss: 0.6329328417778015\n",
      "step 11180, loss: 0.8161126971244812\n",
      "step 11181, loss: 0.46543923020362854\n",
      "step 11182, loss: 0.5038028955459595\n",
      "step 11183, loss: 0.5057348608970642\n",
      "step 11184, loss: 0.5075103640556335\n",
      "step 11185, loss: 0.4797627329826355\n",
      "step 11186, loss: 0.37822672724723816\n",
      "step 11187, loss: 0.3842077851295471\n",
      "step 11188, loss: 0.3867936432361603\n",
      "step 11189, loss: 0.4723724126815796\n",
      "step 11190, loss: 0.3995727598667145\n",
      "step 11191, loss: 0.4425094723701477\n",
      "step 11192, loss: 0.35450178384780884\n",
      "step 11193, loss: 0.3366719186306\n",
      "step 11194, loss: 0.335520476102829\n",
      "step 11195, loss: 0.43707340955734253\n",
      "step 11196, loss: 0.2811092734336853\n",
      "step 11197, loss: 0.464339941740036\n",
      "step 11198, loss: 0.2908245921134949\n",
      "step 11199, loss: 0.49746713042259216\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing :3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GPT(\n",
       "  (transformer): ModuleDict(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(1024, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x Block(\n",
       "        (ln_1): LayerNorm()\n",
       "        (attn): CausalSelfAttention(\n",
       "          (c_attn): LoraLinear(in_features=768, out_features=2304, bias=True)\n",
       "          (c_proj): LoraLinear(in_features=768, out_features=768, bias=True)\n",
       "          (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "          (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (ln_2): LayerNorm()\n",
       "        (mlp): MLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (gelu): GELU(approximate='none')\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=50257, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "andrejgpt = torch.load(\"andrejgpt.pt\")\n",
    "andrejgpt.eval()\n",
    "andrejgpt.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.get_encoding(\"gpt2\")\n",
    "tokens = enc.encode(\"[Q] What is Makemore?\")\n",
    "tokens = torch.tensor(tokens, dtype=torch.long)\n",
    "tokens = tokens.unsqueeze(0)\n",
    "x = tokens.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_tokens = andrejgpt.generate(x, 30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'[Q] What is Makemore?\\n[A] Makemore will Makemore will Makemore\\'s \"make more of things that Makemore will become as Makemore\\'s today. '"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = output_tokens.tolist()[0]\n",
    "output = enc.decode(y)\n",
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not great but it acts as a base for better models :3"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "andrejgpt",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
